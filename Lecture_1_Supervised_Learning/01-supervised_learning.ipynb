{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_main_arcada.png\" style=\"width:1400px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning For Predictive Problems\n",
    "\n",
    "## Instructors:\n",
    "> Amin Majd, PhD & DSc. **Researcher & Lecturer**<br/> \n",
    "(*email*:  amin.majd@arcada.fi)<br/>\n",
    "\n",
    "> Andrey Shcherbakov-parland, MSc. **Senior Lecturer in Information Technology.**<br/> \n",
    "(*email*: andrey.shcherbakov@arcada.fi)\n",
    "\n",
    "> Leonardo Espinosa-Leal, PhD. **Senior Lecturer in Big Data Analytics.**<br/> \n",
    "(*email*: leonardo.espinosaleal@arcada.fi)<br/>\n",
    "https://www.espinosaleal.me/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General objectives\n",
    "* Understand the principles and main techniques of predictive analytics.\n",
    "* Get familiarized with the essential python libraries for predictive analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Content* \n",
    "* Lecture 1: Intro to MLPP. Supervised Learning (07.10).\n",
    "* Lecture 2: Preprocesing and Unsupervised Learning (08.10).\n",
    "* Lecture 3: Model Evaluation and Improvement. Algorithm Chains and Pipelines (21.10).\n",
    "* Lecture 4: Representing Data and Engineering Features. Working with text data (22.10).\n",
    "* Lecture 5: Introduction to Deep Learning and Deep Learning for Computer Vision (04.11).\n",
    "* Lecture 6: Strategies and techniques for cleaning and managing datasets. Wrap-up (05.11).\n",
    "\n",
    "#### * The content might change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "**Grading** (100 points): \n",
    "\n",
    "* 6 Exercises. 1 per lecture. (6 x 15 points = 90 points). Groups of two (max). \n",
    " - Deadline: Two weeks to submit. \n",
    " - Submit gitbub link via itslearning and notebook as a backup.\n",
    "\n",
    "* 6 Hands-on Exercises during each lecture (5 x 2 points = 10 points). \n",
    " - Submit at the end of each lecture individual notebook directly to itslearning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final  equivalence scale\n",
    "\n",
    "51 - 60 = 1 <br>\n",
    "61 - 70 = 2 <br>\n",
    "71 - 80 = 3 <br>\n",
    "81 - 90 = 4 <br>\n",
    "91 - 100 = 5 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Late submissions\n",
    "\n",
    "- We use \"soft deadlines\". It means you can submit after the deadline with a given penalization. \n",
    "- This automatically implies we don't provide the solutions of the assignments openly. Please, contact @Andrey if you have doubts or questions (use slack primarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20) \n",
    "plt.rc('axes', labelsize=20) \n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "  \n",
    "def get_gradding():\n",
    "    with open('grading', 'r') as datafile:\n",
    "        ploting = csv.reader(datafile, delimiter=',')\n",
    "\n",
    "        for ROWS in ploting:\n",
    "            X.append(int(ROWS[0]))\n",
    "            Y.append(int(ROWS[1]))\n",
    "            \n",
    "    plt.figure(figsize=(13,9))\n",
    "    plt.plot(X, Y,lw=4)\n",
    "    plt.title('Grading ladder',fontsize=18)\n",
    "    plt.xlabel('Submission after the deadline [days]')\n",
    "    plt.ylabel('Maximum grade')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_gradding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture 1: Supervised Learning\n",
    "\n",
    "Leonardo A. Espinosa, PhD. <br>Senior Lecturer in Big Data Analytics.<br/>\n",
    "email: leonardo.espinosaleal@arcada.fi<br/>\n",
    "Personal webpage: www.espinosaleal.me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goal for today\n",
    "* Understand the principles of supervised learning.\n",
    "* Identify the pros and cons of the main algorithms for regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/ai_ml_dl.png\" style=\"width:1400px\">\n",
    "AI - Machine learning - Deep Learning.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Statistical Learning\n",
    "\n",
    "**Framework for machine learning drawing from the fields of *statistics* and *functional analysis*.**\n",
    "\n",
    "* It deals with the problem of finding a predictive function (**f**) based on data.\n",
    "\n",
    "* It is basically a set of tools for *understanding the data*.\n",
    "\n",
    ">**Machine learning** is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to \"learn\" (e.g., progressively improve performance on a specific task) from data, without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/ml_code.png\" style=\"width:1400px\">\n",
    "Machine learning systems automatically learn programs from data.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Machine learning\n",
    "\n",
    ">* Supervised learning\n",
    ">* Unsupervised learning\n",
    ">* Reinforcement learning\n",
    "\n",
    "#### Others:\n",
    ">* Semi-supervised learning\n",
    ">* Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/ml_kinds.jpeg\" style=\"width:1400px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning (*Supervised Statistical Learning*)\n",
    "\n",
    ">Building a statistical model for predicting, or estimating, an *output* based on one or more *inputs*. \n",
    "\n",
    "Range of disciplines: Bussines, medicine, astrophysics, public policy , social sciences and many more!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification and Regression\n",
    "\n",
    "* >Classification $\\to$ qualitative or categorical variables.\n",
    "* >Regression $\\to$ Continuos numerical quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalization, Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Generalization**: Building a model on the training set and then be able to make predictions on **\"new data\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Overfitting**: Building a model that is too complex for the amount of available information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Underfitting**: Choosing a too simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "> **Bias** ― The bias of a model is the difference between the expected prediction and the correct model that we try to predict for given data points.\n",
    "\n",
    "> **Variance** ― The variance of a model is the variability of the model prediction for given data points.\n",
    "\n",
    "> **Bias/variance tradeoff** ― The simpler the model, the higher the bias, and the more complex the model, the higher the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/bias-variance.png\" style=\"width:1100px\">\n",
    "Example of Fitting-Underfitting models in classification and regression.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Possible Remedies\n",
    "\n",
    "Underfitting:\n",
    "* Model's complexity \n",
    "* Add more features\n",
    "\n",
    "Overfitting:\n",
    "* Perform regularization\n",
    "* Get more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/02-overfitting_underfitting.png\" style=\"width:1000px\">\n",
    "Trade-off of model complexity againts <i>training</i> and <i>test</i> accuracy.\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Relation of Model Complexity to Dataset Size\n",
    ">Having more data and building appropiately more complex models.\n",
    "However data alone is not enough.\n",
    "\n",
    "Always remember:\n",
    "\n",
    "1. Is generalization that counts.\n",
    "\n",
    "2. The *curse of dimensionality* and the *blessing of non-uniformity*.\n",
    "\n",
    "3. No free-lunch theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/cod.jpg\" style=\"width:1000px\">\n",
    "The curse of dimensionality and the blessing of non-uniformity.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/nflt.jpg\" style=\"width:1000px\">\n",
    "No free-lunch theorem\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Machine Learning Algorithms\n",
    "\n",
    "In this Lecture we are going to explore, using examples, the main algorithms for supervised learning, following a taxonomic approach, including some pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/components.png\" style=\"width:1200px\">\n",
    "The three components of learning algorithms.\n",
    "</center>\n",
    "\n",
    "\n",
    ">A Few Useful Things to Know about Machine Learning by Pedro Domingos (see recommended reading)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1\\. <a href=\"#/32/1\">k-nearest neighbors (KNN)</a>:\n",
    "   * for classification.\n",
    "   * for regression.\n",
    "   \n",
    "2\\. <a href=\"#/50/1\">Linear Models</a>:\n",
    "   * <a href=\"#/51/1\">for Regression</a>:\n",
    "       * Linear regression *aka* least squares.\n",
    "       * Ridge.\n",
    "       * Lasso.\n",
    "       * Elastic Net.\n",
    "   * <a href=\"#/65/1\">for Classification</a>:\n",
    "       * Logistic regression.\n",
    "       * Linear Support Vector Machines.\n",
    "       * Linear models for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3\\. <a href=\"#/83\">Naive-Bayes Classifiers</a>\n",
    "   \n",
    "4\\. <a href=\"#/89/1\">Decision Trees</a>:\n",
    "   * for classification.\n",
    "   * for regression.\n",
    "\n",
    "5\\. <a href=\"#/99/1\">Ensembles of Decision Trees</a>:\n",
    "   * Random Forest.\n",
    "   * Gradient Boosted Decision Trees.\n",
    "       \n",
    "6\\. <a href=\"#/109/1\">Kernelized Support Vector Machines</a>\n",
    "\n",
    "7\\. <a href=\"#/130/1\">Conclusions</a>\n",
    "\n",
    "8\\. <a href=\"#/137/1\">Homework</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Wine dataset\n",
    "\n",
    "<center>    \n",
    "<img src=\"./images/wine_red.jpg\" style=\"width:600px\">\n",
    "https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Attribute Information:** For more information https://link.springer.com/chapter/10.1007/978-3-642-04747-3_8. Input variables (based on physicochemical tests): \n",
    "1. fixed acidity\n",
    "2. volatile acidity\n",
    "3. citric acid\n",
    "4. residual sugar\n",
    "5. chlorides\n",
    "6. free sulfur dioxide\n",
    "7. total sulfur dioxide\n",
    "8. density\n",
    "9. pH\n",
    "10. sulphates\n",
    "11. alcohol\n",
    "12. Output variable (based on sensory data) $\\to$ quality (score between 0 and 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workhorses\n",
    "\n",
    "* Pandas (https://pandas.pydata.org/)\n",
    "* Matplotlib / Seaborn (https://matplotlib.org/  & https://seaborn.pydata.org/)\n",
    "* Scipy (https://www.scipy.org/)\n",
    "* Numpy (https://numpy.org/)\n",
    "* Scikit-learn (https://scikit-learn.org)\n",
    "* Mglearn (https://pypi.org/project/mglearn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/scikit-learn-logo.png\" style=\"width:800px\">\n",
    "http://scikit-learn.org\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Simple and efficient tools for data mining and data analysis\n",
    "* Accessible to everybody, and reusable in various contexts\n",
    "* Built on NumPy, SciPy, and matplotlib\n",
    "* Open source, commercially usable - BSD license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ">Users: J.P.Morgan, Spotify, Evernote, Booking.com a, OKCupid and many others!<br>\n",
    "https://scikit-learn.org/stable/testimonials/testimonials.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/ml_map.png\" style=\"width:1100px\">\n",
    "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/ml_map-cr.png\" style=\"width:1600px\">\n",
    "    Today's Lecture\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mglearn\n",
    "\n",
    "from matplotlib import rc\n",
    "font = {'family' : 'monospace', 'weight' : 'bold', 'size'   : 25}\n",
    "rc('font', **font) \n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.rcParams['lines.linewidth'] = 5.0\n",
    "plt.rcParams['lines.markersize'] = 15.0\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "#plt.rcParams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# In python, the power of tab and ?\n",
    "\n",
    "plt.autumn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/data_preparation.png\" style=\"width:1200px\">\n",
    "<br>\n",
    "<br>\n",
    "Preparing the data first.\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "path_red=\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df_rw = pd.read_csv(path_red,sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "path_white=\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "df_ww = pd.read_csv(path_white,sep=';')\n",
    "\n",
    "#df_ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_rw['type'] = 0\n",
    "df_ww['type'] = 1\n",
    "\n",
    "df_full = pd.concat([df_rw,df_ww], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "names = list(df_full.columns)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can extract data from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_full.iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#df_full.iloc[:,10] # regression\n",
    "#df_full.iloc[:,11] # multiclass  \n",
    "#df_full.iloc[:,12] # binary class\n",
    "\n",
    "# alternative way\n",
    "#df_full['alcohol']\n",
    "#df_full.alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Here we turn into numpy arrays \n",
    "X = df_full.iloc[:,:10].values      # features\n",
    "\n",
    "# predict \n",
    "\n",
    "y_reg = df_full.iloc[:,10].values   # df_full['alcohol'].values  # regression\n",
    "y_cls = df_full.iloc[:,12].values   # df_full['type'].values   # red or white wine\n",
    "y_mul = df_full.iloc[:,11].values   # df_full['quality'].values # multiclass classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_plot_scatter():\n",
    "    \n",
    "    X_dataframe = pd.DataFrame(X, columns=df_full.iloc[:,:10].columns)\n",
    "\n",
    "    axes = pd.plotting.scatter_matrix(X_dataframe, c=y_cls, figsize=(30, 16), marker='o',\n",
    "    hist_kwds={'bins': 20}, s=20, alpha=.5, cmap=mglearn.cm3)\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.yaxis.label.set_size('8') \n",
    "        ax.xaxis.label.set_size('8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "get_plot_scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# *k*-Nearest Neighbors \n",
    "\n",
    "The most intuitive algorithm. There are two versions:\n",
    "\n",
    "1. ### *k*-Neighbors for classification \n",
    "2. ### *k*-Neighbors for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>    \n",
    "<img src=\"./images/02-knns.png\" alt=\"Drawing\" style=\"width: 1200px;\"/>\n",
    "<strong>Figure 2:</strong> KNN examples. <strong>Top</strong>: K=1 and <strong>bottom</strong>: K=3. <strong>Left</strong>: for classification and <strong>right</strong>: for regression.        \n",
    "</center>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cls, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=50)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#KNeighborsClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try n_neighbors from 1 to any number\n",
    "neighbors_settings = list(range(1, 20))\n",
    "for n_neighbors in neighbors_settings:\n",
    "# build the model\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "# record training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "# record generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\",marker='o')\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\",marker='p')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.gca().invert_xaxis()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\",marker='o')\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\",marker='p')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylim([0.92,0.95])\n",
    "plt.gca().invert_xaxis()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analyzing KNeighborsClassifier: Benchmark example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def knn_test():\n",
    "    Xt, yt = mglearn.datasets.make_forge()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3)\n",
    "    for n_neighbors, ax in zip([1, 3, 9], axes):\n",
    "    # the fit method returns the object self, so we can instantiate\n",
    "    # and fit in one line\n",
    "        clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(Xt, yt)\n",
    "        mglearn.plots.plot_2d_separator(clf, Xt, fill=True, eps=0.5, ax=ax, alpha=.4)\n",
    "        mglearn.discrete_scatter(Xt[:, 0], Xt[:, 1], yt, ax=ax)\n",
    "        ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n",
    "        ax.set_xlabel(\"feature 0\")\n",
    "        ax.set_ylabel(\"feature 1\")\n",
    "        axes[0].legend(loc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "knn_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, random_state=0)\n",
    "reg = KNeighborsRegressor(n_neighbors=10)\n",
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try n_neighbors from 1 to wherever\n",
    "neighbors_settings = list(range(1, 100))\n",
    "for n_neighbors in neighbors_settings:\n",
    "# build the model\n",
    "    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    reg.fit(X_train, y_train)\n",
    "# record training set accuracy\n",
    "    training_accuracy.append(reg.score(X_train, y_train))\n",
    "# record generalization accuracy\n",
    "    test_accuracy.append(reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_wines_knn():\n",
    "    plt.plot(neighbors_settings, training_accuracy, label=\"training \")\n",
    "    plt.plot(neighbors_settings, test_accuracy, label=\"test\")\n",
    "    plt.ylabel(\"Score ($R^2$)\")\n",
    "    plt.xlabel(\"n_neighbors\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_wines_knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_wines_knn_zoom():\n",
    "    plt.plot(neighbors_settings, training_accuracy, label=\"training\")\n",
    "    plt.plot(neighbors_settings, test_accuracy, label=\"test\")\n",
    "    plt.ylabel(\"score ($R^2$)\")\n",
    "    plt.xlabel(\"n_neighbors\")\n",
    "    plt.ylim([0.15,0.5])\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_wines_knn_zoom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analyzing KNeighborsRegressor: Benchmark example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def knn_regressor():\n",
    "    X, y = mglearn.datasets.make_wave(n_samples=40) # just a dummy dataset\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "    line = np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
    "    for n_neighbors, ax in zip([1, 3, 20], axes):\n",
    "    # make predictions using 1, 3, or 9 neighbors\n",
    "        reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "        reg.fit(X_train2, y_train2)\n",
    "        ax.plot(line, reg.predict(line))\n",
    "        ax.plot(X_train2, y_train2, '^', c=mglearn.cm2(0), markersize=8)\n",
    "        ax.plot(X_test2, y_test2, 'v', c=mglearn.cm2(1), markersize=8)\n",
    "\n",
    "        ax.set_title(\"{} neighbor(s)\\n train score: {:.2f}\\n test score: {:.2f}\".format(n_neighbors, \n",
    "                    reg.score(X_train2, y_train2),reg.score(X_test2, y_test2)))\n",
    "        ax.set_xlabel(\"Feature\")\n",
    "        ax.set_ylabel(\"Target\")\n",
    "\n",
    "    axes[0].legend([\"Model predictions\", \"Training data/target\",\"Test data/target\"], loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "knn_regressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions on the *KNN* algorithms\n",
    "\n",
    "* Two important parameters: the number of neighbors and how you measure the distance between points. By default is the Minkowski with p=2.\n",
    "\n",
    "$$ d(\\mathbf{x},\\mathbf{y}) = \\left[\\sum_{i=1}^N (x_i - y_i)^p \\right]^{\\frac{1}{p}} $$\n",
    "\n",
    "* It is a model easy to understand. But its perform is poor on large datasets (either in number of features or in number of samples) or sparse data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Models\n",
    "\n",
    "Linear models make a prediction using a linear function of the input features.\n",
    "\n",
    "\n",
    "## Linear models for regression and linear models for classification.\n",
    "\n",
    "$$\\hat{y}(\\mathbf{w},\\mathbf{x}) = w_0 +  w_1 * x_1 + w_2 * x_2 + ... + w_p * x_p $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Models for Regression\n",
    "\n",
    "   * Ordinary Least squares\n",
    "   $$ \\underset{w}{min\\,} {|| X w - y||_2}^2  $$\n",
    "      \n",
    "   * Ridge (L2 regularization)\n",
    "   $$  \\underset{w}{min\\,} {{|| X w - y||_2}^2 + \\alpha {||w||_2}^2} $$\n",
    "   \n",
    "   * Lasso (L1 regularization)\n",
    "   $$ \\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1} $$\n",
    "   \n",
    "   \n",
    "If $X$ is a matrix of size $(n, p)$ this methods have a cost of $O(n p^2)$, assuming that $n \\geq p$.\n",
    "\n",
    "   * Elastic Net (L2 and L1 regularization)\n",
    "   $$ \\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 + \\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>    \n",
    "<img src=\"./images/lin-reg.png\" alt=\"Drawing\" style=\"width: 1400px;\"/>\n",
    "Figure 3: Regularization methods.        \n",
    "</center>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, random_state=42)\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\n",
    "#Ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ridge01 = Ridge(alpha=0.01).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_names = names[:10]\n",
    "\n",
    "def plot_ridge():\n",
    "    plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\n",
    "    plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
    "    plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.01\")\n",
    "    plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
    "    plt.xticks(range(len(X_names)), X_names, rotation=90)\n",
    "    plt.xlabel(\"Coefficient index\")\n",
    "    plt.ylabel(\"Coefficient magnitude\")\n",
    "    plt.hlines(0, 0, len(lr.coef_))\n",
    "    plt.ylim(-15, 10)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import learning_curve, KFold\n",
    "\n",
    "\n",
    "def plot_learning_curve(est, X, y):\n",
    "    training_set_size, train_scores, test_scores = learning_curve(\n",
    "        est, X, y, train_sizes=np.linspace(.1, 1, 20), cv=KFold(20, shuffle=True, random_state=1))\n",
    "    estimator_name = est.__class__.__name__\n",
    "    line = plt.plot(training_set_size, train_scores.mean(axis=1), '--',\n",
    "                    label=\"training \" + estimator_name)\n",
    "    plt.plot(training_set_size, test_scores.mean(axis=1), '-',\n",
    "             label=\"test \" + estimator_name, c=line[0].get_color())\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('Score (R^2)')\n",
    "    plt.ylim(0, 1.1)\n",
    "\n",
    "\n",
    "def plot_ridge_n_samples(X,y,alpha=1):\n",
    "    plot_learning_curve(Ridge(alpha=alpha), X, y)\n",
    "    plot_learning_curve(LinearRegression(), X, y)\n",
    "    plt.legend(loc=(0, 1.05), ncol=2, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "plot_ridge_n_samples(X,y_reg,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_b, y_b = mglearn.datasets.load_extended_boston()\n",
    "plot_ridge_n_samples(X_b,y_b,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, random_state=42)\n",
    "\n",
    "lasso = Lasso(max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_lasso_vs_ridge():\n",
    "    plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\n",
    "    plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
    "    plt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n",
    "    plt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\n",
    "    plt.xticks(range(len(X_names)), X_names, rotation=90)\n",
    "    plt.hlines(0, 0,len(X_names))\n",
    "    plt.legend(ncol=2, loc=(0, 1.05))\n",
    "    plt.ylim(-10, 5)\n",
    "    plt.xlabel(\"Coefficient index\")\n",
    "    plt.ylabel(\"Coefficient magnitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_lasso_vs_ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions on  Linear Models for Regression\n",
    "* Despite their simplicity, linear models for regression are widely used in industry.\n",
    "* Lasso is good for variable selection.\n",
    "* Ridge builds a hard regularized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Models for Classification\n",
    "\n",
    "* Logistic Regression (with L1 or L2 regularization)\n",
    "$$\\underset{w, c}{min\\,} \\|w\\|_1 \\quad or\\quad \\underset{w, c}{min\\,} \\frac{1}{2}w^T w  \\quad + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) .\n",
    "$$\n",
    "\n",
    "* Linear Support Vector Machines  (for $x_i \\in \\mathbb{R}^p, i=1,…, n,$ and $y \\in \\{1, -1\\}^n$)\n",
    "$$ \\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i \\quad \\textrm {subject to }\\quad  y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i,\\\\  \\zeta_i \\geq 0, i=1, ..., n $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>    \n",
    "<img src=\"./images/lr-svm.jpg\" alt=\"Drawing\" style=\"width: 1750px;\"/>\n",
    "Figure 4. Logistic Regression (*left*) and LSVM (*right*).\n",
    "</center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cls, random_state=42)\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "#logreg = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "logreg100 = LogisticRegression(C=100,max_iter=1000,solver='liblinear').fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))\n",
    "\n",
    "#LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression with L2 regularization\n",
    "\n",
    "def plot_lg_l2():\n",
    "    plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
    "    plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "    plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "    plt.xticks(range(len(X_names)), X_names, rotation=90)\n",
    "    plt.hlines(0, 0,len(X_names))\n",
    "    plt.ylim(-5, 2)\n",
    "    plt.xlabel(\"Coefficient index\")\n",
    "    plt.ylabel(\"Coefficient magnitude\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_lg_l2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression with L1 regularization\n",
    "\n",
    "def plot_lg_l1():\n",
    "    for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n",
    "        lr_l1 = LogisticRegression(C=C, penalty=\"l1\",solver='liblinear').fit(X_train, y_train)\n",
    "        #print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
    "            #C, lr_l1.score(X_train, y_train)))\n",
    "        #print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
    "            #C, lr_l1.score(X_test, y_test)))\n",
    "        plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\n",
    "\n",
    "    plt.xticks(range(len(X_names)), X_names, rotation=90)\n",
    "    plt.hlines(0, 0, len(X_names))\n",
    "    plt.xlabel(\"Coefficient index\")\n",
    "    plt.ylabel(\"Coefficient magnitude\")\n",
    "    plt.ylim(-5, 3)\n",
    "    plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_lg_l1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear models for multiclass classification\n",
    "\n",
    "* one-vs.-rest approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>    \n",
    "<img src=\"./images/one-vs-rest.png\" alt=\"Drawing\" style=\"width: 1250px;\"/>\n",
    "Figure 5. Multiclass classification vs binary classification \n",
    "</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X_b, y_b = make_blobs(random_state=42)\n",
    "mglearn.discrete_scatter(X_b[:, 0], X_b[:, 1], y_b)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(X_b, y_b)\n",
    "print(\"Coefficient shape: \", linear_svm.coef_.shape)\n",
    "print(\"Intercept shape: \", linear_svm.intercept_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X_b[:, 0], X_b[:, 1], y_b)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,['b', 'r', 'g']):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1', 'Line class 2'], loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Well linearly separated classes are easy to solve!\n",
    "\n",
    "Now let's go back to our wine dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_full['quality'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_new_full = df_full.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_new_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a new quality class\n",
    "\n",
    "df_new_full.loc[(df_new_full['quality'] == 3) | (df_new_full['quality'] == 4),'NewQuality'] = 0\n",
    "df_new_full.loc[(df_new_full.quality == 5) | (df_new_full.quality == 6) | (df_new_full.quality == 7),'NewQuality'] = 1\n",
    "df_new_full.loc[((df_new_full.quality == 8) | (df_new_full['quality'] == 9)),'NewQuality'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_new_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_new_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_new_full.astype({'NewQuality': 'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = df_new_full.iloc[:,:10].values   # input \n",
    "y_nq = df_new_full.iloc[:,13].values # Newquality multiclass target  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m,n = 3,9\n",
    "mglearn.discrete_scatter(X[:, m], X[:, n], y_nq)\n",
    "plt.xlabel(X_names[m])\n",
    "plt.ylabel(X_names[n])\n",
    "plt.legend(['Low: 0', 'Medium: 1', 'High: 2'], title='wine quality' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#linear_svm = LinearSVC().fit(X, y_nq)\n",
    "linear_svm = LinearSVC().fit(X[:,[m, n]], y_nq)\n",
    "print(\"Coefficient shape: \", linear_svm.coef_.shape)\n",
    "print(\"Intercept shape: \", linear_svm.intercept_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "linear_svm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "line = np.linspace(min(X[:, m]), max(X[:, m]))\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,['b', 'r', 'g']):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "\n",
    "mglearn.discrete_scatter(X[:, m], X[:, n], y_nq)\n",
    "    \n",
    "plt.xlabel(X_names[m])\n",
    "plt.ylabel(X_names[n])\n",
    "plt.legend([ 'Line class 0', 'Line class 1','Line class 2', 'Low: 0', 'Medium: 1', 'High: 2'], loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Question time!!!\n",
    "\n",
    "https://www.poll-maker.com/poll3931062x370249EE-125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions about Linear Models\n",
    "\n",
    "* The main parameter of linear models is the regularization parameter (L1 or L2). If you assume that only a few of your features are actually important, you should use L1. Otherwise, you should use the default  L2. L1 can also be useful if interpretability of the model is important.\n",
    "\n",
    "* Linear models are very fast to train, and also fast to predict. They scale to very large datasets and work well with sparse data.\n",
    "\n",
    "* Linear models often perform well when the number of features is large compared to the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_hands-on.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's fit two linear models, one for classification and one for regression.\n",
    "\n",
    "1. Breast cancer wisconsin dataset  (classification). https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer \n",
    "\n",
    "`from sklearn.datasets import load_breast_cancer\n",
    "X_bc, y_bc = load_breast_cancer(return_X_y=True)`\n",
    "\n",
    "\n",
    "2. California housing dataset (regression) https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing\n",
    "\n",
    "`from sklearn.datasets import fetch_california_housing\n",
    "X_cal, y_cal = fetch_california_housing(return_X_y=True)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tasks\n",
    "\n",
    "* Use train_test_split to create two subsets of data, one for fitting the model and the other for testing the model (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) hint:\n",
    "`from sklearn.model_selection import train_test_split`\n",
    "\n",
    "* Fit one model for each dataset. test two different values of parameters apart of the parameter by default. Check the score on the test set. hint: `from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression`\n",
    "\n",
    "* Hint: use ? e.g. `LogisticRegression?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "\n",
    "X_bc, y_bc = load_breast_cancer(return_X_y=True) # load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X_cal, y_cal = fetch_california_housing(return_X_y=True) # load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/break.png\" style=\"width:1000px\">\n",
    "    Break\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/Thomas_Bayes.png\" style=\"width:1000px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive-Bayes Classifiers\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features.\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)} {P(x_1, \\dots, x_n)}\n",
    " $$\n",
    "\n",
    "Because the argument of independency\n",
    "\n",
    "$$ P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y) \\\\\n",
    "\\Downarrow \\\\\n",
    "\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NB Family\n",
    "*  **Gaussian Naive Bayes**:\n",
    "$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right) $$\n",
    "\n",
    "* **Multinomial Naive Bayes**:\n",
    "   The distribution is parametrized by vectors $\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})$ for each class $y$,\n",
    "$$ \\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n} $$\n",
    "   where $N_{yi} = \\sum_{x \\in T} x_i$ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$, and $N_{y} = \\sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class $y$.\n",
    "   \n",
    "* **Bernoulli Naive Bayes:** Multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable.\n",
    "$$P(x_i \\mid y) = P(i \\mid y) x_i + (1 - P(i \\mid y)) (1 - x_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/NB_scheme.png\" style=\"width:1800px\">\n",
    "We estimate $P(x$$_{\\alpha}$$|y)$ independently in each dimension (middle two images) and then obtain an estimate of the full data distribution by assuming conditional independence $P(x|y)=$$\\prod_{\\alpha}$$P(x$$_{\\alpha}$$|y)$ (very right image).\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Multinomial NB is a NB with a multinomial distribution. We assume the data is distributed following a multinomial distribution. Eg. distribution of words in texts.\n",
    "* Bernoulli NB assumes a binary distribution of data. Eg. Text after using the bag-of-words method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X,y_mul).predict(X)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(X.shape[0],(y_mul != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_mul, random_state=42)\n",
    "gnb_fit = gnb.fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(gnb_fit.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(gnb_fit.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions about NB Classifiers\n",
    "\n",
    "*  GaussianNB is mostly used on very high-dimensional data, while the other two variants of naive Bayes are widely used for sparse count data such as text. MultinomialNB usually performs better than BinaryNB , particularly on datasets with a relatively large number of nonzero features (i.e., large documents).\n",
    "\n",
    "* The naive Bayes models share many of the strengths and weaknesses of the linear models. They are very fast to train and to predict, and the training procedure is easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/decision-trees.jpg\" style=\"width:1000px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees\n",
    "Decision trees are widely used models for classification and regression tasks. Essentially, they learn a hierarchy of if/else questions, leading to a decision.\n",
    "\n",
    "<center>    \n",
    "<img src=\"./images/02-DT.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "Figure 6. A decision tree to distinguish among several animals\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_cls, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"Red\", \"White\"],\n",
    "feature_names=X_names, impurity=False, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Note: you should install graphviz in your system. \n",
    "#! pip install graphviz\n",
    "import graphviz\n",
    "with open('tree.dot') as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), X_names)\n",
    "    plt.title('Feature importance for Wine dataset W/R')\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Decision trees for regression\n",
    "ram_prices = pd.read_csv(\"data/ram_price.csv\")\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# use historical data to forecast prices after the year 2000\n",
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test = ram_prices[ram_prices.date >= 2000]\n",
    "# predict prices based on date\n",
    "X_train = data_train.date[:, np.newaxis]\n",
    "# we use a log-transform to get a simpler relationship of data to target\n",
    "y_train = np.log(data_train.price)\n",
    "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "# predict on all data\n",
    "X_all = ram_prices.date[:, np.newaxis]\n",
    "pred_tree = tree.predict(X_all)\n",
    "pred_lr = linear_reg.predict(X_all)\n",
    "# undo log-transform\n",
    "price_tree = np.exp(pred_tree)\n",
    "price_lr = np.exp(pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogy(data_train.date, data_train.price, label=\"Training data\")\n",
    "plt.semilogy(data_test.date, data_test.price, label=\"Test data\")\n",
    "plt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\n",
    "plt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions about Decision Trees Classifier\n",
    "\n",
    "* One of the main drawbacks of decision trees is the tendency to overfit and provide poor generalization performance.\n",
    "\n",
    "* Usually, picking one of the pre-pruning strategies by setting either *max_depth* , *max_leaf_nodes* , or   *min_samples_leaf* is sufficient to prevent overfitting.\n",
    "\n",
    "* The resulting model can easily be visualized and understood by nonexperts (at least for smaller trees), and the algorithms are completely invariant to scaling of the data.\n",
    "\n",
    "* Decision trees do not have the ability to generate *new* responses, outside of what was seen in the training data. This shortcoming applies to all models based on trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensembles of Decision Trees\n",
    "\n",
    "Ensembles are methods that combine multiple machine learning models to create more powerful models.\n",
    "\n",
    "* Random forests\n",
    "\n",
    "* Gradient boosted regression trees (gradient boosting machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_mul, random_state=42)\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Importance for Random Forest\n",
    "plot_feature_importances(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions about Random Forest\n",
    "\n",
    "* Random forests for regression and classification are currently among the most widely used machine learning methods.\n",
    "\n",
    "* Easy parallelization.\n",
    "\n",
    "* Random forests don’t tend to perform well on very high dimensional, sparse data, such as text data. For this kind of data, linear models might be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient boosted regression trees (gradient boosting machines)\n",
    "\n",
    "* The gradient boosted regression tree is another ensemble method that combines multiple decision trees to create a more powerful model.\n",
    "\n",
    "* Use for both regression and classification. \n",
    "\n",
    "* Gradient boosting works by building trees in a serial manner, where each tree tries to correct the mistakes of the previous one.\n",
    "\n",
    "* Combine many simple models like shallow trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_mul, random_state=42)\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=4)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "plot_feature_importances(gbrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center>    \n",
    "<img src=\"./images/mistakes.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "How to be the Best of the Best (from Kaggle's point of view)\n",
    "</center>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ">learns from the mistakes by increasing the weight of misclassified data points.\n",
    "\n",
    "* AdaBoost (Adaptive Boosting):  \n",
    "\n",
    ">Gradient boosting learns from the mistake — residual error directly, rather than update the weights of data points. \n",
    "\n",
    "* XGBoost \n",
    "* Catboost\n",
    "* LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions about Gradient boosted regression trees\n",
    "\n",
    "* They are among the most powerful and widely used models for supervised learning. \n",
    "\n",
    "* Their main drawback is that they require careful tuning of the parameters and may take a long time to train.\n",
    "\n",
    "* They are usually the winner methods in competitions such as Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/SVM_diagram.jpg\" style=\"width:1000px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernelized Support Vector Machines\n",
    "\n",
    "Kernelized support vector machines (SVMs) are an extension of Linear Support Vector Machines that allows for more complex models that are not defined simply by hyperplanes in the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "plt.rcParams['lines.markersize'] = 25.0\n",
    "\n",
    "\n",
    "X_blob, y_blob = make_blobs(centers=4, random_state=8)\n",
    "y_blob = y_blob % 2\n",
    "mglearn.discrete_scatter(X_blob[:, 0], X_blob[:, 1], y_blob)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def plot_lin_svm():\n",
    "    linear_svm = LinearSVC(max_iter=10000).fit(X_blob, y_blob)\n",
    "    mglearn.plots.plot_2d_separator(linear_svm, X_blob)\n",
    "    mglearn.discrete_scatter(X_blob[:, 0], X_blob[:, 1], y_blob)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_lin_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "\n",
    "X_new = np.hstack([X_blob, X_blob[:, 1:] ** 2])\n",
    "mask = y_blob == 0\n",
    "\n",
    "def plot_3ln_svm():\n",
    "\n",
    "\n",
    "    figure = plt.figure()\n",
    "    # visualize in 3D\n",
    "    ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "    # plot first all the points with y == 0, then all with y == 1\n",
    "   \n",
    "    ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "    ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "    ax.set_xlabel(\"feature0\")\n",
    "    ax.set_ylabel(\"feature1\")\n",
    "    ax.set_zlabel(\"feature1 ** 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "plot_3ln_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "linear_svm_3d = LinearSVC(max_iter=10000).fit(X_new, y_blob)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "X_new = np.hstack([X_blob, X_blob[:, 1:] ** 2])\n",
    "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "\n",
    "\n",
    "def plot_3plane_svm():\n",
    "    \n",
    "\n",
    "\n",
    "    # show linear decision boundary\n",
    "    figure = plt.figure()\n",
    "    ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "    ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "    ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "    ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "    ax.set_xlabel(\"feature0\")\n",
    "    ax.set_ylabel(\"feature1\")\n",
    "    ax.set_zlabel(\"feature0 ** 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "plot_3plane_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_3proj_svm():\n",
    "\n",
    "    ZZ = YY ** 2\n",
    "    dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
    "    plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
    "    cmap=mglearn.cm2, alpha=0.5)\n",
    "    mglearn.discrete_scatter(X_blob[:, 0], X_blob[:, 1], y_blob)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_3proj_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# radial basis function (RBF) kernel, also known as the Gaussian kernel.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def plot_svc_rbf():\n",
    "    X_hc, y_hc = mglearn.tools.make_handcrafted_dataset()\n",
    "    svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X_hc, y_hc)\n",
    "    mglearn.plots.plot_2d_separator(svm, X_hc, eps=.5)\n",
    "    mglearn.discrete_scatter(X_hc[:, 0], X_hc[:, 1], y_hc)\n",
    "    # plot support vectors\n",
    "    sv = svm.support_vectors_\n",
    "    # class labels of support vectors are given by the sign of the dual coefficients\n",
    "    sv_labels = svm.dual_coef_.ravel() > 0\n",
    "    mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_svc_rbf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mul_svc_rbf():\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    for ax, C in zip(axes, [-1, 0, 3]):\n",
    "        for a, gamma in zip(ax, range(-1, 2)):\n",
    "            mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n",
    "    axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n",
    "    ncol=4, loc=(.25, 1.2),fontsize='25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_mul_svc_rbf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 25})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_cls, random_state=42)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feats_svc_rbf():\n",
    "\n",
    "    plt.plot(X_train.min(axis=0), 'o', label=\"min\")\n",
    "    plt.xticks(range(len(X_names)), X_names, rotation=90)\n",
    "    plt.plot(X_train.max(axis=0), '^', label=\"max\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.xlabel(\"Feature index\")\n",
    "    plt.ylabel(\"Feature magnitude\")\n",
    "\n",
    "# For SVN Preprocessing is very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_feats_svc_rbf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions about Kernelized Support Vector Machines\n",
    "\n",
    "* SVMs allow for complex decision boundaries, even if the data has only a few features.\n",
    "\n",
    "* They work very well on high- and low- dimensional data.\n",
    "\n",
    "* Quite poor scaling with the number of samples (more than 100k can be a headache).\n",
    "\n",
    "* SVMs require very careful tuning of parameters and preprocesing of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Question time !!!\n",
    "\n",
    "https://www.poll-maker.com/poll3931091x20Ec229C-125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_hands-on.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's fit two models one tree based and one SVC.\n",
    "\n",
    "Use the wine dataset from scikit-learn. Similar to the previous hands-on exercises, split the data and run using different parameters to obtain the best score on the test set (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html).\n",
    "\n",
    "`from sklearn.datasets import load_wine\n",
    "X_w , y_w = load_wine(return_X_y=True)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "X_w , y_w = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How different models do classify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "def foooo(h =.02): # step size in the mesh\n",
    "\n",
    "    names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "             \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "             \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(kernel=\"linear\", C=0.025),\n",
    "        SVC(gamma=2, C=1),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "        MLPClassifier(alpha=1,max_iter=10000),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "    X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                               random_state=1, n_clusters_per_class=1)\n",
    "    rng = np.random.RandomState(2)\n",
    "    X += 2 * rng.uniform(size=X.shape)\n",
    "    linearly_separable = (X, y)\n",
    "\n",
    "    datasets = [make_moons(noise=0.3, random_state=0),\n",
    "                make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "                linearly_separable\n",
    "                ]\n",
    "\n",
    "    figure = plt.figure(figsize=(30, 12))\n",
    "    i = 1\n",
    "    # iterate over datasets\n",
    "    for ds_cnt, ds in enumerate(datasets):\n",
    "        # preprocess dataset, split into training and test part\n",
    "        X, y = ds\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "\n",
    "        # just plot the dataset first\n",
    "        cm = plt.cm.RdBu\n",
    "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(\"Input data\")\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "                   edgecolors='k')\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        i += 1\n",
    "\n",
    "        # iterate over classifiers\n",
    "        for name, clf in zip(names, classifiers):\n",
    "            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "\n",
    "            # Plot the decision boundary. For that, we will assign a color to each\n",
    "            # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "            if hasattr(clf, \"decision_function\"):\n",
    "                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "            else:\n",
    "                Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "            # Put the result into a color plot\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "            # Plot the training points\n",
    "            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                       edgecolors='k')\n",
    "            # Plot the testing points\n",
    "            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                       edgecolors='k', alpha=0.6)\n",
    "\n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "            if ds_cnt == 0:\n",
    "                ax.set_title(name)\n",
    "            ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                    size=15, horizontalalignment='right')\n",
    "            i += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['lines.markersize'] = 12.0\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "foooo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General Conclusion I\n",
    "\n",
    "* **Nearest neighbors**: For small datasets, good as a baseline, easy to explain.\n",
    "* **Linear models**: Go-to as a first algorithm to try, good for very large datasets, good for very highdimensional data.\n",
    "* **Naive Bayes**: Only for classification. Even faster than linear models, good for very large data sets and high-dimensional data. Often less accurate than linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General Conclusion II\n",
    "\n",
    "* **Decision trees**: Very fast, don't need scaling of the data, can be visualized and easily explained. They don't predict on new data out of their training set.\n",
    "* **Random forests**: Nearly always perform better than a single decision tree, very robust and powerful. Don’t need scaling of data. Not good for very high-dimensional sparse data.\n",
    "* **Gradient boosted decision trees**: Often slightly more accurate than random forests. Slower to train but faster to predict than random forests, and smaller in memory. Need more parameter tuning than random forests.\n",
    "* **Support vector machines**: Powerful for medium-sized datasets of features with similar meaning. Require scaling of data, sensitive to parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General Conclusion III\n",
    "\n",
    "* Copy and paste an algorithm for any given problem is the zero step. The most important part is rehearse and explore different methods as much as you can on different interesting problems. \n",
    "\n",
    "* Simplicity is a virtue by itself. **Occam's razor**.\n",
    "\n",
    "* The best way to stand out over the crowd is try to understand the concepts (and the math) behind the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extras: Model selection in competitive data science vs real world\n",
    "\n",
    "* *Problem Definition*: The real world sucks! Use as much as possible domain knowledge.\n",
    "* *Metrics*: In the real world, they can be very problem-dependent. Remember Goodhart's law.\n",
    "* *Interpretability*: People who pay for your time need to know what you do.\n",
    "* *Data Quality*: Here is the line that divides the reality of the kaggle world.\n",
    "* *Scalability*: Because benchmarking is nice, but real life is hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bibliography:\n",
    "<center>\n",
    "<img src=\"./images/biblio.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "</center>\n",
    "\n",
    "* http://shop.oreilly.com/product/0636920030515.do\n",
    "* https://sebastianraschka.com/books.html (Perlego access)\n",
    "\n",
    "Theoretical books\n",
    "\n",
    "* http://www-bcf.usc.edu/~gareth/ISL/\n",
    "* https://web.stanford.edu/~hastie/ElemStatLearn/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other sources:\n",
    "* https://coursera.org/learn/machine-learning\n",
    "* https://www.kdnuggets.com/\n",
    "\n",
    "## Recomended lecture:\n",
    "*A Few Useful Things to Know about Machine Learning* by Pedro Domingos (Communications of the ACM, Vol. 55 No. 10, Pages 78-87, 2012.):\n",
    "https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_hmw.png\" style=\"width:1000px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assignment 1\n",
    "\n",
    "Remember -> submission using itslearning, both notebook and github repo *deadline on 22.11.2021*\n",
    "\n",
    "#### Task 1 (3 points):\n",
    "\n",
    "Regression $\\to$ Superconductivity Data Set\n",
    "\n",
    "The goal here is to predict the critical temperature based on the features extracted.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data\n",
    "\n",
    "* Fit two regression models to predict the critical temperature. Report the score with the default parameters of each model.\n",
    "* Perform a simple manual optimization for one of the default parameters (at least 5 different values) and plot the new obtained score as a function of the chosen parameter. Plot the coefficient magnitudes for the best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Task 2 (3 points):\n",
    "\n",
    "Binary classification $\\to$ Default of credit card clients Data Set\n",
    "\n",
    "The goal here is to predict the default payment next month.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "* Fit two binary classification models to predict the client's credit card default. Report accuracy with the default parameters of each model.\n",
    "* Perform a simple manual optimization for one of the default parameters (at least 5 different values) and plot the new obtained accuracy as a function of the chosen parameter.. Plot the feature importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Task 3 (9 points): \n",
    "\n",
    "Multiclass & binary classification $\\to$ Drug consumption (quantified) Data Set\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29\n",
    "\n",
    "Data Set Information:\n",
    "\n",
    "Database contains records for 1885 respondents. For each respondent 12 attributes are known: Personality measurements which include NEO-FFI-R (neuroticism, extraversion, openness to experience, agreeableness, and conscientiousness), BIS-11 (impulsivity), and ImpSS (sensation seeking), level of education, age, gender, country of residence and ethnicity. All input attributes are originally categorical and are quantified. After quantification values of all input features can be considered as real-valued. In addition, participants were questioned concerning their use of 18 legal and illegal drugs (alcohol, amphetamines, amyl nitrite, benzodiazepine, cannabis, chocolate, cocaine, caffeine, crack, ecstasy, heroin, ketamine, legal highs, LSD, methadone, mushrooms, nicotine and volatile substance abuse and one fictitious drug (Semeron) which was introduced to identify over-claimers. For each drug they have to select one of the answers: never used the drug, used it over a decade ago, or in the last decade, year, month, week, or day.\n",
    "\n",
    "Database contains 18 classification problems. Each of independent label variables contains seven classes: \"Never Used\", \"Used over a Decade Ago\", \"Used in Last Decade\", \"Used in Last Year\", \"Used in Last Month\", \"Used in Last Week\", and \"Used in Last Day\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Fit two multiclass classification models to predict two selected features out of 18. Use numerical values to represent each class. Report accuracy with the default parameters of each model.\n",
    "* Perform a simple manual optimization for one of the default parameters (at least 5 different values) for one of the previous models. Plot the new obtained accuracy as a function of the chosen parameter. \n",
    "* Fit one multiclass classification model for all the rest 16 features. Comment on the accuracy of predicting each feature, for all the seven classes. \n",
    "* Run one binary classification model for 3 features out of 18. Test the performance of the model by choosing as:\n",
    " - one class (class 0) the variable \"Used in Last Decade\" and the remaining variables for the other class (class 1).\n",
    " - one class (class 0) the variables \"Used in Last Decade\" and \"Used in Last Year\" and the remaining variables for the other class (class 1).\n",
    " - one class (class 0) the variables \"Used in Last Decade\", \"Used in Last Year\", \"Used in Last Month\" and the remaining variables for the other class (class 1).\n",
    " \n",
    " Comment your results, and point which selection of classes have better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_thats_all.jpg\" style=\"width:1000px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_metadata": {
   "author": "Leonardo Espinosa",
   "title": "MLPP: Supervised Learning"
  },
  "livereveal": {
   "overlay": "<div class='myheader'><h2 class='headertekst'>Machine Learning for Predictive Problems. Big Data Analytics Specialization <img src='images/arcada.png' width='200' height='50'></img></h2><h3 ><a href='#/21/1'>(index)</a></h3></div>",
   "progress": true,
   "scroll": true,
   "theme": "serif",
   "transition": "simple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
