{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Mining information from Text Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will explore and analyze the information stored in a particular dataset. In this case the ACL Anthology dataset (https://aclanthology.org/). We will explore different techniques for obtainingn valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Finding Similar Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select 1000 abstracts from the whole dataset. Find the similar items using pairwise Jaccard similarities, MinHash and LSH (vectorized versions) .\n",
    "\n",
    "   1. Compare the performance in time and the results for *k*-shingles = 3, 5 and 10, for the three methods and similarity thresholds *s*=0.9 and 0.95. Use 50 hashing functions. Comment your results. \n",
    "      \n",
    "   2. Compare the results obtained for MinHash and LSH for different similarity thresholds *s* = 0.5, 0.9 and 0.95  and 50, 100 and 200 hashing functions. Comment your results.\n",
    "   \n",
    "   3. For MinHashing using 100 hashing functions and *s* = 0.5 and 0.9,  find the Jaccard distances (1-Jaccard similarity) for all possible pairs. Use the obtained values within a k-NN algorithm, and for k=1,3 and, 5 identify the clusters with similar abstracts for each *s*. Describe the obtained clusters, are they different?. Select randomly at least 5 abstracts per cluster, upon visual inspection, what are the main topics?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@proceedings{woah-2021-online,\n",
      "    title = \"Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)\",\n",
      "    editor = \"Mostafazadeh Davani, Aida  and\n",
      "      Kiela, Douwe  and\n",
      "      Lambert, Mathias  and\n",
      "      Vidgen, Bertie  and\n",
      "      Prabhakaran, Vinodkumar  and\n",
      "      Waseem, Zeerak\",\n",
      "    month = aug,\n",
      "    year = \"2021\",\n",
      "    address = \"Online\",\n",
      "    publisher = \"Association for Computational Linguistics\",\n",
      "    url = \"https://aclanthology.org/2021.woah-1.0\",\n",
      "}\n",
      "@inproceedings{singh-li-2021-exploiting,\n",
      "    title = \"Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers\",\n",
      "    author = \"Singh, Sumer  and\n",
      "      Li, Sheng\",\n",
      "    booktitle = \"Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)\",\n",
      "    month = aug,\n",
      "    year = \"2021\",\n",
      "    address = \"Online\",\n",
      "    publisher = \"Association for Computational Linguistics\",\n",
      "    url = \"https://aclanthology.org/2021.woah-1.1\",\n",
      "    doi = \"10.18653/v1/2021.woah-1.1\",\n",
      "    pages = \"1--5\",\n",
      "    abstract = \"Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.\",\n",
      "}\n",
      "@inproceedings{hahn-etal-2021-modeling,\n",
      "    title = \"Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces\",\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "import gzip\n",
    "\n",
    "url = 'https://aclanthology.org/anthology+abstracts.bib.gz'\n",
    "with gzip.open(BytesIO (urlopen(url).read()), 'rb') as fb:\n",
    "    with open('anthology.bib', 'wb') as f:\n",
    "        f.write(fb.read())\n",
    "\n",
    "file = open('anthology.bib')\n",
    "for n in range (30):\n",
    "    print(file.readline()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "\n",
    "with open('anthology.bib') as bibtex_file:\n",
    "    bib_database = bibtexparser.bparser.BibTexParser(common_strings=True).parse_file(bibtex_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://aclanthology.org/2021.woah-1.0', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'editor': 'Mostafazadeh Davani, Aida  and\\nKiela, Douwe  and\\nLambert, Mathias  and\\nVidgen, Bertie  and\\nPrabhakaran, Vinodkumar  and\\nWaseem, Zeerak', 'title': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'ENTRYTYPE': 'proceedings', 'ID': 'woah-2021-online'}, {'abstract': 'Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.', 'pages': '1--5', 'doi': '10.18653/v1/2021.woah-1.1', 'url': 'https://aclanthology.org/2021.woah-1.1', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Singh, Sumer  and\\nLi, Sheng', 'title': 'Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers', 'ENTRYTYPE': 'inproceedings', 'ID': 'singh-li-2021-exploiting'}, {'abstract': 'Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.', 'pages': '6--16', 'doi': '10.18653/v1/2021.woah-1.2', 'url': 'https://aclanthology.org/2021.woah-1.2', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Hahn, Vanessa  and\\nRuiter, Dana  and\\nKleinbauer, Thomas  and\\nKlakow, Dietrich', 'title': 'Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces', 'ENTRYTYPE': 'inproceedings', 'ID': 'hahn-etal-2021-modeling'}, {'abstract': 'We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.', 'pages': '17--25', 'doi': '10.18653/v1/2021.woah-1.3', 'url': 'https://aclanthology.org/2021.woah-1.3', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': \"Caselli, Tommaso  and\\nBasile, Valerio  and\\nMitrovi{\\\\'c}, Jelena  and\\nGranitzer, Michael\", 'title': '{H}ate{BERT}: Retraining {BERT} for Abusive Language Detection in {E}nglish', 'ENTRYTYPE': 'inproceedings', 'ID': 'caselli-etal-2021-hatebert'}, {'abstract': \"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to {`}memes in the wild{'}. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that {`}memes in the wild{'} differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than {`}traditional memes{'}, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\", 'pages': '26--35', 'doi': '10.18653/v1/2021.woah-1.4', 'url': 'https://aclanthology.org/2021.woah-1.4', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Kirk, Hannah  and\\nJun, Yennie  and\\nRauba, Paulius  and\\nWachtel, Gal  and\\nLi, Ruining  and\\nBai, Xingjian  and\\nBroestl, Noah  and\\nDoff-Sotta, Martin  and\\nShtedritski, Aleksandar  and\\nAsano, Yuki M', 'title': 'Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset', 'ENTRYTYPE': 'inproceedings', 'ID': 'kirk-etal-2021-memes'}, {'abstract': 'Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.', 'pages': '36--53', 'doi': '10.18653/v1/2021.woah-1.5', 'url': 'https://aclanthology.org/2021.woah-1.5', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Kivlichan, Ian  and\\nLin, Zi  and\\nLiu, Jeremiah  and\\nVasserman, Lucy', 'title': 'Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation', 'ENTRYTYPE': 'inproceedings', 'ID': 'kivlichan-etal-2021-measuring'}, {'abstract': 'As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language. The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.', 'pages': '54--66', 'doi': '10.18653/v1/2021.woah-1.6', 'url': 'https://aclanthology.org/2021.woah-1.6', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Caselli, Tommaso  and\\nSchelhaas, Arjan  and\\nWeultjes, Marieke  and\\nLeistra, Folkert  and\\nvan der Veen, Hylke  and\\nTimmerman, Gerben  and\\nNissim, Malvina', 'title': '{DALC}: the {D}utch Abusive Language Corpus', 'ENTRYTYPE': 'inproceedings', 'ID': 'caselli-etal-2021-dalc'}, {'abstract': 'Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassment. Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression. Existing approaches to this problem are mostly available for resource-rich languages such as English and German. In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text. We also present experiments for detecting offensive language using supervised machine learning. Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic.', 'pages': '67--75', 'doi': '10.18653/v1/2021.woah-1.7', 'url': 'https://aclanthology.org/2021.woah-1.7', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Niraula, Nobal B.  and\\nDulal, Saurab  and\\nKoirala, Diwa', 'title': 'Offensive Language Detection in {N}epali Social Media', 'ENTRYTYPE': 'inproceedings', 'ID': 'niraula-etal-2021-offensive'}, {'abstract': 'Hate speech-related lexicons have been proved to be useful for many tasks such as data collection and classification. However, existing Portuguese lexicons do not distinguish between European and Brazilian Portuguese, and do not include neutral terms that are potentially useful to detect a broader spectrum of content referring to minorities. In this work, we present MIN{\\\\_}PT, a new European Portuguese Lexicon for Minorities-Related Terms specifically designed to tackle the limitations of existing resources. We describe the data collection and annotation process, discuss the limitation and ethical concerns, and prove the utility of the resource by applying it to a use case for the Portuguese 2021 presidential elections.', 'pages': '76--80', 'doi': '10.18653/v1/2021.woah-1.8', 'url': 'https://aclanthology.org/2021.woah-1.8', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': \"Fortuna, Paula  and\\nCortez, Vanessa  and\\nSozinho Ramalho, Miguel  and\\nP{\\\\'e}rez-Mayos, Laura\", 'title': '{MIN}{\\\\_}{PT}: An {E}uropean {P}ortuguese Lexicon for Minorities Related Terms', 'ENTRYTYPE': 'inproceedings', 'ID': 'fortuna-etal-2021-min'}, {'abstract': 'Current abusive language detection systems have demonstrated unintended bias towards sensitive features such as nationality or gender. This is a crucial issue, which may harm minorities and underrepresented groups if such systems were integrated in real-world applications. In this paper, we create ad hoc tests through the CheckList tool (Ribeiro et al., 2020) to detect biases within abusive language classifiers for English. We compare the behaviour of two BERT-based models, one trained on a generic hate speech dataset and the other on a dataset for misogyny detection. Our evaluation shows that, although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation. We release both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList.', 'pages': '81--91', 'doi': '10.18653/v1/2021.woah-1.9', 'url': 'https://aclanthology.org/2021.woah-1.9', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Manerba, Marta Marchiori  and\\nTonelli, Sara', 'title': 'Fine-Grained Fairness Analysis of Abusive Language Detection Systems with {C}heck{L}ist', 'ENTRYTYPE': 'inproceedings', 'ID': 'manerba-tonelli-2021-fine'}, {'abstract': \"Bias mitigation approaches reduce models{'} dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In hate speech detection, however, equalizing model predictions may ignore important differences among targeted social groups, as hate speech can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pre-trained language models) among counterfactuals, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.\", 'pages': '92--101', 'doi': '10.18653/v1/2021.woah-1.10', 'url': 'https://aclanthology.org/2021.woah-1.10', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Mostafazadeh Davani, Aida  and\\nOmrani, Ali  and\\nKennedy, Brendan  and\\nAtari, Mohammad  and\\nRen, Xiang  and\\nDehghani, Morteza', 'title': 'Improving Counterfactual Generation for Fair Hate Speech Detection', 'ENTRYTYPE': 'inproceedings', 'ID': 'mostafazadeh-davani-etal-2021-improving'}, {'abstract': \"There have been several attempts to create an accurate and thorough emotion lexicon in English, which identifies the emotional content of words. Of the several commonly used resources, the NRC emotion lexicon (Mohammad and Turney, 2013b) has received the most attention due to its availability, size, and its choice of Plutchik{'}s expressive 8-class emotion model. In this paper we identify a large number of troubling entries in the NRC lexicon, where words that should in most contexts be emotionally neutral, with no affect (e.g., {`}lesbian{'}, {`}stone{'}, {`}mountain{'}), are associated with emotional labels that are inaccurate, nonsensical, pejorative, or, at best, highly contingent and context-dependent (e.g., {`}lesbian{'} labeled as Disgust and Sadness, {`}stone{'} as Anger, or {`}mountain{'} as Anticipation). We describe a procedure for semi-automatically correcting these problems in the NRC, which includes disambiguating POS categories and aligning NRC entries with other emotion lexicons to infer the accuracy of labels. We demonstrate via an experimental benchmark that the quality of the resources is thus improved. We release the revised resource and our code to enable other researchers to reproduce and build upon results.\", 'pages': '102--113', 'doi': '10.18653/v1/2021.woah-1.11', 'url': 'https://aclanthology.org/2021.woah-1.11', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Zad, Samira  and\\nJimenez, Joshuan  and\\nFinlayson, Mark', 'title': 'Hell Hath No Fury? Correcting Bias in the {NRC} Emotion Lexicon', 'ENTRYTYPE': 'inproceedings', 'ID': 'zad-etal-2021-hell'}, {'abstract': 'Automatic detection of toxic language plays an essential role in protecting social media users, especially minority groups, from verbal abuse. However, biases toward some attributes, including gender, race, and dialect, exist in most training datasets for toxicity detection. The biases make the learned models unfair and can even exacerbate the marginalization of people. Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (InvRat), a game-theoretic framework consisting of a rationale generator and a predictor, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels. We empirically show that our method yields lower false positive rate in both lexical and dialectal attributes than previous debiasing methods.', 'pages': '114--120', 'doi': '10.18653/v1/2021.woah-1.12', 'url': 'https://aclanthology.org/2021.woah-1.12', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Chuang, Yung-Sung  and\\nGao, Mingye  and\\nLuo, Hongyin  and\\nGlass, James  and\\nLee, Hung-yi  and\\nChen, Yun-Nung  and\\nLi, Shang-Wen', 'title': 'Mitigating Biases in Toxic Language Detection through Invariant Rationalization', 'ENTRYTYPE': 'inproceedings', 'ID': 'chuang-etal-2021-mitigating'}, {'abstract': 'We present a data set consisting of German news articles labeled for political bias on a five-point scale in a semi-supervised way. While earlier work on hyperpartisan news detection uses binary classification (i.e., hyperpartisan or not) and English data, we argue for a more fine-grained classification, covering the full political spectrum (i.e., far-left, left, centre, right, far-right) and for extending research to German data. Understanding political bias helps in accurately detecting hate speech and online abuse. We experiment with different classification methods for political bias detection. Their comparatively low performance (a macro-F1 of 43 for our best setup, compared to a macro-F1 of 79 for the binary classification task) underlines the need for more (balanced) data annotated in a fine-grained way.', 'pages': '121--131', 'doi': '10.18653/v1/2021.woah-1.13', 'url': 'https://aclanthology.org/2021.woah-1.13', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Aksenov, Dmitrii  and\\nBourgonje, Peter  and\\nZaczynska, Karolina  and\\nOstendorff, Malte  and\\nMoreno-Schneider, Julian  and\\nRehm, Georg', 'title': 'Fine-grained Classification of Political Bias in {G}erman News: A Data Set and Initial Experiments', 'ENTRYTYPE': 'inproceedings', 'ID': 'aksenov-etal-2021-fine'}, {'abstract': \"Online abuse and offensive language on social media have become widespread problems in today{'}s digital age. In this paper, we contribute a Reddit-based dataset, consisting of 68,159 insults and 51,102 compliments targeted at individuals instead of targeting a particular community or race. Secondly, we benchmark multiple existing state-of-the-art models for both classification and unsupervised style transfer on the dataset. Finally, we analyse the experimental results and conclude that the transfer task is challenging, requiring the models to understand the high degree of creativity exhibited in the data.\", 'pages': '132--139', 'doi': '10.18653/v1/2021.woah-1.14', 'url': 'https://aclanthology.org/2021.woah-1.14', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Sodhi, Ravsimar  and\\nPant, Kartikey  and\\nMamidi, Radhika', 'title': 'Jibes {\\\\&} Delights: A Dataset of Targeted Insults and Compliments to Tackle Online Abuse', 'ENTRYTYPE': 'inproceedings', 'ID': 'sodhi-etal-2021-jibes'}, {'abstract': 'User posts whose perceived toxicity depends on the conversational context are rare in current toxicity detection datasets. Hence, toxicity detectors trained on current datasets will also disregard context, making the detection of context-sensitive toxicity a lot harder when it occurs. We constructed and publicly release a dataset of 10k posts with two kinds of toxicity labels per post, obtained from annotators who considered (i) both the current post and the previous one as context, or (ii) only the current post. We introduce a new task, context-sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. Using the new dataset, we show that systems can be developed for this task. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce additional costs.', 'pages': '140--145', 'doi': '10.18653/v1/2021.woah-1.15', 'url': 'https://aclanthology.org/2021.woah-1.15', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Xenos, Alexandros  and\\nPavlopoulos, John  and\\nAndroutsopoulos, Ion', 'title': 'Context Sensitivity Estimation in Toxicity Detection', 'ENTRYTYPE': 'inproceedings', 'ID': 'xenos-etal-2021-context'}, {'abstract': \"In this paper, we introduce a new English Twitter-based dataset for cyberbullying detection and online abuse. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, trolling, profanity, sarcasm, threat, porn and exclusion. We recruited a pool of 17 annotators to perform fine-grained annotation on the dataset with each tweet annotated by three annotators. All our annotators are high school educated and frequent users of social media. Inter-rater agreement for the dataset as measured by Krippendorff{'}s Alpha is 0.67. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer-based deep learning models returning impressive results.\", 'pages': '146--156', 'doi': '10.18653/v1/2021.woah-1.16', 'url': 'https://aclanthology.org/2021.woah-1.16', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Salawu, Semiu  and\\nLumsden, Jo  and\\nHe, Yulan', 'title': 'A Large-Scale {E}nglish Multi-Label {T}witter Dataset for Cyberbullying and Online Abuse Detection', 'ENTRYTYPE': 'inproceedings', 'ID': 'salawu-etal-2021-large'}, {'abstract': 'With the rise of research on toxic comment classification, more and more annotated datasets have been released. The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings. Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset. They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects. To overcome these issues, we present a collection of more than thirty datasets in the form of a software tool that automatizes downloading and processing of the data and presents them in a unified data format that also offers a mapping of compatible class labels. Another advantage of that tool is that it gives an overview of properties of available datasets, such as different languages, platforms, and class labels to make it easier to select suitable training and test data.', 'pages': '157--163', 'doi': '10.18653/v1/2021.woah-1.17', 'url': 'https://aclanthology.org/2021.woah-1.17', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Risch, Julian  and\\nSchmidt, Philipp  and\\nKrestel, Ralf', 'title': 'Data Integration for Toxic Comment Classification: Making More Than 40 Datasets Easily Accessible in One Unified Format', 'ENTRYTYPE': 'inproceedings', 'ID': 'risch-etal-2021-toxic'}, {'abstract': 'Community-level bans are a common tool against groups that enable online harassment and harmful speech. Unfortunately, the efficacy of community bans has only been partially studied and with mixed results. Here, we provide a flexible unsupervised methodology to identify in-group language and track user activity on Reddit both before and after the ban of a community (subreddit). We use a simple word frequency divergence to identify uncommon words overrepresented in a given community, not as a proxy for harmful speech but as a linguistic signature of the community. We apply our method to 15 banned subreddits, and find that community response is heterogeneous between subreddits and between users of a subreddit. Top users were more likely to become less active overall, while random users often reduced use of in-group language without decreasing activity. Finally, we find some evidence that the effectiveness of bans aligns with the content of a community. Users of dark humor communities were largely unaffected by bans while users of communities organized around white supremacy and fascism were the most affected. Altogether, our results show that bans do not affect all groups or users equally, and pave the way to understanding the effect of bans across communities.', 'pages': '164--178', 'doi': '10.18653/v1/2021.woah-1.18', 'url': 'https://aclanthology.org/2021.woah-1.18', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': \"Trujillo, Milo  and\\nRosenblatt, Sam  and\\nde Anda J{\\\\'a}uregui, Guillermo  and\\nMoog, Emily  and\\nSamson, Briane Paul V.  and\\nH{\\\\'e}bert-Dufresne, Laurent  and\\nRoth, Allison M.\", 'title': 'When the Echo Chamber Shatters: Examining the Use of Community-Specific Language Post-Subreddit Ban', 'ENTRYTYPE': 'inproceedings', 'ID': 'trujillo-etal-2021-echo'}, {'abstract': 'Mainstream research on hate speech focused so far predominantly on the task of classifying mainly social media posts with respect to predefined typologies of rather coarse-grained hate speech categories. This may be sufficient if the goal is to detect and delete abusive language posts. However, removal is not always possible due to the legislation of a country. Also, there is evidence that hate speech cannot be successfully combated by merely removing hate speech posts; they should be countered by education and counter-narratives. For this purpose, we need to identify (i) who is the target in a given hate speech post, and (ii) what aspects (or characteristics) of the target are attributed to the target in the post. As the first approximation, we propose to adapt a generic state-of-the-art concept extraction model to the hate speech domain. The outcome of the experiments is promising and can serve as inspiration for further work on the task', 'pages': '179--190', 'doi': '10.18653/v1/2021.woah-1.19', 'url': 'https://aclanthology.org/2021.woah-1.19', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Shvets, Alexander  and\\nFortuna, Paula  and\\nSoler, Juan  and\\nWanner, Leo', 'title': 'Targets and Aspects in Social Media Hate Speech', 'ENTRYTYPE': 'inproceedings', 'ID': 'shvets-etal-2021-targets'}, {'abstract': 'Abusive language is a growing phenomenon on social media platforms. Its effects can reach beyond the online context, contributing to mental or emotional stress on users. Automatic tools for detecting abuse can alleviate the issue. In practice, developing automated methods to detect abusive language relies on good quality data. However, there is currently a lack of standards for creating datasets in the field. These standards include definitions of what is considered abusive language, annotation guidelines and reporting on the process. This paper introduces an annotation framework inspired by legal concepts to define abusive language in the context of online harassment. The framework uses a 7-point Likert scale for labelling instead of class labels. We also present ALYT {--} a dataset of Abusive Language on YouTube. ALYT includes YouTube comments in English extracted from videos on different controversial topics and labelled by Law students. The comments were sampled from the actual collected data, without artificial methods for increasing the abusive content. The paper describes the annotation process thoroughly, including all its guidelines and training steps.', 'pages': '191--200', 'doi': '10.18653/v1/2021.woah-1.20', 'url': 'https://aclanthology.org/2021.woah-1.20', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Bertaglia, Thales  and\\nGrigoriu, Andreea  and\\nDumontier, Michel  and\\nvan Dijck, Gijs', 'title': 'Abusive Language on Social Media Through the Legal Looking Glass', 'ENTRYTYPE': 'inproceedings', 'ID': 'bertaglia-etal-2021-abusive'}, {'abstract': 'We present the results and main findings of the shared task at WOAH 5 on hateful memes detection. The task include two subtasks relating to distinct challenges in the fine-grained detection of hateful memes: (1) the protected category attacked by the meme and (2) the attack type. 3 teams submitted system description papers. This shared task builds on the hateful memes detection task created by Facebook AI Research in 2020.', 'pages': '201--206', 'doi': '10.18653/v1/2021.woah-1.21', 'url': 'https://aclanthology.org/2021.woah-1.21', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Mathias, Lambert  and\\nNie, Shaoliang  and\\nMostafazadeh Davani, Aida  and\\nKiela, Douwe  and\\nPrabhakaran, Vinodkumar  and\\nVidgen, Bertie  and\\nWaseem, Zeerak', 'title': 'Findings of the {WOAH} 5 Shared Task on Fine Grained Hateful Memes Detection', 'ENTRYTYPE': 'inproceedings', 'ID': 'mathias-etal-2021-findings'}, {'abstract': 'This paper describes our submission (winning solution for Task A) to the Shared Task on Hateful Meme Detection at WOAH 2021. We build our system on top of a state-of-the-art system for binary hateful meme classification that already uses image tags such as race, gender, and web entities. We add further metadata such as emotions and experiment with data augmentation techniques, as hateful instances are underrepresented in the data set.', 'pages': '207--214', 'doi': '10.18653/v1/2021.woah-1.22', 'url': 'https://aclanthology.org/2021.woah-1.22', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Aggarwal, Piush  and\\nLiman, Michelle Espranita  and\\nGold, Darina  and\\nZesch, Torsten', 'title': '{VL}-{BERT}+: Detecting Protected Groups in Hateful Multimodal Memes', 'ENTRYTYPE': 'inproceedings', 'ID': 'aggarwal-etal-2021-vl'}, {'abstract': 'Memes are the combinations of text and images that are often humorous in nature. But, that may not always be the case, and certain combinations of texts and images may depict hate, referred to as hateful memes. This work presents a multimodal pipeline that takes both visual and textual features from memes into account to (1) identify the protected category (e.g. race, sex etc.) that has been attacked; and (2) detect the type of attack (e.g. contempt, slurs etc.). Our pipeline uses state-of-the-art pre-trained visual and textual representations, followed by a simple logistic regression classifier. We employ our pipeline on the Hateful Memes Challenge dataset with additional newly created fine-grained labels for protected category and type of attack. Our best model achieves an AUROC of 0.96 for identifying the protected category, and 0.97 for detecting the type of attack. We release our code at https://github.com/harisbinzia/HatefulMemes', 'pages': '215--219', 'doi': '10.18653/v1/2021.woah-1.23', 'url': 'https://aclanthology.org/2021.woah-1.23', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Zia, Haris Bin  and\\nCastro, Ignacio  and\\nTyson, Gareth', 'title': 'Racist or Sexist Meme? Classifying Memes beyond Hateful', 'ENTRYTYPE': 'inproceedings', 'ID': 'zia-etal-2021-racist'}, {'abstract': 'The Shared Task on Hateful Memes is a challenge that aims at the detection of hateful content in memes by inviting the implementation of systems that understand memes, potentially by combining image and textual information. The challenge consists of three detection tasks: hate, protected category and attack type. The first is a binary classification task, while the other two are multi-label classification tasks. Our participation included a text-based BERT baseline (TxtBERT), the same but adding information from the image (ImgBERT), and neural retrieval approaches. We also experimented with retrieval augmented classification models. We found that an ensemble of TxtBERT and ImgBERT achieves the best performance in terms of ROC AUC score in two out of the three tasks on our development set.', 'pages': '220--225', 'doi': '10.18653/v1/2021.woah-1.24', 'url': 'https://aclanthology.org/2021.woah-1.24', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'August', 'booktitle': 'Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)', 'author': 'Kougia, Vasiliki  and\\nPavlopoulos, John', 'title': 'Multimodal or Text? Retrieval or {BERT}? Benchmarking Classifiers for the Shared Task on Hateful Memes', 'ENTRYTYPE': 'inproceedings', 'ID': 'kougia-pavlopoulos-2021-multimodal'}, {'url': 'https://aclanthology.org/2021.wnut-1.0', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'editor': 'Xu, Wei  and\\nRitter, Alan  and\\nBaldwin, Tim  and\\nRahimi, Afshin', 'title': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'ENTRYTYPE': 'proceedings', 'ID': 'wnut-2021-noisy'}, {'abstract': 'Text simplification is the process of splitting and rephrasing a sentence to a sequence of sentences making it easier to read and understand while preserving the content and approximating the original meaning. Text simplification has been exploited in NLP applications like machine translation, summarization, semantic role labeling, and information extraction, opening a broad avenue for its exploitation in comprehension-based question-answering downstream tasks. In this work, we investigate the effect of text simplification in the task of question-answering using a comprehension context. We release Simple-SQuAD, a simplified version of the widely-used SQuAD dataset. Firstly, we outline each step in the dataset creation pipeline, including style transfer, thresholding of sentences showing correct transfer, and offset finding for each answer. Secondly, we verify the quality of the transferred sentences through various methodologies involving both automated and human evaluation. Thirdly, we benchmark the newly created corpus and perform an ablation study for examining the effect of the simplification process in the SQuAD-based question answering task. Our experiments show that simplification leads to up to 2.04{\\\\%} and 1.74{\\\\%} increase in Exact Match and F1, respectively. Finally, we conclude with an analysis of the transfer process, investigating the types of edits made by the model, and the effect of sentence length on the transfer model.', 'pages': '1--10', 'doi': '10.18653/v1/2021.wnut-1.1', 'url': 'https://aclanthology.org/2021.wnut-1.1', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Dadu, Tanvi  and\\nPant, Kartikey  and\\nNagar, Seema  and\\nBarbhuiya, Ferdous  and\\nDey, Kuntal', 'title': 'Text Simplification for Comprehension-based Question-Answering', 'ENTRYTYPE': 'inproceedings', 'ID': 'dadu-etal-2021-text'}, {'abstract': 'Finding informative COVID-19 posts in a stream of tweets is very useful to monitor health-related updates. Prior work focused on a balanced data setup and on English, but informative tweets are rare, and English is only one of the many languages spoken in the world. In this work, we introduce a new dataset of 5,000 tweets for finding informative COVID-19 tweets for Danish. In contrast to prior work, which balances the label distribution, we model the problem by keeping its natural distribution. We examine how well a simple probabilistic model and a convolutional neural network (CNN) perform on this task. We find a weighted CNN to work well but it is sensitive to embedding and hyperparameter choices. We hope the contributed dataset is a starting point for further work in this direction.', 'pages': '11--19', 'doi': '10.18653/v1/2021.wnut-1.2', 'url': 'https://aclanthology.org/2021.wnut-1.2', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Olsen, Benjamin  and\\nPlank, Barbara', 'title': 'Finding the needle in a haystack: Extraction of Informative {COVID}-19 {D}anish Tweets', 'ENTRYTYPE': 'inproceedings', 'ID': 'olsen-plank-2021-finding'}, {'abstract': 'We present the first openly available corpus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based models. We achieve a 77.53{\\\\%} accuracy with a Thai BERT model in detecting depression. This establishes a good baseline for future researcher on the same corpus. Furthermore, we identify a need for Thai embeddings that have been trained on a more varied corpus than Wikipedia. Our corpus, code and trained models have been released openly on Zenodo.', 'pages': '20--25', 'doi': '10.18653/v1/2021.wnut-1.3', 'url': 'https://aclanthology.org/2021.wnut-1.3', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'H{\\\\\"a}m{\\\\\"a}l{\\\\\"a}inen, Mika  and\\nPatpong, Pattama  and\\nAlnajjar, Khalid  and\\nPartanen, Niko  and\\nRueter, Jack', 'title': 'Detecting Depression in {T}hai Blog Posts: a Dataset and a Baseline', 'ENTRYTYPE': 'inproceedings', 'ID': 'hamalainen-etal-2021-detecting'}, {'abstract': 'Extracting keyphrases that summarize the main points of a document is a fundamental task in natural language processing. Supervised approaches to keyphrase extraction(KPE) are largely developed based on the assumption that the training data is fully annotated. However, due to the difficulty of keyphrase annotating, KPE models severely suffer from incomplete annotated problem in many scenarios. To this end, we propose a more robust training method that learns to mitigate the misguidance brought by unlabeled keyphrases. We introduce negative sampling to adjust training loss, and conduct experiments under different scenarios. Empirical studies on synthetic datasets and open domain dataset show that our model is robust to incomplete annotated problem and surpasses prior baselines. Extensive experiments on five scientific domain datasets of different scales demonstrate that our model is competitive with the state-of-the-art method.', 'pages': '26--34', 'doi': '10.18653/v1/2021.wnut-1.4', 'url': 'https://aclanthology.org/2021.wnut-1.4', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Lei, Yanfei  and\\nHu, Chunming  and\\nMa, Guanghui  and\\nZhang, Richong', 'title': 'Keyphrase Extraction with Incomplete Annotated Training Data', 'ENTRYTYPE': 'inproceedings', 'ID': 'lei-etal-2021-keyphrase'}, {'abstract': 'Fine-grained temporal relation extraction (FineTempRel) aims to recognize the durations and timeline of event mentions in text. A missing part in the current deep learning models for FineTempRel is their failure to exploit the syntactic structures of the input sentences to enrich the representation vectors. In this work, we propose to fill this gap by introducing novel methods to integrate the syntactic structures into the deep learning models for FineTempRel. The proposed model focuses on two types of syntactic information from the dependency trees, i.e., the syntax-based importance scores for representation learning of the words and the syntactic connections to identify important context words for the event mentions. We also present two novel techniques to facilitate the knowledge transfer between the subtasks of FineTempRel, leading to a novel model with the state-of-the-art performance for this task.', 'pages': '35--45', 'doi': '10.18653/v1/2021.wnut-1.5', 'url': 'https://aclanthology.org/2021.wnut-1.5', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Tran Phu, Minh  and\\nNguyen, Minh Van  and\\nNguyen, Thien Huu', 'title': 'Fine-grained Temporal Relation Extraction with Ordered-Neuron {LSTM} and Graph Convolutional Networks', 'ENTRYTYPE': 'inproceedings', 'ID': 'tran-phu-etal-2021-fine'}, {'abstract': 'The goal of Event Factuality Prediction (EFP) is to determine the factual degree of an event mention, representing how likely the event mention has happened in text. Current deep learning models has demonstrated the importance of syntactic and semantic structures of the sentences to identify important context words for EFP. However, the major problem with these EFP models is that they only encode the one-hop paths between the words (i.e., the direct connections) to form the sentence structures. In this work, we show that the multi-hop paths between the words are also necessary to compute the sentence structures for EFP. To this end, we introduce a novel deep learning model for EFP that explicitly considers multi-hop paths with both syntax-based and semantic-based edges between the words to obtain sentence structures for representation learning in EFP. We demonstrate the effectiveness of the proposed model via the extensive experiments in this work.', 'pages': '46--55', 'doi': '10.18653/v1/2021.wnut-1.6', 'url': 'https://aclanthology.org/2021.wnut-1.6', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Le, Duong  and\\nNguyen, Thien Huu', 'title': 'Does It Happen? Multi-hop Path Structures for Event Factuality Prediction with Graph Transformer Networks', 'ENTRYTYPE': 'inproceedings', 'ID': 'le-nguyen-2021-happen'}, {'abstract': \"WARNING: This article contains contents that may offend the readers. Strategies that insert intentional noise into text when posting it are commonly observed in the online space, and sometimes they aim to let only certain community users understand the genuine semantics. In this paper, we explore the purpose of such actions by categorizing them into tricks, memes, fillers, and codes, and organize the linguistic strategies that are used for each purpose. Through this, we identify that such strategies can be conducted by authors for multiple purposes, regarding the presence of stakeholders such as {`}Peers{'} and {`}Others{'}. We finally analyze how these strategies appear differently in each circumstance, along with the unified taxonomy accompanying examples.\", 'pages': '56--61', 'doi': '10.18653/v1/2021.wnut-1.7', 'url': 'https://aclanthology.org/2021.wnut-1.7', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Cho, Won Ik  and\\nKim, Soomin', 'title': '{G}oogle-trickers, Yaminjeongeum, and Leetspeak: An Empirical Taxonomy for Intentionally Noisy User-Generated Text', 'ENTRYTYPE': 'inproceedings', 'ID': 'cho-kim-2021-google'}, {'abstract': \"ICD-9 coding is a relevant clinical billing task, where unstructured texts with information about a patient{'}s diagnosis and treatments are annotated with multiple ICD-9 codes. Automated ICD-9 coding is an active research field, where CNN- and RNN-based model architectures represent the state-of-the-art approaches. In this work, we propose a description-based label attention classifier to improve the model explainability when dealing with noisy texts like clinical notes.\", 'pages': '62--66', 'doi': '10.18653/v1/2021.wnut-1.8', 'url': 'https://aclanthology.org/2021.wnut-1.8', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Feucht, Malte  and\\nWu, Zhiliang  and\\nAlthammer, Sophia  and\\nTresp, Volker', 'title': 'Description-based Label Attention Classifier for Explainable {ICD}-9 Classification', 'ENTRYTYPE': 'inproceedings', 'ID': 'feucht-etal-2021-description'}, {'abstract': 'Lexical normalization, in addition to word segmentation and part-of-speech tagging, is a fundamental task for Japanese user-generated text processing. In this paper, we propose a text editing model to solve the three task jointly and methods of pseudo-labeled data generation to overcome the problem of data deficiency. Our experiments showed that the proposed model achieved better normalization performance when trained on more diverse pseudo-labeled data.', 'pages': '67--80', 'doi': '10.18653/v1/2021.wnut-1.9', 'url': 'https://aclanthology.org/2021.wnut-1.9', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Higashiyama, Shohei  and\\nUtiyama, Masao  and\\nWatanabe, Taro  and\\nSumita, Eiichiro', 'title': 'A Text Editing Approach to Joint {J}apanese Word Segmentation, {POS} Tagging, and Lexical Normalization', 'ENTRYTYPE': 'inproceedings', 'ID': 'higashiyama-etal-2021-text'}, {'abstract': 'Language models used in speech recognition are often either evaluated intrinsically using perplexity on test data, or extrinsically with an automatic speech recognition (ASR) system. The former evaluation does not always correlate well with ASR performance, while the latter could be specific to particular ASR systems. Recent work proposed to evaluate language models by using them to classify ground truth sentences among alternative phonetically similar sentences generated by a fine state transducer. Underlying such an evaluation is the assumption that the generated sentences are linguistically incorrect. In this paper, we first put this assumption into question, and observe that alternatively generated sentences could often be linguistically correct when they differ from the ground truth by only one edit. Secondly, we showed that by using multi-lingual BERT, we can achieve better performance than previous work on two code-switching data sets. Our implementation is publicly available on Github at https://github.com/sikfeng/language-modelling-for-code-switching.', 'pages': '81--86', 'doi': '10.18653/v1/2021.wnut-1.10', 'url': 'https://aclanthology.org/2021.wnut-1.10', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Cheong, Sik Feng  and\\nChieu, Hai Leong  and\\nLim, Jing', 'title': 'Intrinsic evaluation of language models for code-switching', 'ENTRYTYPE': 'inproceedings', 'ID': 'cheong-etal-2021-intrinsic'}, {'abstract': \"Multimodal named entity recognition (MNER) requires to bridge the gap between language understanding and visual context. While many multimodal neural techniques have been proposed to incorporate images into the MNER task, the model{'}s ability to leverage multimodal interactions remains poorly understood. In this work, we conduct in-depth analyses of existing multimodal fusion techniques from different perspectives and describe the scenarios where adding information from the image does not always boost performance. We also study the use of captions as a way to enrich the context for MNER. Experiments on three datasets from popular social platforms expose the bottleneck of existing multimodal models and the situations where using captions is beneficial.\", 'pages': '87--96', 'doi': '10.18653/v1/2021.wnut-1.11', 'url': 'https://aclanthology.org/2021.wnut-1.11', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Chen, Shuguang  and\\nAguilar, Gustavo  and\\nNeves, Leonardo  and\\nSolorio, Thamar', 'title': 'Can images help recognize entities? A study of the role of images for Multimodal {NER}', 'ENTRYTYPE': 'inproceedings', 'ID': 'chen-etal-2021-images'}, {'abstract': \"Existing sarcasm detection systems focus on exploiting linguistic markers, context, or user-level priors. However, social studies suggest that the relationship between the author and the audience can be equally relevant for the sarcasm usage and interpretation. In this work, we propose a framework jointly leveraging (1) a user context from their historical tweets together with (2) the social information from a user{'}s conversational neighborhood in an interaction graph, to contextualize the interpretation of the post. We use graph attention networks (GAT) over users and tweets in a conversation thread, combined with dense user history representations. Apart from achieving state-of-the-art results on the recently published dataset of 19k Twitter users with 30K labeled tweets, adding 10M unlabeled tweets as context, our results indicate that the model contributes to interpreting the sarcastic intentions of an author more than to predicting the sarcasm perception by others.\", 'pages': '97--105', 'doi': '10.18653/v1/2021.wnut-1.12', 'url': 'https://aclanthology.org/2021.wnut-1.12', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Plepi, Joan  and\\nFlek, Lucie', 'title': 'Perceived and Intended Sarcasm Detection with Graph Attention Networks', 'ENTRYTYPE': 'inproceedings', 'ID': 'plepi-flek-2021-perceived'}, {'abstract': 'State-of-the-art approaches to spelling error correction problem include Transformer-based Seq2Seq models, which require large training sets and suffer from slow inference time; and sequence labeling models based on Transformer encoders like BERT, which involve token-level label space and therefore a large pre-defined vocabulary dictionary. In this paper we present a Hierarchical Character Tagger model, or HCTagger, for short text spelling error correction. We use a pre-trained language model at the character level as a text encoder, and then predict character-level edits to transform the original text into its error-free form with a much smaller label space. For decoding, we propose a hierarchical multi-task approach to alleviate the issue of long-tail label distribution without introducing extra model parameters. Experiments on two public misspelling correction datasets demonstrate that HCTagger is an accurate and much faster approach than many existing models.', 'pages': '106--113', 'doi': '10.18653/v1/2021.wnut-1.13', 'url': 'https://aclanthology.org/2021.wnut-1.13', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Gao, Mengyi  and\\nXu, Canran  and\\nShi, Peng', 'title': 'Hierarchical Character Tagger for Short Text Spelling Error Correction', 'ENTRYTYPE': 'inproceedings', 'ID': 'gao-etal-2021-hierarchical'}, {'abstract': \"Large-scale language models such as ELMo and BERT have pushed the horizon of what is possible in semantic role labeling (SRL), solving the out-of-vocabulary problem and enabling end-to-end systems, but they have also introduced significant biases. We evaluate three SRL parsers on very simple transitive sentences with verbs usually associated with animate subjects and objects, such as, {``}Mary babysat Tom{''}: a state-of-the-art parser based on BERT, an older parser based on GloVe, and an even older parser from before the days of word embeddings. When arguments are word forms predominantly used as person names, aligning with common sense expectations of animacy, the BERT-based parser is unsurprisingly superior; yet, with abstract or random nouns, the opposite picture emerges. We refer to this as {``}common sense bias{''} and present a challenge dataset for evaluating the extent to which parsers are sensitive to such a bias. Our code and challenge dataset are available here: github.com/coastalcph/comte\", 'pages': '114--119', 'doi': '10.18653/v1/2021.wnut-1.14', 'url': 'https://aclanthology.org/2021.wnut-1.14', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Lent, Heather  and\\nS{\\\\o}gaard, Anders', 'title': 'Common Sense Bias in Semantic Role Labeling', 'ENTRYTYPE': 'inproceedings', 'ID': 'lent-sogaard-2021-common'}, {'abstract': 'WhatsApp Messenger is one of the most popular channels for spreading information with a current reach of more than 180 countries and 2 billion people. Its widespread usage has made it one of the most popular media for information propagation among the masses during any socially engaging event. In the recent past, several countries have witnessed its effectiveness and influence in political and social campaigns. We observe a high surge in information and propaganda flow during election campaigning. In this paper, we explore a high-quality large-scale user-generated dataset curated from WhatsApp comprising of 281 groups, 31,078 unique users, and 223,404 messages shared before, during, and after the Indian General Elections 2019, encompassing all major Indian political parties and leaders. In addition to the raw noisy user-generated data, we present a fine-grained annotated dataset of 3,848 messages that will be useful to understand the various dimensions of WhatsApp political campaigning. We present several complementary insights into the investigative and sensational news stories from the same period. Exploratory data analysis and experiments showcase several exciting results and future research opportunities. To facilitate reproducible research, we make the anonymized datasets available in the public domain.', 'pages': '120--130', 'doi': '10.18653/v1/2021.wnut-1.15', 'url': 'https://aclanthology.org/2021.wnut-1.15', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Srivastava, Vivek  and\\nSingh, Mayank', 'title': '{P}oli{WAM}: An Exploration of a Large Scale Corpus of Political Discussions on {W}hats{A}pp Messenger', 'ENTRYTYPE': 'inproceedings', 'ID': 'srivastava-singh-2021-poliwam'}, {'abstract': \"As a result of unstructured sentences and some misspellings and errors, finding named entities in a noisy environment such as social media takes much more effort. ParsTwiNER contains about 250k tokens, based on standard instructions like MUC-6 or CoNLL 2003, gathered from Persian Twitter. Using Cohen{'}s Kappa coefficient, the consistency of annotators is 0.95, a high score. In this study, we demonstrate that some state-of-the-art models degrade on these corpora, and trained a new model using parallel transfer learning based on the BERT architecture. Experimental results show that the model works well in informal Persian as well as in formal Persian.\", 'pages': '131--136', 'doi': '10.18653/v1/2021.wnut-1.16', 'url': 'https://aclanthology.org/2021.wnut-1.16', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Aghajani, MohammadMahdi  and\\nBadri, AliAkbar  and\\nBeigy, Hamid', 'title': '{P}ars{T}wi{NER}: A Corpus for Named Entity Recognition at Informal {P}ersian', 'ENTRYTYPE': 'inproceedings', 'ID': 'aghajani-etal-2021-parstwiner'}, {'abstract': \"We present DreamDrug, a crowdsourced dataset for detecting mentions of drugs in noisy user-generated item listings from darknet markets. Our dataset contains nearly 15,000 manually annotated drug entities in over 3,500 item listings scraped from the darknet market platform {``}DreamMarket{''} in 2017. We also train and evaluate baseline models for detecting these entities, using contextual language models fine-tuned in a few-shot setting and on the full dataset, and examine the effect of pretraining on in-domain unannotated corpora.\", 'pages': '137--157', 'doi': '10.18653/v1/2021.wnut-1.17', 'url': 'https://aclanthology.org/2021.wnut-1.17', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': \"Bogensperger, Johannes  and\\nSchlarb, Sven  and\\nHanbury, Allan  and\\nRecski, G{\\\\'a}bor\", 'title': '{D}ream{D}rug - A crowdsourced {NER} dataset for detecting drugs in darknet markets', 'ENTRYTYPE': 'inproceedings', 'ID': 'bogensperger-etal-2021-dreamdrug'}, {'abstract': 'Code-mixed text generation systems have found applications in many downstream tasks, including speech recognition, translation and dialogue. A paradigm of these generation systems relies on well-defined grammatical theories of code-mixing, and there is a lack of comparison of these theories. We present a large-scale human evaluation of two popular grammatical theories, Matrix-Embedded Language (ML) and Equivalence Constraint (EC). We compare them against three heuristic-based models and quantitatively demonstrate the effectiveness of the two grammatical theories.', 'pages': '158--167', 'doi': '10.18653/v1/2021.wnut-1.18', 'url': 'https://aclanthology.org/2021.wnut-1.18', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Pratapa, Adithya  and\\nChoudhury, Monojit', 'title': 'Comparing Grammatical Theories of Code-Mixing', 'ENTRYTYPE': 'inproceedings', 'ID': 'pratapa-choudhury-2021-comparing'}, {'abstract': 'Automatic Speech Recognition (ASR) systems generally do not produce punctuated transcripts. To make transcripts more readable and follow the expected input format for downstream language models, it is necessary to add punctuation marks. In this paper, we tackle the punctuation restoration problem specifically for the noisy text (e.g., phone conversation scenarios). To leverage the available written text datasets, we introduce a data sampling technique based on an n-gram language model to sample more training data that are similar to our in-domain data. Moreover, we propose a two-stage fine-tuning approach that utilizes the sampled external data as well as our in-domain dataset for models based on BERT. Extensive experiments show that the proposed approach outperforms the baseline with an improvement of 1.12{\\\\%} F1 score.', 'pages': '168--174', 'doi': '10.18653/v1/2021.wnut-1.19', 'url': 'https://aclanthology.org/2021.wnut-1.19', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Fu, Xue-Yong  and\\nChen, Cheng  and\\nLaskar, Md Tahmid Rahman  and\\nBhushan, Shashi  and\\nCorston-Oliver, Simon', 'title': 'Improving Punctuation Restoration for Speech Transcripts via External Data', 'ENTRYTYPE': 'inproceedings', 'ID': 'fu-etal-2021-improving'}, {'abstract': 'In this work, we propose a novel and easy-to-apply data augmentation strategy, namely \\\\textbf{Bi}lateral \\\\textbf{G}eneration (\\\\textbf{BiG}), with a contrastive training objective for improving the performance of ranking question answer pairs with existing labeled data. In specific, we synthesize pseudo-positive QA pairs in contrast to the original negative QA pairs with two pre-trained generation models, one for question generation, the other for answer generation, which are fine-tuned on the limited positive QA pairs from the original dataset. With the augmented dataset, we design a contrastive training objective for learning to rank question answer pairs. Experimental results on three benchmark datasets show that our method significantly improves the performance of ranking models by making full use of existing labeled data and can be easily applied to different ranking models.', 'pages': '175--181', 'doi': '10.18653/v1/2021.wnut-1.20', 'url': 'https://aclanthology.org/2021.wnut-1.20', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Deng, Yang  and\\nZhang, Wenxuan  and\\nLam, Wai', 'title': 'Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation', 'ENTRYTYPE': 'inproceedings', 'ID': 'deng-etal-2021-learning'}, {'abstract': 'Fake news causes significant damage to society. To deal with these fake news, several studies on building detection models and arranging datasets have been conducted. Most of the fake news datasets depend on a specific time period. Consequently, the detection models trained on such a dataset have difficulty detecting novel fake news generated by political changes and social changes; they may possibly result in biased output from the input, including specific person names and organizational names. We refer to this problem as Diachronic Bias because it is caused by the creation date of news in each dataset. In this study, we confirm the bias, especially proper nouns including person names, from the deviation of phrase appearances in each dataset. Based on these findings, we propose masking methods using Wikidata to mitigate the influence of person names and validate whether they make fake news detection models robust through experiments with in-domain and out-of-domain data.', 'pages': '182--188', 'doi': '10.18653/v1/2021.wnut-1.21', 'url': 'https://aclanthology.org/2021.wnut-1.21', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Murayama, Taichi  and\\nWakamiya, Shoko  and\\nAramaki, Eiji', 'title': 'Mitigation of Diachronic Bias in Fake News Detection Dataset', 'ENTRYTYPE': 'inproceedings', 'ID': 'murayama-etal-2021-mitigation'}, {'abstract': 'This work takes a critical look at the evaluation of user-generated content automatic translation, the well-known specificities of which raise many challenges for MT. Our analyses show that measuring the average-case performance using a standard metric on a UGC test set falls far short of giving a reliable image of the UGC translation quality. That is why we introduce a new data set for the evaluation of UGC translation in which UGC specificities have been manually annotated using a fine-grained typology. Using this data set, we conduct several experiments to measure the impact of different kinds of UGC specificities on translation quality, more precisely than previously possible.', 'pages': '189--198', 'doi': '10.18653/v1/2021.wnut-1.22', 'url': 'https://aclanthology.org/2021.wnut-1.22', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': \"Rosales N{\\\\'u}{\\\\~n}ez, Jos{\\\\'e} Carlos  and\\nSeddah, Djam{\\\\'e}  and\\nWisniewski, Guillaume\", 'title': 'Understanding the Impact of {UGC} Specificities on Translation Quality', 'ENTRYTYPE': 'inproceedings', 'ID': 'rosales-nunez-etal-2021-understanding'}, {'abstract': 'This work explores the capacities of character-based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such approaches to handle productive UGC phenomena, which almost by definition, cannot be seen at training time. Within a strict zero-shot scenario, we first study the detrimental impact on translation performance of various user-generated content phenomena on a small annotated dataset we developed and then show that such models are indeed incapable of handling unknown letters, which leads to catastrophic translation failure once such characters are encountered. We further confirm this behavior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation.', 'pages': '199--211', 'doi': '10.18653/v1/2021.wnut-1.23', 'url': 'https://aclanthology.org/2021.wnut-1.23', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': \"Rosales N{\\\\'u}{\\\\~n}ez, Jos{\\\\'e} Carlos  and\\nWisniewski, Guillaume  and\\nSeddah, Djam{\\\\'e}\", 'title': 'Noisy {UGC} Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models', 'ENTRYTYPE': 'inproceedings', 'ID': 'rosales-nunez-etal-2021-noisy'}, {'abstract': 'Twitter data has become established as a valuable source of data for various application scenarios in the past years. For many such applications, it is necessary to know where Twitter posts (tweets) were sent from or what location they refer to. Researchers have frequently used exact coordinates provided in a small percentage of tweets, but Twitter removed the option to share these coordinates in mid-2019. Moreover, there is reason to suspect that a large share of the provided coordinates did not correspond to GPS coordinates of the user even before that. In this paper, we explain the situation and the 2019 policy change and shed light on the various options of still obtaining location information from tweets. We provide usage statistics including changes over time, and analyze what the removal of exact coordinates means for various common research tasks performed with Twitter data. Finally, we make suggestions for future research requiring geolocated tweets.', 'pages': '212--221', 'doi': '10.18653/v1/2021.wnut-1.24', 'url': 'https://aclanthology.org/2021.wnut-1.24', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Kruspe, Anna  and\\nH{\\\\\"a}berle, Matthias  and\\nHoffmann, Eike J.  and\\nRode-Hasinger, Samyo  and\\nAbdulahhad, Karam  and\\nZhu, Xiao Xiang', 'title': 'Changes in {T}witter geolocations: Insights and suggestions for future usage', 'ENTRYTYPE': 'inproceedings', 'ID': 'kruspe-etal-2021-changes'}, {'abstract': 'Despite excellent performance on tasks such as question answering, Transformer-based architectures remain sensitive to syntactic and contextual ambiguities. Question Paraphrasing (QP) offers a promising solution as a means to augment existing datasets. The main challenges of current QP models include lack of training data and difficulty in generating diverse and natural questions. In this paper, we present Conquest, a framework for generating synthetic datasets for contextual question paraphrasing. To this end, Conquest first employs an answer-aware question generation (QG) model to create a question-pair dataset and then uses this data to train a contextualized question paraphrasing model. We extensively evaluate Conquest and show its ability to produce more diverse and fluent question pairs than existing approaches. Our contextual paraphrase model also establishes a strong baseline for end-to-end contextual paraphrasing. Further, We find that context can improve BLEU-1 score on contextual compression and expansion by 4.3 and 11.2 respectively, compared to a non-contextual model.', 'pages': '222--229', 'doi': '10.18653/v1/2021.wnut-1.25', 'url': 'https://aclanthology.org/2021.wnut-1.25', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Mirshekari, Mostafa  and\\nGu, Jing  and\\nSisto, Aaron', 'title': '{C}on{Q}uest: Contextual Question Paraphrasing through Answer-Aware Synthetic Question Generation', 'ENTRYTYPE': 'inproceedings', 'ID': 'mirshekari-etal-2021-conquest'}, {'abstract': 'Adverse Drug Event (ADE) extraction models can rapidly examine large collections of social media texts, detecting mentions of drug-related adverse reactions and trigger medical investigations. However, despite the recent advances in NLP, it is currently unknown if such models are robust in face of negation, which is pervasive across language varieties. In this paper we evaluate three state-of-the-art systems, showing their fragility against negation, and then we introduce two possible strategies to increase the robustness of these models: a pipeline approach, relying on a specific component for negation detection; an augmentation of an ADE extraction dataset to artificially create negated samples and further train the models. We show that both strategies bring significant increases in performance, lowering the number of spurious entities predicted by the models. Our dataset and code will be publicly released to encourage research on the topic.', 'pages': '230--237', 'doi': '10.18653/v1/2021.wnut-1.26', 'url': 'https://aclanthology.org/2021.wnut-1.26', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Scaboro, Simone  and\\nPortelli, Beatrice  and\\nChersoni, Emmanuele  and\\nSantus, Enrico  and\\nSerra, Giuseppe', 'title': '{NADE}: A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations', 'ENTRYTYPE': 'inproceedings', 'ID': 'scaboro-etal-2021-nade'}, {'abstract': 'Following the increasing performance of neural machine translation systems, the paradigm of using automatically translated data for cross-lingual adaptation is now studied in several applicative domains. The capacity to accurately project annotations remains however an issue for sequence tagging tasks where annotation must be projected with correct spans. Additionally, when the task implies noisy user-generated text, the quality of translation and annotation projection can be affected. In this paper we propose to tackle multilingual sequence tagging with a new span alignment method and apply it to opinion target extraction from customer reviews. We show that provided suitable heuristics, translated data with automatic span-level annotation projection can yield improvements both for cross-lingual adaptation compared to zero-shot transfer, and data augmentation compared to a multilingual baseline.', 'pages': '238--248', 'doi': '10.18653/v1/2021.wnut-1.27', 'url': 'https://aclanthology.org/2021.wnut-1.27', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': \"Jacqmin, L{\\\\'e}o  and\\nMarzinotto, Gabriel  and\\nGromada, Justyna  and\\nSzczekocka, Ewelina  and\\nKo{\\\\l}ody{\\\\'n}ski, Robert  and\\nDamnati, G{\\\\'e}raldine\", 'title': '{S}pan{A}lign: Efficient Sequence Tagging Annotation Projection into Translated Data applied to Cross-Lingual Opinion Mining', 'ENTRYTYPE': 'inproceedings', 'ID': 'jacqmin-etal-2021-spanalign'}, {'abstract': 'Social media is an essential tool to share information about crisis events, such as natural disasters. Event Detection aims at extracting information in the form of an event, but considers each event in isolation, without combining information across sentences or events. Many posts in Crisis NLP contain repetitive or complementary information which needs to be aggregated (e.g., the number of trapped people and their location) for disaster response. Although previous approaches in Crisis NLP aggregate information across posts, they only use shallow representations of the content (e.g., keywords), which cannot adequately represent the semantics of a crisis event and its sub-events. In this work, we propose a novel framework to extract critical sub-events from a large-scale crisis event by combining important information across relevant tweets. Our framework first converts all the tweets from a crisis event into a temporally-ordered set of graphs. Then it extracts sub-graphs that represent semantic relationships connecting verbs and nouns in 3 to 6 node sub-graphs. It does this by learning edge weights via Dynamic Graph Convolutional Networks (DGCNs) and extracting smaller, relevant sub-graphs. Our experiments show that our extracted structures (1) are semantically meaningful sub-events and (2) contain information important for the large crisis-event. Furthermore, we show that our approach significantly outperforms event detection baselines, highlighting the importance of aggregating information across tweets for our task.', 'pages': '249--259', 'doi': '10.18653/v1/2021.wnut-1.28', 'url': 'https://aclanthology.org/2021.wnut-1.28', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Spiliopoulou, Evangelia  and\\nSaha, Tanay Kumar  and\\nTetreault, Joel  and\\nJaimes, Alejandro', 'title': 'A Novel Framework for Detecting Important Subevents from Crisis Events via Dynamic Semantic Graphs', 'ENTRYTYPE': 'inproceedings', 'ID': 'spiliopoulou-etal-2021-novel'}, {'abstract': 'Extracting temporal information is critical to process health-related text. Temporal information extraction is a challenging task for language models because it requires processing both texts and numbers. Moreover, the fundamental challenge is how to obtain a large-scale training dataset. To address this, we propose a synthetic data generation algorithm. Also, we propose a novel multi-task temporal information extraction model and investigate whether multi-task learning can contribute to performance improvement by exploiting additional training signals with the existing training data. For experiments, we collected a custom dataset containing unstructured texts with temporal information of sleep-related activities. Experimental results show that utilising synthetic data can improve the performance when the augmentation factor is 3. The results also show that when multi-task learning is used with an appropriate amount of synthetic data, the performance can significantly improve from 82. to 88.6 and from 83.9 to 91.9 regarding micro-and macro-average exact match scores of normalised time prediction, respectively.', 'pages': '260--273', 'doi': '10.18653/v1/2021.wnut-1.29', 'url': 'https://aclanthology.org/2021.wnut-1.29', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Shim, Heereen  and\\nLowet, Dietwig  and\\nLuca, Stijn  and\\nVanrumste, Bart', 'title': 'Synthetic Data Generation and Multi-Task Learning for Extracting Temporal Information from Health-Related Narrative Text', 'ENTRYTYPE': 'inproceedings', 'ID': 'shim-etal-2021-synthetic'}, {'abstract': \"Most of the existing studies of language use in social media content have focused on the surface-level linguistic features (e.g., function words and punctuation marks) and the semantic level aspects (e.g., the topics, sentiment, and emotions) of the comments. The writer{'}s strategies of constructing and connecting text segments have not been widely explored even though this knowledge is expected to shed light on how people reason in online environments. Contributing to this analysis direction for social media studies, we build an openly accessible neural RST parsing system that analyzes discourse relations in an online comment. Our experiments demonstrate that this system achieves comparable performance among all the neural RST parsing systems. To demonstrate the use of this tool in social media analysis, we apply it to identify the discourse relations in persuasive and non-persuasive comments and examine the relationships among the binary discourse tree depth, discourse relations, and the perceived persuasiveness of online comments. Our work demonstrates the potential of analyzing discourse structures of online comments with our system and the implications of these structures for understanding online communications.\", 'pages': '274--283', 'doi': '10.18653/v1/2021.wnut-1.30', 'url': 'https://aclanthology.org/2021.wnut-1.30', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Li, Jinfen  and\\nXiao, Lu', 'title': 'Neural-based {RST} Parsing And Analysis In Persuasive Discourse', 'ENTRYTYPE': 'inproceedings', 'ID': 'li-xiao-2021-neural'}, {'abstract': 'Optical character recognition (OCR) from newspaper page images is susceptible to noise due to degradation of old documents and variation in typesetting. In this report, we present a novel approach to OCR post-correction. We cast error correction as a translation task, and fine-tune BART, a transformer-based sequence-to-sequence language model pretrained to denoise corrupted text. We are the first to use sentence-level transformer models for OCR post-correction, and our best model achieves a 29.4{\\\\%} improvement in character accuracy over the original noisy OCR text. Our results demonstrate the utility of pretrained language models for dealing with noisy text.', 'pages': '284--290', 'doi': '10.18653/v1/2021.wnut-1.31', 'url': 'https://aclanthology.org/2021.wnut-1.31', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Soper, Elizabeth  and\\nFujimoto, Stanley  and\\nYu, Yen-Yun', 'title': '{BART} for Post-Correction of {OCR} Newspaper Text', 'ENTRYTYPE': 'inproceedings', 'ID': 'soper-etal-2021-bart'}, {'abstract': 'We present new state-of-the-art benchmarks for paraphrase detection on all six languages in the Opusparcus sentential paraphrase corpus: English, Finnish, French, German, Russian, and Swedish. We reach these baselines by fine-tuning BERT. The best results are achieved on smaller and cleaner subsets of the training sets than was observed in previous research. Additionally, we study a translation-based approach that is competitive for the languages with more limited and noisier training data.', 'pages': '291--296', 'doi': '10.18653/v1/2021.wnut-1.32', 'url': 'https://aclanthology.org/2021.wnut-1.32', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Vahtola, Teemu  and\\nCreutz, Mathias  and\\nSj{\\\\\"o}blom, Eetu  and\\nItkonen, Sami', 'title': 'Coping with Noisy Training Data Labels in Paraphrase Detection', 'ENTRYTYPE': 'inproceedings', 'ID': 'vahtola-etal-2021-coping'}, {'abstract': 'Knowledge Distillation (KD) is extensively used to compress and deploy large pre-trained language models on edge devices for real-world applications. However, one neglected area of research is the impact of noisy (corrupted) labels on KD. We present, to the best of our knowledge, the first study on KD with noisy labels in Natural Language Understanding (NLU). We document the scope of the problem and present two methods to mitigate the impact of label noise. Experiments on the GLUE benchmark show that our methods are effective even under high noise levels. Nevertheless, our results indicate that more research is necessary to cope with label noise under the KD.', 'pages': '297--303', 'doi': '10.18653/v1/2021.wnut-1.33', 'url': 'https://aclanthology.org/2021.wnut-1.33', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Bhardwaj, Shivendra  and\\nGhaddar, Abbas  and\\nRashid, Ahmad  and\\nBibi, Khalil  and\\nLi, Chengyang  and\\nGhodsi, Ali  and\\nLanglais, Phillippe  and\\nRezagholizadeh, Mehdi', 'title': 'Knowledge Distillation with Noisy Labels for Natural Language Understanding', 'ENTRYTYPE': 'inproceedings', 'ID': 'bhardwaj-etal-2021-knowledge'}, {'abstract': \"Stance detection (SD) entails classifying the sentiment of a text towards a given target, and is a relevant sub-task for opinion mining and social media analysis. Recent works have explored knowledge infusion supplementing the linguistic competence and latent knowledge of large pre-trained language models with structured knowledge graphs (KGs), yet few works have applied such methods to the SD task. In this work, we first perform stance-relevant knowledge probing on Transformers-based pre-trained models in a zero-shot setting, showing these models{'} latent real-world knowledge about SD targets and their sensitivity to context. We then train and evaluate new knowledge-enriched stance detection models on two Twitter stance datasets, achieving state-of-the-art performance on both.\", 'pages': '304--312', 'doi': '10.18653/v1/2021.wnut-1.34', 'url': 'https://aclanthology.org/2021.wnut-1.34', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Clark, Thomas  and\\nConforti, Costanza  and\\nLiu, Fangyu  and\\nMeng, Zaiqiao  and\\nShareghi, Ehsan  and\\nCollier, Nigel', 'title': 'Integrating Transformers and Knowledge Graphs for {T}witter Stance Detection', 'ENTRYTYPE': 'inproceedings', 'ID': 'clark-etal-2021-integrating'}, {'abstract': 'Online social media platforms increasingly rely on Natural Language Processing (NLP) techniques to detect abusive content at scale in order to mitigate the harms it causes to their users. However, these techniques suffer from various sampling and association biases present in training data, often resulting in sub-par performance on content relevant to marginalized groups, potentially furthering disproportionate harms towards them. Studies on such biases so far have focused on only a handful of axes of disparities and subgroups that have annotations/lexicons available. Consequently, biases concerning non-Western contexts are largely ignored in the literature. In this paper, we introduce a weakly supervised method to robustly detect lexical biases in broader geo-cultural contexts. Through a case study on a publicly available toxicity detection model, we demonstrate that our method identifies salient groups of cross-geographic errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts. We also conduct analysis of a model trained on a dataset with ground truth labels to better understand these biases, and present preliminary mitigation experiments.', 'pages': '313--328', 'doi': '10.18653/v1/2021.wnut-1.35', 'url': 'https://aclanthology.org/2021.wnut-1.35', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Ghosh, Sayan  and\\nBaker, Dylan  and\\nJurgens, David  and\\nPrabhakaran, Vinodkumar', 'title': 'Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media', 'ENTRYTYPE': 'inproceedings', 'ID': 'ghosh-etal-2021-detecting'}, {'abstract': \"On Wikipedia, an online crowdsourced encyclopedia, volunteers enforce the encyclopedia{'}s editorial policies. Wikipedia{'}s policy on maintaining a neutral point of view has inspired recent research on bias detection, including {``}weasel words{''} and {``}hedges{''}. Yet to date, little work has been done on identifying {``}puffery,{''} phrases that are overly positive without a verifiable source. We demonstrate that collecting training data for this task requires some care, and construct a dataset by combining Wikipedia editorial annotations and information retrieval techniques. We compare several approaches to predicting puffery, and achieve 0.963 f1 score by incorporating citation features into a RoBERTa model. Finally, we demonstrate how to integrate our model with Wikipedia{'}s public infrastructure to give back to the Wikipedia editor community.\", 'pages': '329--333', 'doi': '10.18653/v1/2021.wnut-1.36', 'url': 'https://aclanthology.org/2021.wnut-1.36', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Bertsch, Amanda  and\\nBethard, Steven', 'title': 'Detection of Puffery on the {E}nglish {W}ikipedia', 'ENTRYTYPE': 'inproceedings', 'ID': 'bertsch-bethard-2021-detection'}, {'abstract': \"Understanding robustness and sensitivity of BERT models predicting Alzheimer{'}s disease from text is important for both developing better classification models and for understanding their capabilities and limitations. In this paper, we analyze how a controlled amount of desired and undesired text alterations impacts performance of BERT. We show that BERT is robust to natural linguistic variations in text. On the other hand, we show that BERT is not sensitive to removing clinically important information from text.\", 'pages': '334--339', 'doi': '10.18653/v1/2021.wnut-1.37', 'url': 'https://aclanthology.org/2021.wnut-1.37', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Novikova, Jekaterina', 'title': \"Robustness and Sensitivity of {BERT} Models Predicting {A}lzheimer{'}s Disease from Text\", 'ENTRYTYPE': 'inproceedings', 'ID': 'novikova-2021-robustness'}, {'abstract': \"Sensitivity of deep-neural models to input noise is known to be a challenging problem. In NLP, model performance often deteriorates with naturally occurring noise, such as spelling errors. To mitigate this issue, models may leverage artificially noised data. However, the amount and type of generated noise has so far been determined arbitrarily. We therefore propose to model the errors statistically from grammatical-error-correction corpora. We present a thorough evaluation of several state-of-the-art NLP systems{'} robustness in multiple languages, with tasks including morpho-syntactic analysis, named entity recognition, neural machine translation, a subset of the GLUE benchmark and reading comprehension. We also compare two approaches to address the performance drop: a) training the NLP models with noised data generated by our framework; and b) reducing the input noise with external system for natural language correction. The code is released at https://github.com/ufal/kazitext.\", 'pages': '340--350', 'doi': '10.18653/v1/2021.wnut-1.38', 'url': 'https://aclanthology.org/2021.wnut-1.38', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': \"N{\\\\'a}plava, Jakub  and\\nPopel, Martin  and\\nStraka, Milan  and\\nStrakov{\\\\'a}, Jana\", 'title': 'Understanding Model Robustness to User-generated Noisy Texts', 'ENTRYTYPE': 'inproceedings', 'ID': 'naplava-etal-2021-understanding'}, {'abstract': \"This paper shows that CIDEr-D, a traditional evaluation metric for image description, does not work properly on datasets where the number of words in the sentence is significantly greater than those in the MS COCO Captions dataset. We also show that CIDEr-D has performance hampered by the lack of multiple reference sentences and high variance of sentence length. To bypass this problem, we introduce CIDEr-R, which improves CIDEr-D, making it more flexible in dealing with datasets with high sentence length variance. We demonstrate that CIDEr-R is more accurate and closer to human judgment than CIDEr-D; CIDEr-R is more robust regarding the number of available references. Our results reveal that using Self-Critical Sequence Training to optimize CIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized, the generated captions{'} length tends to be similar to the reference length. However, the models also repeat several times the same word to increase the sentence length.\", 'pages': '351--360', 'doi': '10.18653/v1/2021.wnut-1.39', 'url': 'https://aclanthology.org/2021.wnut-1.39', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Oliveira dos Santos, Gabriel  and\\nColombini, Esther Luna  and\\nAvila, Sandra', 'title': '{CIDE}r-{R}: Robust Consensus-based Image Description Evaluation', 'ENTRYTYPE': 'inproceedings', 'ID': 'oliveira-dos-santos-etal-2021-cider'}, {'abstract': 'We explore the application of state-of-the-art NER algorithms to ASR-generated call center transcripts. Previous work in this domain focused on the use of a BiLSTM-CRF model which relied on Flair embeddings; however, such a model is unwieldy in terms of latency and memory consumption. In a production environment, end users require low-latency models which can be readily integrated into existing pipelines. To that end, we present two different models which can be utilized based on the latency and accuracy requirements of the user. First, we propose a set of models which utilize state-of-the-art Transformer language models (RoBERTa) to develop a high-accuracy NER system trained on custom annotated set of call center transcripts. We then use our best-performing Transformer-based model to label a large number of transcripts, which we use to pretrain a BiLSTM-CRF model and further fine-tune on our annotated dataset. We show that this model, while not as accurate as its Transformer-based counterpart, is highly effective in identifying items which require redaction for privacy law compliance. Further, we propose a new general annotation scheme for NER in the call-center environment.', 'pages': '361--370', 'doi': '10.18653/v1/2021.wnut-1.40', 'url': 'https://aclanthology.org/2021.wnut-1.40', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Davidson, Sam  and\\nHosier, Jordan  and\\nZhou, Yu  and\\nGurbani, Vijay', 'title': 'Improved Named Entity Recognition for Noisy Call Center Transcripts', 'ENTRYTYPE': 'inproceedings', 'ID': 'davidson-etal-2021-improved'}, {'abstract': 'Certain types of classification problems may be performed at multiple levels of granularity; for example, we might want to know the sentiment polarity of a document or a sentence, or a phrase. Often, the prediction at a greater-context (e.g., sentences or paragraphs) may be informative for a more localized prediction at a smaller semantic unit (e.g., words or phrases). However, directly inferring the most salient local features from the global prediction may overlook the semantics of this relationship. This work argues that inference along the contraposition relationship of the local prediction and the corresponding global prediction makes an inference framework that is more accurate and robust to noise. We show how this contraposition framework can be implemented as a transfer function that rewrites a greater-context from one class to another and demonstrate how an appropriate transfer function can be trained from a noisy user-generated corpus. The experimental results validate our insight that the proposed contrapositive framework outperforms the alternative approaches on resource-constrained problem domains.', 'pages': '371--380', 'doi': '10.18653/v1/2021.wnut-1.41', 'url': 'https://aclanthology.org/2021.wnut-1.41', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Kashefi, Omid  and\\nHwa, Rebecca', 'title': 'Contrapositive Local Class Inference', 'ENTRYTYPE': 'inproceedings', 'ID': 'kashefi-hwa-2021-contrapositive'}, {'abstract': 'We evaluate a simple approach to improving zero-shot multilingual transfer of mBERT on social media corpus by adding a pretraining task called translation pair prediction (TPP), which predicts whether a pair of cross-lingual texts are a valid translation. Our approach assumes access to translations (exact or approximate) between source-target language pairs, where we fine-tune a model on source language task data and evaluate the model in the target language. In particular, we focus on language pairs where transfer learning is difficult for mBERT: those where source and target languages are different in script, vocabulary, and linguistic typology. We show improvements from TPP pretraining over mBERT alone in zero-shot transfer from English to Hindi, Arabic, and Japanese on two social media tasks: NER (a 37{\\\\%} average relative improvement in F1 across target languages) and sentiment classification (12{\\\\%} relative improvement in F1) on social media text, while also benchmarking on a non-social media task of Universal Dependency POS tagging (6.7{\\\\%} relative improvement in accuracy). Our results are promising given the lack of social media bitext corpus. Our code can be found at: https://github.com/twitter-research/multilingual-alignment-tpp.', 'pages': '381--388', 'doi': '10.18653/v1/2021.wnut-1.42', 'url': 'https://aclanthology.org/2021.wnut-1.42', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Mishra, Shubhanshu  and\\nHaghighi, Aria', 'title': 'Improved Multilingual Language Model Pretraining for Social Media Text via Translation Pair Prediction', 'ENTRYTYPE': 'inproceedings', 'ID': 'mishra-haghighi-2021-improved'}, {'abstract': 'Commits in version control systems (e.g. Git) track changes in a software project. Commits comprise noisy user-generated natural language and code patches. Automatic commit classification (CC) has been used to determine the type of code maintenance activities performed, as well as to detect bug fixes in code repositories. Much prior work occurs in the fully-supervised setting {--} a setting that can be a stretch in resource-scarce situations presenting difficulties in labeling commits. In this paper, we apply co-training, a semi-supervised learning method, to take advantage of the two views available {--} the commit message (natural language) and the code changes (programming language) {--} to improve commit classification.', 'pages': '389--395', 'doi': '10.18653/v1/2021.wnut-1.43', 'url': 'https://aclanthology.org/2021.wnut-1.43', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Lee, Jian Yi David  and\\nChieu, Hai Leong', 'title': 'Co-training for Commit Classification', 'ENTRYTYPE': 'inproceedings', 'ID': 'lee-chieu-2021-co'}, {'abstract': 'Twitter is commonly used for civil unrest detection and forecasting tasks, but there is a lack of work in evaluating \\\\textit{how} civil unrest manifests on Twitter across countries and events. We present two in-depth case studies for two specific large-scale events, one in a country with high (English) Twitter usage (Johannesburg riots in South Africa) and one in a country with low Twitter usage (Burayu massacre protests in Ethiopia). We show that while there is event signal during the events, there is little signal leading up to the events. In addition to the case studies, we train Ngram-based models on a larger set of Twitter civil unrest data across time, events, and countries and use machine learning explainability tools (SHAP) to identify important features. The models were able to find words indicative of civil unrest that generalized across countries. The 42 countries span Africa, Middle East, and Southeast Asia and the events range occur between 2014 and 2019.', 'pages': '396--409', 'doi': '10.18653/v1/2021.wnut-1.44', 'url': 'https://aclanthology.org/2021.wnut-1.44', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Chinta, Abhinav  and\\nZhang, Jingyu  and\\nDeLucia, Alexandra  and\\nDredze, Mark  and\\nBuczak, Anna L.', 'title': 'Study of Manifestation of Civil Unrest on {T}witter', 'ENTRYTYPE': 'inproceedings', 'ID': 'chinta-etal-2021-study'}, {'abstract': 'User-generated texts include various types of stylistic properties, or noises. Such texts are not properly processed by existing morpheme analyzers or language models based on formal texts such as encyclopedias or news articles. In this paper, we propose a simple morphologically tight-fitting tokenizer (K-MT) that can better process proper nouns, coinages, and internet slang among other types of noise in Korean user-generated texts. We tested our tokenizer by performing classification tasks on Korean user-generated movie reviews and hate speech datasets, and the Korean Named Entity Recognition dataset. Through our tests, we found that K-MT is better fit to process internet slangs, proper nouns, and coinages, compared to a morpheme analyzer and a character-level WordPiece tokenizer.', 'pages': '410--416', 'doi': '10.18653/v1/2021.wnut-1.45', 'url': 'https://aclanthology.org/2021.wnut-1.45', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Lee, Sangah  and\\nShin, Hyopil', 'title': 'The {K}orean Morphologically Tight-Fitting Tokenizer for Noisy User-Generated Texts', 'ENTRYTYPE': 'inproceedings', 'ID': 'lee-shin-2021-korean'}, {'abstract': 'We propose a character-based non-autoregressive GEC approach, with automatically generated character transformations. Recently, per-word classification of correction edits has proven an efficient, parallelizable alternative to current encoder-decoder GEC systems. We show that word replacement edits may be suboptimal and lead to explosion of rules for spelling, diacritization and errors in morphologically rich languages, and propose a method for generating character transformations from GEC corpus. Finally, we train character transformation models for Czech, German and Russian, reaching solid results and dramatic speedup compared to autoregressive systems. The source code is released at https://github.com/ufal/wnut2021{\\\\_}character{\\\\_}transformations{\\\\_}gec.', 'pages': '417--422', 'doi': '10.18653/v1/2021.wnut-1.46', 'url': 'https://aclanthology.org/2021.wnut-1.46', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': \"Straka, Milan  and\\nN{\\\\'a}plava, Jakub  and\\nStrakov{\\\\'a}, Jana\", 'title': 'Character Transformations for Non-Autoregressive {GEC} Tagging', 'ENTRYTYPE': 'inproceedings', 'ID': 'straka-etal-2021-character'}, {'abstract': 'Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages remains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Arabic written using an extension of the Latin script, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability set- tings.', 'pages': '423--436', 'doi': '10.18653/v1/2021.wnut-1.47', 'url': 'https://aclanthology.org/2021.wnut-1.47', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': \"Riabi, Arij  and\\nSagot, Beno{\\\\^\\\\i}t  and\\nSeddah, Djam{\\\\'e}\", 'title': 'Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?', 'ENTRYTYPE': 'inproceedings', 'ID': 'riabi-etal-2021-character'}, {'abstract': \"The increasing use of social media sites in countries like India has given rise to large volumes of code-mixed data. Sentiment analysis of this data can provide integral insights into people{'}s perspectives and opinions. Code-mixed data is often noisy in nature due to multiple spellings for the same word, lack of definite order of words in a sentence, and random abbreviations. Thus, working with code-mixed data is more challenging than monolingual data. Interpreting a model{'}s predictions allows us to determine the robustness of the model against different forms of noise. In this paper, we propose a methodology to integrate explainable approaches into code-mixed sentiment analysis. By interpreting the predictions of sentiment analysis models we evaluate how well the model is able to adapt to the implicit noises present in code-mixed data.\", 'pages': '437--444', 'doi': '10.18653/v1/2021.wnut-1.48', 'url': 'https://aclanthology.org/2021.wnut-1.48', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Priyanshu, Aman  and\\nVardhan, Aleti  and\\nSivakumar, Sudarshan  and\\nVijay, Supriti  and\\nChhabra, Nipuna', 'title': \"{``}Something Something Hota Hai!{''} An Explainable Approach towards Sentiment Analysis on {I}ndian Code-Mixed Data\", 'ENTRYTYPE': 'inproceedings', 'ID': 'priyanshu-etal-2021-something'}, {'abstract': 'We introduce BERTweetFR, the first large-scale pre-trained language model for French tweets. Our model is initialised using a general-domain French language model CamemBERT which follows the base architecture of BERT. Experiments show that BERTweetFR outperforms all previous general-domain French language models on two downstream Twitter NLP tasks of offensiveness identification and named entity recognition. The dataset used in the offensiveness detection task is first created and annotated by our team, filling in the gap of such analytic datasets in French. We make our model publicly available in the transformers library with the aim of promoting future research in analytic tasks for French tweets.', 'pages': '445--450', 'doi': '10.18653/v1/2021.wnut-1.49', 'url': 'https://aclanthology.org/2021.wnut-1.49', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Guo, Yanzhu  and\\nRennard, Virgile  and\\nXypolopoulos, Christos  and\\nVazirgiannis, Michalis', 'title': '{BERT}weet{FR} : Domain Adaptation of Pre-Trained Language Models for {F}rench Tweets', 'ENTRYTYPE': 'inproceedings', 'ID': 'guo-etal-2021-bertweetfr'}, {'abstract': 'How difficult is it for English-as-a-second language (ESL) learners to read noisy English texts? Do ESL learners need lexical normalization to read noisy English texts? These questions may also affect community formation on social networking sites where differences can be attributed to ESL learners and native English speakers. However, few studies have addressed these questions. To this end, we built highly accurate readability assessors to evaluate the readability of texts for ESL learners. We then applied these assessors to noisy English texts to further assess the readability of the texts. The experimental results showed that although intermediate-level ESL learners can read most noisy English texts in the first place, lexical normalization significantly improves the readability of noisy English texts for ESL learners.', 'pages': '451--456', 'doi': '10.18653/v1/2021.wnut-1.50', 'url': 'https://aclanthology.org/2021.wnut-1.50', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Ehara, Yo', 'title': 'To What Extent Does Lexical Normalization Help {E}nglish-as-a-Second Language Learners to Read Noisy {E}nglish Texts?', 'ENTRYTYPE': 'inproceedings', 'ID': 'ehara-2021-extent-lexical'}, {'abstract': \"The task of converting a nonstandard text to a standard and readable text is known as lexical normalization. Almost all the Natural Language Processing (NLP) applications require the text data in normalized form to build quality task-specific models. Hence, lexical normalization has been proven to improve the performance of numerous natural language processing tasks on social media. This study aims to solve the problem of Lexical Normalization by formulating the Lexical Normalization task as a Sequence Labeling problem. This paper proposes a sequence labeling approach to solve the problem of Lexical Normalization in combination with the word-alignment technique. The goal is to use a single model to normalize text in various languages namely Croatian, Danish, Dutch, English, Indonesian-English, German, Italian, Serbian, Slovenian, Spanish, Turkish, and Turkish-German. This is a shared task in {``}2021 The 7th Workshop on Noisy User-generated Text (W-NUT){''} in which the participants are expected to create a system/model that performs lexical normalization, which is the translation of non-canonical texts into their canonical equivalents, comprising data from over 12 languages. The proposed single multilingual model achieves an overall ERR score of 43.75 on intrinsic evaluation and an overall Labeled Attachment Score (LAS) score of 63.12 on extrinsic evaluation. Further, the proposed method achieves the highest Error Reduction Rate (ERR) score of 61.33 among the participants in the shared task. This study highlights the effects of using additional training data to get better results as well as using a pre-trained Language model trained on multiple languages rather than only on one language.\", 'pages': '457--464', 'doi': '10.18653/v1/2021.wnut-1.51', 'url': 'https://aclanthology.org/2021.wnut-1.51', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Kubal, Divesh  and\\nNagvenkar, Apurva', 'title': 'Multilingual Sequence Labeling Approach to solve Lexical Normalization', 'ENTRYTYPE': 'inproceedings', 'ID': 'kubal-nagvenkar-2021-multilingual'}, {'abstract': 'This paper describes the HEL-LJU submissions to the MultiLexNorm shared task on multilingual lexical normalization. Our system is based on a BERT token classification preprocessing step, where for each token the type of the necessary transformation is predicted (none, uppercase, lowercase, capitalize, modify), and a character-level SMT step where the text is translated from original to normalized given the BERT-predicted transformation constraints. For some languages, depending on the results on development data, the training data was extended by back-translating OpenSubtitles data. In the final ordering of the ten participating teams, the HEL-LJU team has taken the second place, scoring better than the previous state-of-the-art.', 'pages': '465--472', 'doi': '10.18653/v1/2021.wnut-1.52', 'url': 'https://aclanthology.org/2021.wnut-1.52', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': \"Scherrer, Yves  and\\nLjube{\\\\v{s}}i{\\\\'c}, Nikola\", 'title': 'Sesame Street to Mount Sinai: {BERT}-constrained character-level {M}oses models for multilingual lexical normalization', 'ENTRYTYPE': 'inproceedings', 'ID': 'scherrer-ljubesic-2021-sesame'}, {'abstract': 'Current benchmark tasks for natural language processing contain text that is qualitatively different from the text used in informal day to day digital communication. This discrepancy has led to severe performance degradation of state-of-the-art NLP models when fine-tuned on real-world data. One way to resolve this issue is through lexical normalization, which is the process of transforming non-standard text, usually from social media, into a more standardized form. In this work, we propose a sentence-level sequence-to-sequence model based on mBART, which frames the problem as a machine translation problem. As the noisy text is a pervasive problem across languages, not just English, we leverage the multi-lingual pre-training of mBART to fine-tune it to our data. While current approaches mainly operate at the word or subword level, we argue that this approach is straightforward from a technical standpoint and builds upon existing pre-trained transformer networks. Our results show that while word-level, intrinsic, performance evaluation is behind other methods, our model improves performance on extrinsic, downstream tasks through normalization compared to models operating on raw, unprocessed, social media text.', 'pages': '473--482', 'doi': '10.18653/v1/2021.wnut-1.53', 'url': 'https://aclanthology.org/2021.wnut-1.53', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Bucur, Ana-Maria  and\\nCosma, Adrian  and\\nDinu, Liviu P.', 'title': 'Sequence-to-Sequence Lexical Normalization with Multilingual Transformers', 'ENTRYTYPE': 'inproceedings', 'ID': 'bucur-etal-2021-sequence'}, {'abstract': 'We present the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which evaluates lexical-normalization systems on 12 social media datasets in 11 languages. We base our solution on a pre-trained byte-level language model, ByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then fine-tune on authentic normalization data. Our system achieves the best performance by a wide margin in intrinsic evaluation, and also the best performance in extrinsic evaluation through dependency parsing. The source code is released at https://github.com/ufal/multilexnorm2021 and the fine-tuned models at https://huggingface.co/ufal.', 'pages': '483--492', 'doi': '10.18653/v1/2021.wnut-1.54', 'url': 'https://aclanthology.org/2021.wnut-1.54', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'Samuel, David  and\\nStraka, Milan', 'title': \"{{\\\\'U}FAL} at {M}ulti{L}ex{N}orm 2021: Improving Multilingual Lexical Normalization by Fine-tuning {B}y{T}5\", 'ENTRYTYPE': 'inproceedings', 'ID': 'samuel-straka-2021-ufal'}, {'abstract': 'Lexical normalization is the task of transforming an utterance into its standardized form. This task is beneficial for downstream analysis, as it provides a way to harmonize (often spontaneous) linguistic variation. Such variation is typical for social media on which information is shared in a multitude of ways, including diverse languages and code-switching. Since the seminal work of Han and Baldwin (2011) a decade ago, lexical normalization has attracted attention in English and multiple other languages. However, there exists a lack of a common benchmark for comparison of systems across languages with a homogeneous data and evaluation setup. The MultiLexNorm shared task sets out to fill this gap. We provide the largest publicly available multilingual lexical normalization benchmark including 13 language variants. We propose a homogenized evaluation setup with both intrinsic and extrinsic evaluation. As extrinsic evaluation, we use dependency parsing and part-of-speech tagging with adapted evaluation metrics (a-LAS, a-UAS, and a-POS) to account for alignment discrepancies. The shared task hosted at W-NUT 2021 attracted 9 participants and 18 submissions. The results show that neural normalization systems outperform the previous state-of-the-art system by a large margin. Downstream parsing and part-of-speech tagging performance is positively affected but to varying degrees, with improvements of up to 1.72 a-LAS, 0.85 a-UAS, and 1.54 a-POS for the winning system.', 'pages': '493--509', 'doi': '10.18653/v1/2021.wnut-1.55', 'url': 'https://aclanthology.org/2021.wnut-1.55', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'van der Goot, Rob  and\\nRamponi, Alan  and\\nZubiaga, Arkaitz  and\\nPlank, Barbara  and\\nMuller, Benjamin  and\\nSan Vicente Roncal, I{\\\\~n}aki  and\\nLjube{\\\\v{s}}i{\\\\\\'c}, Nikola  and\\n{\\\\c{C}}etino{\\\\u{g}}lu, {\\\\\"O}zlem  and\\nMahendra, Rahmad  and\\n{\\\\c{C}}olako{\\\\u{g}}lu, Talha  and\\nBaldwin, Timothy  and\\nCaselli, Tommaso  and\\nSidorenko, Wladimir', 'title': '{M}ulti{L}ex{N}orm: A Shared Task on Multilingual Lexical Normalization', 'ENTRYTYPE': 'inproceedings', 'ID': 'van-der-goot-etal-2021-multilexnorm'}, {'abstract': 'Social media is notoriously difficult to process for existing natural language processing tools, because of spelling errors, non-standard words, shortenings, non-standard capitalization and punctuation. One method to circumvent these issues is to normalize input data before processing. Most previous work has focused on only one language, which is mostly English. In this paper, we are the first to propose a model for cross-lingual normalization, with which we participate in the WNUT 2021 shared task. To this end, we use MoNoise as a starting point, and make a simple adaptation for cross-lingual application. Our proposed model outperforms the leave-as-is baseline provided by the organizers which copies the input. Furthermore, we explore a completely different model which converts the task to a sequence labeling task. Performance of this second system is low, as it does not take capitalization into account in our implementation.', 'pages': '510--514', 'doi': '10.18653/v1/2021.wnut-1.56', 'url': 'https://aclanthology.org/2021.wnut-1.56', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)', 'author': 'van der Goot, Rob', 'title': '{CL}-{M}o{N}oise: Cross-lingual Lexical Normalization', 'ENTRYTYPE': 'inproceedings', 'ID': 'van-der-goot-2021-cl'}, {'url': 'https://aclanthology.org/2021.wmt-1.0', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'editor': 'Barrault, Loic  and\\nBojar, Ondrej  and\\nBougares, Fethi  and\\nChatterjee, Rajen  and\\nCosta-jussa, Marta R.  and\\nFedermann, Christian  and\\nFishel, Mark  and\\nFraser, Alexander  and\\nFreitag, Markus  and\\nGraham, Yvette  and\\nGrundkiewicz, Roman  and\\nGuzman, Paco  and\\nHaddow, Barry  and\\nHuck, Matthias  and\\nYepes, Antonio Jimeno  and\\nKoehn, Philipp  and\\nKocmi, Tom  and\\nMartins, Andre  and\\nMorishita, Makoto  and\\nMonz, Christof', 'title': 'Proceedings of the Sixth Conference on Machine Translation', 'ENTRYTYPE': 'proceedings', 'ID': 'wmt-2021-machine'}, {'abstract': 'This paper presents the results of the newstranslation task, the multilingual low-resourcetranslation for Indo-European languages, thetriangular translation task, and the automaticpost-editing task organised as part of the Con-ference on Machine Translation (WMT) 2021.In the news task, participants were asked tobuild machine translation systems for any of10 language pairs, to be evaluated on test setsconsisting mainly of news stories. The taskwas also opened up to additional test suites toprobe specific aspects of translation.', 'pages': '1--88', 'url': 'https://aclanthology.org/2021.wmt-1.1', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Akhbardeh, Farhad  and\\nArkhangorodsky, Arkady  and\\nBiesialska, Magdalena  and\\nBojar, Ond{\\\\v{r}}ej  and\\nChatterjee, Rajen  and\\nChaudhary, Vishrav  and\\nCosta-jussa, Marta R.  and\\nEspa{\\\\~n}a-Bonet, Cristina  and\\nFan, Angela  and\\nFedermann, Christian  and\\nFreitag, Markus  and\\nGraham, Yvette  and\\nGrundkiewicz, Roman  and\\nHaddow, Barry  and\\nHarter, Leonie  and\\nHeafield, Kenneth  and\\nHoman, Christopher  and\\nHuck, Matthias  and\\nAmponsah-Kaakyire, Kwabena  and\\nKasai, Jungo  and\\nKhashabi, Daniel  and\\nKnight, Kevin  and\\nKocmi, Tom  and\\nKoehn, Philipp  and\\nLourie, Nicholas  and\\nMonz, Christof  and\\nMorishita, Makoto  and\\nNagata, Masaaki  and\\nNagesh, Ajay  and\\nNakazawa, Toshiaki  and\\nNegri, Matteo  and\\nPal, Santanu  and\\nTapo, Allahsera Auguste  and\\nTurchi, Marco  and\\nVydrin, Valentin  and\\nZampieri, Marcos', 'title': 'Findings of the 2021 Conference on Machine Translation ({WMT}21)', 'ENTRYTYPE': 'inproceedings', 'ID': 'akhbardeh-etal-2021-findings'}, {'abstract': \"We present the results of the first task on Large-Scale Multilingual Machine Translation. The task consists on the many-to-many evaluation of a single model across a variety of source and target languages. This year, the task consisted on three different settings: (i) SMALL-TASK1 (Central/South-Eastern European Languages), (ii) the SMALL-TASK2 (South-East Asian Languages), and (iii) FULL-TASK (all 101 x 100 language pairs). All the tasks used the FLORES-101 dataset as the evaluation benchmark. To ensure the longevity of the dataset, the test sets were not publicly released and the models were evaluated in a controlled environment on Dynabench. There were a total of 10 participating teams for the tasks, with a total of 151 intermediate model submissions and 13 final models. This year{'}s result show a significant improvement over the known base-lines with +17.8 BLEU for SMALL-TASK2, +10.6 for FULL-TASK and +3.6 for SMALL-TASK1.\", 'pages': '89--99', 'url': 'https://aclanthology.org/2021.wmt-1.2', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': \"Wenzek, Guillaume  and\\nChaudhary, Vishrav  and\\nFan, Angela  and\\nGomez, Sahir  and\\nGoyal, Naman  and\\nJain, Somya  and\\nKiela, Douwe  and\\nThrush, Tristan  and\\nGuzm{\\\\'a}n, Francisco\", 'title': 'Findings of the {WMT} 2021 Shared Task on Large-Scale Multilingual Machine Translation', 'ENTRYTYPE': 'inproceedings', 'ID': 'wenzek-etal-2021-findings'}, {'abstract': \"This paper describes the Global Tone Communication Co., Ltd.{'}s submission of the WMT21 shared news translation task. We participate in six directions: English to/from Hausa, Hindi to/from Bengali and Zulu to/from Xhosa. Our submitted systems are unconstrained and focus on multilingual translation odel, backtranslation and forward-translation. We also apply rules and language model to filter monolingual, parallel sentences and synthetic sentences.\", 'pages': '100--103', 'url': 'https://aclanthology.org/2021.wmt-1.3', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Bei, Chao  and\\nZong, Hao', 'title': '{GTCOM} Neural Machine Translation Systems for {WMT}21', 'ENTRYTYPE': 'inproceedings', 'ID': 'bei-zong-2021-gtcom'}, {'abstract': \"This paper presents the University of Edinburgh{'}s constrained submissions of English-German and English-Hausa systems to the WMT 2021 shared task on news translation. We build En-De systems in three stages: corpus filtering, back-translation, and fine-tuning. For En-Ha we use an iterative back-translation approach on top of pre-trained En-De models and investigate vocabulary embedding mapping.\", 'pages': '104--109', 'url': 'https://aclanthology.org/2021.wmt-1.4', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Chen, Pinzhen  and\\nHelcl, Jind{\\\\v{r}}ich  and\\nGermann, Ulrich  and\\nBurchell, Laurie  and\\nBogoychev, Nikolay  and\\nMiceli Barone, Antonio Valerio  and\\nWaldendorf, Jonas  and\\nBirch, Alexandra  and\\nHeafield, Kenneth', 'title': \"The {U}niversity of {E}dinburgh{'}s {E}nglish-{G}erman and {E}nglish-{H}ausa Submissions to the {WMT}21 News Translation Task\", 'ENTRYTYPE': 'inproceedings', 'ID': 'chen-etal-2021-university'}, {'abstract': 'This paper describes the Air Force Research Laboratory (AFRL) machine translation sys- tems and the improvements that were developed during the WMT21 evaluation campaign. This year, we explore various methods of adapting our baseline models from WMT20 and again measure improvements in performance on the Russian{--}English language pair.', 'pages': '110--116', 'url': 'https://aclanthology.org/2021.wmt-1.5', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Erdmann, Grant  and\\nGwinnup, Jeremy  and\\nAnderson, Tim', 'title': 'Tune in: The {AFRL} {WMT}21 News-Translation Systems', 'ENTRYTYPE': 'inproceedings', 'ID': 'erdmann-etal-2021-tune'}, {'abstract': 'This paper describes the submission to the WMT 2021 news translation shared task by the UPC Machine Translation group. The goal of the task is to translate German to French (De-Fr) and French to German (Fr-De). Our submission focuses on fine-tuning a pre-trained model to take advantage of monolingual data. We fine-tune mBART50 using the filtered data, and additionally, we train a Transformer model on the same data from scratch. In the experiments, we show that fine-tuning mBART50 results in 31.69 BLEU for De-Fr and 23.63 BLEU for Fr-De, which increases 2.71 and 1.90 BLEU accordingly, as compared to the model we train from scratch. Our final submission is an ensemble of these two models, further increasing 0.3 BLEU for Fr-De.', 'pages': '117--122', 'url': 'https://aclanthology.org/2021.wmt-1.6', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': \"Escolano, Carlos  and\\nTsiamas, Ioannis  and\\nBasta, Christine  and\\nFerrando, Javier  and\\nCosta-jussa, Marta R.  and\\nFonollosa, Jos{\\\\'e} A. R.\", 'title': 'The {TALP}-{UPC} Participation in {WMT}21 News Translation Task: an m{BART}-based {NMT} Approach', 'ENTRYTYPE': 'inproceedings', 'ID': 'escolano-etal-2021-talp'}, {'abstract': 'We describe our two NMT systems submitted to the WMT2021 shared task in English-Czech news translation: CUNI-DocTransformer (document-level CUBBITT) and CUNI-Marian-Baselines. We improve the former with a better sentence-segmentation pre-processing and a post-processing for fixing errors in numbers and units. We use the latter for experiments with various backtranslation techniques.', 'pages': '123--129', 'url': 'https://aclanthology.org/2021.wmt-1.7', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': \"Gebauer, Petr  and\\nBojar, Ond{\\\\v{r}}ej  and\\n{\\\\v{S}}vandel{\\\\'\\\\i}k, Vojt{\\\\v{e}}ch  and\\nPopel, Martin\", 'title': '{CUNI} Systems in {WMT}21: Revisiting Backtranslation Techniques for {E}nglish-{C}zech {NMT}', 'ENTRYTYPE': 'inproceedings', 'ID': 'gebauer-etal-2021-cuni'}, {'abstract': 'This paper describes the Microsoft Egypt Development Center (EgDC) submission to the constrained track of WMT21 shared news translation task. We focus on the three relatively low resource language pairs Bengali  Hindi, English  Hausa and Xhosa  Zulu. To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective employing both parallel and monolingual data. In addition, we augment the data using back translation. We also train a bilingual model incorporating back translation and knowledge distillation then combine the two models using sequence-to-sequence mapping. We see around 70{\\\\%} relative gain in BLEU point for En  Ha and around 25{\\\\%} relative improvements for Bn  Hi and Xh  Zu compared to bilingual baselines.', 'pages': '130--135', 'url': 'https://aclanthology.org/2021.wmt-1.8', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Hendy, Amr  and\\nGad, Esraa A.  and\\nAbdelghaffar, Mohamed  and\\nElMosalami, Jailan S.  and\\nAfify, Mohamed  and\\nTawfik, Ahmed Y.  and\\nAwadalla, Hany Hassan', 'title': 'Ensembling of Distilled Models from Multi-task Teachers for Constrained Resource Language Pairs', 'ENTRYTYPE': 'inproceedings', 'ID': 'hendy-etal-2021-ensembling'}, {'abstract': \"We present Mi{\\\\dh}eind{'}s submission for the EnglishIcelandic and IcelandicEnglish subsets of the 2021 WMT news translation task. Transformer-base models are trained for translation on parallel data to generate backtranslations teratively. A pretrained mBART-25 model is then adapted for translation using parallel data as well as the last backtranslation iteration. This adapted pretrained model is then used to re-generate backtranslations, and the training of the adapted model is continued.\", 'pages': '136--139', 'url': 'https://aclanthology.org/2021.wmt-1.9', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': \"S{\\\\'\\\\i}monarson, Haukur Barri  and\\nSn{\\\\ae}bjarnarson, V{\\\\'e}steinn  and\\nRagnarson, P{\\\\'e}tur Orri  and\\nJ{\\\\'o}nsson, Haukur  and\\nThorsteinsson, Vilhjalmur\", 'title': \"Mi{\\\\dh}eind{'}s {WMT} 2021 Submission\", 'ENTRYTYPE': 'inproceedings', 'ID': 'simonarson-etal-2021-mideinds'}, {'abstract': 'We submitted two uni-directional models, one for EnglishIcelandic direction and other for IcelandicEnglish direction. Our news translation system is based on the transformer-big architecture, it makes use of corpora filtering, back-translation and forward translation applied to parallel and monolingual data alike', 'pages': '140--143', 'url': 'https://aclanthology.org/2021.wmt-1.10', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Koszowski, Miko{\\\\l}aj  and\\nGrzegorczyk, Karol  and\\nHadeliya, Tsimur', 'title': 'Allegro.eu Submission to {WMT}21 News Translation Task', 'ENTRYTYPE': 'inproceedings', 'ID': 'koszowski-etal-2021-allegro'}, {'abstract': 'This system paper describes an end-to-end NMT pipeline for the Japanese $\\\\leftrightarrow$ English news translation task as submitted to WMT 2021, where we explore the efficacy of techniques such as tokenizing with language-independent and language-dependent tokenizers, normalizing by orthographic conversion, creating a politeness-and-formality-aware model by implementing a tagger, back-translation, model ensembling, and n-best reranking. We use parallel corpora provided by WMT 2021 organizers for training, and development and test data from WMT 2020 for evaluation of different experiment models. The preprocessed corpora are trained with a Transformer neural network model. We found that combining various techniques described herein, such as language-independent BPE tokenization, incorporating politeness and formality tags, model ensembling, n-best reranking, and back-translation produced the best translation models relative to other experiment systems.', 'pages': '144--153', 'url': 'https://aclanthology.org/2021.wmt-1.11', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Le, Giang  and\\nMori, Shinka  and\\nSchwartz, Lane', 'title': '{I}llinois {J}apanese $\\\\leftrightarrow$ {E}nglish {N}ews {T}ranslation for {WMT} 2021', 'ENTRYTYPE': 'inproceedings', 'ID': 'le-etal-2021-illinois'}, {'abstract': \"In this paper, we describe our MiSS system that participated in the WMT21 news translation task. We mainly participated in the evaluation of the three translation directions of English-Chinese and Japanese-English translation tasks. In the systems submitted, we primarily considered wider networks, deeper networks, relative positional encoding, and dynamic convolutional networks in terms of model structure, while in terms of training, we investigated contrastive learning-reinforced domain adaptation, self-supervised training, and optimization objective switching training methods. According to the final evaluation results, a deeper, wider, and stronger network can improve translation performance in general, yet our data domain adaption method can improve performance even more. In addition, we found that switching to the use of our proposed objective during the finetune phase using relatively small domain-related data can effectively improve the stability of the model{'}s convergence and achieve better optimal performance.\", 'pages': '154--161', 'url': 'https://aclanthology.org/2021.wmt-1.12', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Li, Zuchao  and\\nUtiyama, Masao  and\\nSumita, Eiichiro  and\\nZhao, Hai', 'title': '{M}i{SS}@{WMT}21: Contrastive Learning-reinforced Domain Adaptation in Neural Machine Translation', 'ENTRYTYPE': 'inproceedings', 'ID': 'li-etal-2021-miss-wmt21'}, {'abstract': 'This paper describes the Fujitsu DMATH systems used for WMT 2021 News Translation and Biomedical Translation tasks. We focused on low-resource pairs, using a simple system. We conducted experiments on English-Hausa, Xhosa-Zulu and English-Basque, and submitted the results for XhosaZulu in the News Translation Task, and EnglishBasque in the Biomedical Translation Task, abstract and terminology translation subtasks. Our system combines BPE dropout, sub-subword features and back-translation with a Transformer (base) model, achieving good results on the evaluation sets.', 'pages': '162--166', 'url': 'https://aclanthology.org/2021.wmt-1.13', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Martinez, Ander', 'title': 'The Fujitsu {DMATH} Submissions for {WMT}21 News Translation and Biomedical Translation Tasks', 'ENTRYTYPE': 'inproceedings', 'ID': 'martinez-2021-fujitsu'}, {'abstract': \"This paper presents the Adam Mickiewicz University{'}s (AMU) submissions to the WMT 2021 News Translation Task. The submissions focus on the EnglishHausa translation directions, which is a low-resource translation scenario between distant languages. Our approach involves thorough data cleaning, transfer learning using a high-resource language pair, iterative training, and utilization of monolingual data via back-translation. We experiment with NMT and PB-SMT approaches alike, using the base Transformer architecture for all of the NMT models while utilizing PB-SMT systems as comparable baseline solutions.\", 'pages': '167--171', 'url': 'https://aclanthology.org/2021.wmt-1.14', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Nowakowski, Artur  and\\nDwojak, Tomasz', 'title': \"{A}dam {M}ickiewicz {U}niversity{'}s {E}nglish-{H}ausa Submissions to the {WMT} 2021 News Translation Task\", 'ENTRYTYPE': 'inproceedings', 'ID': 'nowakowski-dwojak-2021-adam'}, {'abstract': \"The paper describes the 3 NMT models submitted by the eTranslation team to the WMT 2021 news translation shared task. We developed systems in language pairs that are actively used in the European Commission{'}s eTranslation service. In the WMT news task, recent years have seen a steady increase in the need for computational resources to train deep and complex architectures to produce competitive systems. We took a different approach and explored alternative strategies focusing on data selection and filtering to improve the performance of baseline systems. In the domain constrained task for the French{--}German language pair our approach resulted in the best system by a significant margin in BLEU. For the other two systems (English{--}German and English-Czech) we tried to build competitive models using standard best practices.\", 'pages': '172--179', 'url': 'https://aclanthology.org/2021.wmt-1.15', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': \"Oravecz, Csaba  and\\nBontcheva, Katina  and\\nKolovratn{\\\\'\\\\i}k, David  and\\nBhaskar, Bhavani  and\\nJellinghaus, Michael  and\\nEisele, Andreas\", 'title': \"e{T}ranslation{'}s Submissions to the {WMT} 2021 News Translation Task\", 'ENTRYTYPE': 'inproceedings', 'ID': 'oravecz-etal-2021-etranslations'}, {'abstract': \"We describe the University of Edinburgh{'}s Bengali$\\\\leftrightarrow$Hindi constrained systems submitted to the WMT21 News Translation task. We submitted ensembles of Transformer models built with large-scale back-translation and fine-tuned on subsets of training data retrieved based on similarity to the target domain.\", 'pages': '180--186', 'url': 'https://aclanthology.org/2021.wmt-1.16', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Pal, Proyag  and\\nAji, Alham Fikri  and\\nChen, Pinzhen  and\\nSen, Sukanta', 'title': \"The {U}niversity of {E}dinburgh{'}s {B}engali-{H}indi Submissions to the {WMT}21 News Translation Task\", 'ENTRYTYPE': 'inproceedings', 'ID': 'pal-etal-2021-university'}, {'abstract': \"This paper describes the Volctrans{'} submission to the WMT21 news translation shared task for German-{\\\\textgreater}English translation. We build a parallel (i.e., non-autoregressive) translation system using the Glancing Transformer, which enables fast and accurate parallel decoding in contrast to the currently prevailing autoregressive models. To the best of our knowledge, this is the first parallel translation system that can be scaled to such a practical scenario like WMT competition. More importantly, our parallel translation system achieves the best BLEU score (35.0) on German-{\\\\textgreater}English translation task, outperforming all strong autoregressive counterparts.\", 'pages': '187--196', 'url': 'https://aclanthology.org/2021.wmt-1.17', 'publisher': 'Association for Computational Linguistics', 'address': 'Online', 'year': '2021', 'month': 'November', 'booktitle': 'Proceedings of the Sixth Conference on Machine Translation', 'author': 'Qian, Lihua  and\\nZhou, Yi  and\\nZheng, Zaixiang  and\\nZhu, Yaoming  and\\nLin, Zehui  and\\nFeng, Jiangtao  and\\nCheng, Shanbo  and\\nLi, Lei  and\\nWang, Mingxuan  and\\nZhou, Hao', 'title': 'The Volctrans {GLAT} System: Non-autoregressive Translation Meets {WMT}21', 'ENTRYTYPE': 'inproceedings', 'ID': 'qian-etal-2021-volctrans'}]\n"
     ]
    }
   ],
   "source": [
    "print(bib_database.entries[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bib_df = pd.DataFrame(bib_database.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>address</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>editor</th>\n",
       "      <th>title</th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>ID</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pages</th>\n",
       "      <th>doi</th>\n",
       "      <th>booktitle</th>\n",
       "      <th>author</th>\n",
       "      <th>volume</th>\n",
       "      <th>journal</th>\n",
       "      <th>language</th>\n",
       "      <th>number</th>\n",
       "      <th>isbn</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://aclanthology.org/2021.woah-1.0</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>Mostafazadeh Davani, Aida  and\\nKiela, Douwe  ...</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>woah-2021-online</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://aclanthology.org/2021.woah-1.1</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exploiting Auxiliary Data for Offensive Langua...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>singh-li-2021-exploiting</td>\n",
       "      <td>Offensive language detection (OLD) has receive...</td>\n",
       "      <td>1--5</td>\n",
       "      <td>10.18653/v1/2021.woah-1.1</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>Singh, Sumer  and\\nLi, Sheng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://aclanthology.org/2021.woah-1.2</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Modeling Profanity and Hate Speech in Social M...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>hahn-etal-2021-modeling</td>\n",
       "      <td>Hate speech and profanity detection suffer fro...</td>\n",
       "      <td>6--16</td>\n",
       "      <td>10.18653/v1/2021.woah-1.2</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>Hahn, Vanessa  and\\nRuiter, Dana  and\\nKleinba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://aclanthology.org/2021.woah-1.3</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{H}ate{BERT}: Retraining {BERT} for Abusive La...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>caselli-etal-2021-hatebert</td>\n",
       "      <td>We introduce HateBERT, a re-trained BERT model...</td>\n",
       "      <td>17--25</td>\n",
       "      <td>10.18653/v1/2021.woah-1.3</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>Caselli, Tommaso  and\\nBasile, Valerio  and\\nM...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://aclanthology.org/2021.woah-1.4</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>August</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Memes in the Wild: Assessing the Generalizabil...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>kirk-etal-2021-memes</td>\n",
       "      <td>Hateful memes pose a unique challenge for curr...</td>\n",
       "      <td>26--35</td>\n",
       "      <td>10.18653/v1/2021.woah-1.4</td>\n",
       "      <td>Proceedings of the 5th Workshop on Online Abus...</td>\n",
       "      <td>Kirk, Hannah  and\\nJun, Yennie  and\\nRauba, Pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      url  \\\n",
       "0  https://aclanthology.org/2021.woah-1.0   \n",
       "1  https://aclanthology.org/2021.woah-1.1   \n",
       "2  https://aclanthology.org/2021.woah-1.2   \n",
       "3  https://aclanthology.org/2021.woah-1.3   \n",
       "4  https://aclanthology.org/2021.woah-1.4   \n",
       "\n",
       "                                   publisher address  year   month  \\\n",
       "0  Association for Computational Linguistics  Online  2021  August   \n",
       "1  Association for Computational Linguistics  Online  2021  August   \n",
       "2  Association for Computational Linguistics  Online  2021  August   \n",
       "3  Association for Computational Linguistics  Online  2021  August   \n",
       "4  Association for Computational Linguistics  Online  2021  August   \n",
       "\n",
       "                                              editor  \\\n",
       "0  Mostafazadeh Davani, Aida  and\\nKiela, Douwe  ...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                               title      ENTRYTYPE  \\\n",
       "0  Proceedings of the 5th Workshop on Online Abus...    proceedings   \n",
       "1  Exploiting Auxiliary Data for Offensive Langua...  inproceedings   \n",
       "2  Modeling Profanity and Hate Speech in Social M...  inproceedings   \n",
       "3  {H}ate{BERT}: Retraining {BERT} for Abusive La...  inproceedings   \n",
       "4  Memes in the Wild: Assessing the Generalizabil...  inproceedings   \n",
       "\n",
       "                           ID  \\\n",
       "0            woah-2021-online   \n",
       "1    singh-li-2021-exploiting   \n",
       "2     hahn-etal-2021-modeling   \n",
       "3  caselli-etal-2021-hatebert   \n",
       "4        kirk-etal-2021-memes   \n",
       "\n",
       "                                            abstract   pages  \\\n",
       "0                                                NaN     NaN   \n",
       "1  Offensive language detection (OLD) has receive...    1--5   \n",
       "2  Hate speech and profanity detection suffer fro...   6--16   \n",
       "3  We introduce HateBERT, a re-trained BERT model...  17--25   \n",
       "4  Hateful memes pose a unique challenge for curr...  26--35   \n",
       "\n",
       "                         doi  \\\n",
       "0                        NaN   \n",
       "1  10.18653/v1/2021.woah-1.1   \n",
       "2  10.18653/v1/2021.woah-1.2   \n",
       "3  10.18653/v1/2021.woah-1.3   \n",
       "4  10.18653/v1/2021.woah-1.4   \n",
       "\n",
       "                                           booktitle  \\\n",
       "0                                                NaN   \n",
       "1  Proceedings of the 5th Workshop on Online Abus...   \n",
       "2  Proceedings of the 5th Workshop on Online Abus...   \n",
       "3  Proceedings of the 5th Workshop on Online Abus...   \n",
       "4  Proceedings of the 5th Workshop on Online Abus...   \n",
       "\n",
       "                                              author volume journal language  \\\n",
       "0                                                NaN    NaN     NaN      NaN   \n",
       "1                       Singh, Sumer  and\\nLi, Sheng    NaN     NaN      NaN   \n",
       "2  Hahn, Vanessa  and\\nRuiter, Dana  and\\nKleinba...    NaN     NaN      NaN   \n",
       "3  Caselli, Tommaso  and\\nBasile, Valerio  and\\nM...    NaN     NaN      NaN   \n",
       "4  Kirk, Hannah  and\\nJun, Yennie  and\\nRauba, Pa...    NaN     NaN      NaN   \n",
       "\n",
       "  number isbn note  \n",
       "0    NaN  NaN  NaN  \n",
       "1    NaN  NaN  NaN  \n",
       "2    NaN  NaN  NaN  \n",
       "3    NaN  NaN  NaN  \n",
       "4    NaN  NaN  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74393 entries, 0 to 74392\n",
      "Data columns (total 20 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   url        74393 non-null  object\n",
      " 1   publisher  63423 non-null  object\n",
      " 2   address    66527 non-null  object\n",
      " 3   year       74393 non-null  object\n",
      " 4   month      66269 non-null  object\n",
      " 5   editor     1610 non-null   object\n",
      " 6   title      74393 non-null  object\n",
      " 7   ENTRYTYPE  74393 non-null  object\n",
      " 8   ID         74393 non-null  object\n",
      " 9   abstract   30560 non-null  object\n",
      " 10  pages      57812 non-null  object\n",
      " 11  doi        27982 non-null  object\n",
      " 12  booktitle  69812 non-null  object\n",
      " 13  author     71517 non-null  object\n",
      " 14  volume     2319 non-null   object\n",
      " 15  journal    2538 non-null   object\n",
      " 16  language   3029 non-null   object\n",
      " 17  number     1834 non-null   object\n",
      " 18  isbn       1405 non-null   object\n",
      " 19  note       200 non-null    object\n",
      "dtypes: object(20)\n",
      "memory usage: 11.4+ MB\n"
     ]
    }
   ],
   "source": [
    "bib_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = bib_df['abstract'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{``},Transformer,Transformer,,,,,,,,,1.48BLEU{''} \n",
      "\n",
      "{``},XLM,,Bi-LSTMWMT{'}19,XLM{''} \n",
      "\n",
      "               empty    109\n",
      "   short(< 200 char)     57\n",
      "           non latin    145\n",
      "               other  30249\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "minimum = 200\n",
    "example = 0\n",
    "\n",
    "count = {'empty': 0,\n",
    "        f'short(< {minimum} char)': 0,\n",
    "        'non latin': 0,\n",
    "        'other': 0}\n",
    "\n",
    "for a in abstract:\n",
    "    if len(a) == 0:\n",
    "        count['empty'] += 1\n",
    "    elif len(re.findall('[a-zA-Z]',a )) < .2*len(a):\n",
    "        count['non latin'] += 1\n",
    "        if example < 2:\n",
    "            print (a, '\\n')\n",
    "            example += 1\n",
    "    elif len(a) < minimum:\n",
    "        count[f'short(< { minimum} char)'] += 1\n",
    "    else:\n",
    "        count['other'] += 1\n",
    "    \n",
    "for k, v in count.items():\n",
    "    print(f'{k:>20}', f'{v:>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in abstract.index:\n",
    "    if len(abstract[i]) < 200:\n",
    "        abstract = abstract.drop(labels=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in abstract.index:\n",
    "    if len(re.findall('[a-zA-Z]', abstract[i])) < .2*len(abstract[i]):\n",
    "        abstract = abstract.drop(labels=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_sample = abstract.sample(n=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "\n",
    "def get_shingles(s, k):\n",
    "    \"\"\"Get all shingles from all abstract sample packed in a series\n",
    "    \"\"\"\n",
    "    # get abstract as a single string\n",
    "    abstract_text = ' '.join([elem.strip() for elem in s])\n",
    "    abstract_text = re.sub(' +', ' ', abstract_text)  # remove double spaces\n",
    "    \n",
    "    L = len(abstract_text)\n",
    "    shingles = set()  # we use a set to automatically eliminate duplicates\n",
    "    #shingles_original = set()\n",
    "    for i in range(L-k+1):\n",
    "        shingle = abstract_text[i:i+k]\n",
    "        crc = binascii.crc32(shingle.encode('utf-8')) #& 0xffffffff  # hash the shingle to a 32-bit integer\n",
    "        shingles.add(crc)\n",
    "        #shingles_original.add(shingle)\n",
    "        \n",
    "    return shingles#, shingles_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of shingles in case of k = 3: 15286\n",
      "Number of shingles in case of k = 5: 95646\n",
      "Number of shingles in case of k = 10: 505207\n"
     ]
    }
   ],
   "source": [
    "shingles_3 = get_shingles(abstract_sample, 3)\n",
    "shingles_5 = get_shingles(abstract_sample, 5)\n",
    "shingles_10 = get_shingles(abstract_sample, 10)\n",
    "\n",
    "print('Number of shingles in case of k = {}: {}'. format(3, len(get_shingles(abstract_sample, 3))))\n",
    "print('Number of shingles in case of k = {}: {}'. format(5, len(get_shingles(abstract_sample, 5))))\n",
    "print('Number of shingles in case of k = {}: {}'. format(10, len(get_shingles(abstract_sample, 10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles_vectors = []\n",
    "for item in abstract_sample[:100]:\n",
    "    sh = list(get_shingles(item, k=5))\n",
    "    shingles_vectors.append(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2456597222222222"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_similarity_score(x, y):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity J (A,B) = | Intersection (A,B) | /\n",
    "                                    | Union (A,B) |\n",
    "    \"\"\"\n",
    "    intersection_cardinality = len(set(x).intersection(set(y)))\n",
    "    union_cardinality = len(set(x).union(set(y)))\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "jaccard_similarity_score(shingles_vectors[0], shingles_vectors[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 452 s\n",
      "Number of similar items: 499500\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import itertools\n",
    "\n",
    "\n",
    "s = 0.1\n",
    "candidates = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(abstract_sample, 2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], 3),get_shingles(pair[1], 3))\n",
    "    \n",
    "    if js > s:\n",
    "        #print(pair)\n",
    "        candidates.append(pair)\n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 481 s\n",
      "Number of similar items: 499480\n"
     ]
    }
   ],
   "source": [
    "s = 0.1\n",
    "candidates = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(abstract_sample, 2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], 5),get_shingles(pair[1], 5))\n",
    "    \n",
    "    if js > s:\n",
    "        #print(pair)\n",
    "        candidates.append(pair)\n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 515 s\n",
      "Number of similar items: 40\n"
     ]
    }
   ],
   "source": [
    "s = 0.1\n",
    "candidates = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(abstract_sample, 2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], 10),get_shingles(pair[1], 10))\n",
    "    \n",
    "    if js > s:\n",
    "        #print(pair)\n",
    "        candidates.append(pair)\n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 452 s\n",
      "Number of similar items: 499500\n"
     ]
    }
   ],
   "source": [
    "ss = 0.2\n",
    "candidates = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(abstract_sample, 2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], 3),get_shingles(pair[1], 3))\n",
    "    \n",
    "    if js > ss:\n",
    "        #print(pair)\n",
    "        candidates.append(pair)\n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 484 s\n",
      "Number of similar items: 403979\n"
     ]
    }
   ],
   "source": [
    "ss = 0.2\n",
    "candidates = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(abstract_sample, 2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], 5),get_shingles(pair[1], 5))\n",
    "    \n",
    "    if js > ss:\n",
    "        #print(pair)\n",
    "        candidates.append(pair)\n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 537 s\n",
      "Number of similar items: 0\n"
     ]
    }
   ],
   "source": [
    "ss = 0.2\n",
    "candidates = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(abstract_sample, 2):\n",
    "    js = jaccard_similarity_score(get_shingles(pair[0], 10),get_shingles(pair[1], 10))\n",
    "    \n",
    "    if js > ss:\n",
    "        #print(pair)\n",
    "        candidates.append(pair)\n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "nsig_50 = 50 # number of random hashing functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A_50 = numpy.random.randint(0, nextPrime, size=(nsig_50,),dtype=numpy.int64)\n",
    "B_50 = numpy.random.randint(0, nextPrime, size=(nsig_50,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig):\n",
    "    signature = numpy.ones((nsig,)) * (maxShingleID + 1)\n",
    "\n",
    "    for ShingleID in shingles:\n",
    "        hashCodes = ((A*ShingleID + B) % nextPrime) % maxShingleID\n",
    "        numpy.minimum(signature, hashCodes, out=signature)\n",
    "\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.9680042266845703 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_3_mh_s = []\n",
    "for element in abstract_sample:\n",
    "    shingles_3_mh = get_shingles(element, k=3)\n",
    "    signature = minhash_vectorized(shingles_3_mh, A_50, B_50, nextPrime, maxShingleID, nsig_50)\n",
    "    signatures_3_mh_s.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_3_mh_s)\n",
    "t = time.time()\n",
    "candidates_3_mh_s = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_3_mh_s[i] == signatures_3_mh_s[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_3_mh_s.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_3_mh_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.8300442695617676 seconds\n",
      "found 491541 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_5_mh_s = []\n",
    "for element in abstract_sample:\n",
    "    shingles_5_mh = get_shingles(element, k=5)\n",
    "    signature = minhash_vectorized(shingles_5_mh, A_50, B_50, nextPrime, maxShingleID, nsig_50)\n",
    "    signatures_5_mh_s.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_5_mh_s)\n",
    "t = time.time()\n",
    "candidates_5_mh_s = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_5_mh_s[i] == signatures_5_mh_s[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_5_mh_s.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_5_mh_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.6692378520965576 seconds\n",
      "found 12655 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_10_mh_s = []\n",
    "for element in abstract_sample:\n",
    "    shingles_10_mh = get_shingles(element, k=10)\n",
    "    signature = minhash_vectorized(shingles_10_mh, A_50, B_50, nextPrime, maxShingleID, nsig_50)\n",
    "    signatures_10_mh_s.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_10_mh_s)\n",
    "t = time.time()\n",
    "candidates_10_mh_s = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_10_mh_s[i] == signatures_10_mh_s[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_10_mh_s.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_10_mh_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.9908225536346436 seconds\n",
      "found 499437 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_3_mh_ss = []\n",
    "for element in abstract_sample:\n",
    "    shingles_3_mh_ss = get_shingles(element, k=3)\n",
    "    signature = minhash_vectorized(shingles_3_mh_ss, A_50, B_50, nextPrime, maxShingleID, nsig_50)\n",
    "    signatures_3_mh_ss.append(signature)\n",
    "\n",
    "ss = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures_3_mh_ss)\n",
    "t = time.time()\n",
    "candidates_3_mh_ss = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_3_mh_ss[i] == signatures_3_mh_ss[j])  # average number of similar items in \n",
    "        if Jsim >= ss:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_3_mh_ss.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_3_mh_ss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.627650260925293 seconds\n",
      "found 326345 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_5_mh_ss = []\n",
    "for element in abstract_sample:\n",
    "    shingles_5_mh_ss = get_shingles(element, k=5)\n",
    "    signature = minhash_vectorized(shingles_5_mh_ss, A_50, B_50, nextPrime, maxShingleID, nsig_50)\n",
    "    signatures_5_mh_ss.append(signature)\n",
    "\n",
    "ss = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures_5_mh_ss)\n",
    "t = time.time()\n",
    "candidates_5_mh_ss = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_5_mh_ss[i] == signatures_5_mh_ss[j])  # average number of similar items in \n",
    "        if Jsim >= ss:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_5_mh_ss.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_5_mh_ss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "finding candidates took 2.736363172531128 seconds\n",
      "found 25 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_10_mh_ss = []\n",
    "for element in abstract_sample:\n",
    "    shingles_10_mh_ss = get_shingles(element, k=10)\n",
    "    signature = minhash_vectorized(shingles_10_mh_ss, A_50, B_50, nextPrime, maxShingleID, nsig_50)\n",
    "    signatures_10_mh_ss.append(signature)\n",
    "\n",
    "\n",
    "ss = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures_10_mh_ss)\n",
    "print(Nfiles)\n",
    "t = time.time()\n",
    "candidates_10_mh_ss = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_10_mh_ss[i] == signatures_10_mh_ss[j])  # average number of similar items in \n",
    "        if Jsim >= ss:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_10_mh_ss.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_10_mh_ss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSH(signatures, bands, rows, Ab, Bb, nextPrime, maxShingleID):\n",
    "    \"\"\"Locality Sensitive Hashing\n",
    "    \"\"\"\n",
    "    numItems = signatures.shape[1]\n",
    "    signBands = numpy.array_split(signatures, bands, axis=0)\n",
    "    candidates = set()\n",
    "    for nb in range(bands):\n",
    "        hashTable = {}\n",
    "        for ni in range(numItems):\n",
    "            item = signBands[nb][:,ni]\n",
    "            hash = (numpy.dot(Ab[nb,:], item) + Bb[nb]) % nextPrime % maxShingleID\n",
    "            if hash not in hashTable:\n",
    "                hashTable[hash] = [ni]\n",
    "            else:\n",
    "                hashTable[hash].append(ni)\n",
    "        for _,items in hashTable.items():\n",
    "            if len(items) > 1:\n",
    "                L = len(items)\n",
    "                for i in range(L-1):\n",
    "                    for j in range(i+1, L):\n",
    "                        cand = [items[i], items[j]]\n",
    "                        numpy.sort(cand)\n",
    "                        candidates.add(tuple(cand))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_50 = 5\n",
    "rows_50 = 10\n",
    "nsig_50 = 50 # nsig_50 = bands_50*rows_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local sensitive hashing with k=3 and 50 hashing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1000)\n",
      "1000\n",
      "finding candidates took 7.1178343296051025 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "# find candidates with LSH\n",
    "signatures_3 = []\n",
    "for element in abstract_sample:\n",
    "    shingle_3 = get_shingles(element, k=3)\n",
    "    signature = minhash_vectorized(shingles_3, A_50, B_50, nextPrime, maxShingleID, nsig_50)\n",
    "    signatures_3.append(signature)\n",
    "\n",
    "\n",
    "# prepare data for LSH\n",
    "A2 = numpy.random.randint(0, nextPrime/2, size=(bands_50, rows_50))  \n",
    "B2 = numpy.random.randint(0, nextPrime/2, size=(bands_50, ))\n",
    "signatures_3 = numpy.array(signatures_3).T  \n",
    "print(signatures_3.shape)\n",
    "\n",
    "\n",
    "Nfiles = signatures_3.shape[1]  # number of different files\n",
    "print(Nfiles)\n",
    "t = time.time()\n",
    "candidates_3 = LSH(signatures_3, bands_50, rows_50, A2, B2, nextPrime, maxShingleID)\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1000)\n",
      "1000\n",
      "finding candidates took 7.187246322631836 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_5 = []\n",
    "for element in abstract_sample:\n",
    "    shingle_5 = get_shingles(element, k=5)\n",
    "    signature = minhash_vectorized(shingles_5, A_50, B_50, nextPrime, maxShingleID, nsig_50)\n",
    "    signatures_5.append(signature)\n",
    "\n",
    "\n",
    "# prepare data for LSH\n",
    "A2 = numpy.random.randint(0, nextPrime/2, size=(bands_50, rows_50))  \n",
    "B2 = numpy.random.randint(0, nextPrime/2, size=(bands_50, ))\n",
    "signatures_5 = numpy.array(signatures_5).T  \n",
    "print(signatures_5.shape)\n",
    "\n",
    "\n",
    "Nfiles = signatures_5.shape[1]  # number of different files\n",
    "print(Nfiles)\n",
    "t = time.time()\n",
    "candidates_5 = LSH(signatures_5, bands_50, rows_50, A2, B2, nextPrime, maxShingleID)\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1000)\n",
      "1000\n",
      "finding candidates took 6.943505525588989 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_10 = []\n",
    "for element in abstract_sample:\n",
    "    shingle_10 = get_shingles(element, k=10)\n",
    "    signature = minhash_vectorized(shingles_10, A_50, B_50, nextPrime, maxShingleID, nsig_50)\n",
    "    signatures_10.append(signature)\n",
    "\n",
    "\n",
    "# prepare data for LSH\n",
    "A2 = numpy.random.randint(0, nextPrime/2, size=(bands_50, rows_50))  \n",
    "B2 = numpy.random.randint(0, nextPrime/2, size=(bands_50, ))\n",
    "signatures_10 = numpy.array(signatures_10).T  \n",
    "print(signatures_10.shape)\n",
    "rows_50 = 10\n",
    "nsig_50 = 50 # nsig_50 = bands_50*rows_50\n",
    "\n",
    "Nfiles = signatures_10.shape[1]  # number of different files\n",
    "print(Nfiles)\n",
    "t = time.time()\n",
    "candidates_10 = LSH(signatures_10, bands_50, rows_50, A2, B2, nextPrime, maxShingleID)\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsig_100 = 100 # number of random hashing functions\n",
    "\n",
    "A_100 = numpy.random.randint(0, nextPrime, size=(nsig_100,),dtype=numpy.int64)\n",
    "B_100 = numpy.random.randint(0, nextPrime, size=(nsig_100,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 3, 100 hashing functions, and similarity threshold s = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.60734224319458 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_3_mh100_s = []\n",
    "for element in abstract_sample:\n",
    "    shingles_3_mh = get_shingles(element, k=3)\n",
    "    signature = minhash_vectorized(shingles_3_mh, A_100, B_100, nextPrime, maxShingleID, nsig_100)\n",
    "    signatures_3_mh100_s.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_3_mh100_s)\n",
    "t = time.time()\n",
    "candidates_3_mh100_s = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_3_mh100_s[i] == signatures_3_mh100_s[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_3_mh100_s.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_3_mh100_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 5, 100 hashing functions, and similarity threshold s = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.7404792308807373 seconds\n",
      "found 491227 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_5_mh100_s = []\n",
    "for element in abstract_sample:\n",
    "    shingles_5_mh = get_shingles(element, k=5)\n",
    "    signature = minhash_vectorized(shingles_5_mh, A_100, B_100, nextPrime, maxShingleID, nsig_100)\n",
    "    signatures_5_mh100_s.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_5_mh100_s)\n",
    "t = time.time()\n",
    "candidates_5_mh100_s = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_5_mh100_s[i] == signatures_5_mh100_s[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_5_mh100_s.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_5_mh100_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 10, 100 hashing functions, and similarity threshold s = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.6760640144348145 seconds\n",
      "found 15852 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_10_mh100_s = []\n",
    "for element in abstract_sample:\n",
    "    shingles_10_mh = get_shingles(element, k=10)\n",
    "    signature = minhash_vectorized(shingles_10_mh, A_100, B_100, nextPrime, maxShingleID, nsig_100)\n",
    "    signatures_10_mh100_s.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_10_mh100_s)\n",
    "t = time.time()\n",
    "candidates_10_mh100_s = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_10_mh100_s[i] == signatures_10_mh100_s[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_10_mh100_s.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_10_mh100_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 3, 100 hashing functions, and similarity threshold s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.624701738357544 seconds\n",
      "found 499494 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_3_mh100_ss = []\n",
    "for element in abstract_sample:\n",
    "    shingles_3_mh = get_shingles(element, k=3)\n",
    "    signature = minhash_vectorized(shingles_3_mh, A_100, B_100, nextPrime, maxShingleID, nsig_100)\n",
    "    signatures_3_mh100_ss.append(signature)\n",
    "\n",
    "ss = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures_3_mh100_ss)\n",
    "t = time.time()\n",
    "candidates_3_mh100_ss = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_3_mh100_ss[i] == signatures_3_mh100_ss[j])  # average number of similar items in \n",
    "        if Jsim >= ss:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_3_mh100_ss.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_3_mh100_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 5, 100 hashing functions, and similarity threshold s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.589125156402588 seconds\n",
      "found 278246 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_5_mh100_ss = []\n",
    "for element in abstract_sample:\n",
    "    shingles_5_mh = get_shingles(element, k=5)\n",
    "    signature = minhash_vectorized(shingles_5_mh, A_100, B_100, nextPrime, maxShingleID, nsig_100)\n",
    "    signatures_5_mh100_ss.append(signature)\n",
    "\n",
    "ss = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures_5_mh100_ss)\n",
    "t = time.time()\n",
    "candidates_5_mh100_ss = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_5_mh100_ss[i] == signatures_5_mh100_ss[j])  # average number of similar items in \n",
    "        if Jsim >= ss:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_5_mh100_ss.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_5_mh100_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 10, 100 hashing functions, and similarity threshold s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.4946372509002686 seconds\n",
      "found 2 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_10_mh100_ss = []\n",
    "for element in abstract_sample:\n",
    "    shingles_10_mh = get_shingles(element, k=10)\n",
    "    signature = minhash_vectorized(shingles_10_mh, A_100, B_100, nextPrime, maxShingleID, nsig_100)\n",
    "    signatures_10_mh100_ss.append(signature)\n",
    "\n",
    "ss = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures_10_mh100_ss)\n",
    "t = time.time()\n",
    "candidates_10_mh100_ss = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_10_mh100_ss[i] == signatures_10_mh100_ss[j])  # average number of similar items in \n",
    "        if Jsim >= ss:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_10_mh100_ss.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_10_mh100_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 3,  200 hashing functions, and similarity threshold s = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsig_200 = 200 # number of random hashing functions\n",
    "\n",
    "A_200 = numpy.random.randint(0, nextPrime, size=(nsig_200,),dtype=numpy.int64)\n",
    "B_200 = numpy.random.randint(0, nextPrime, size=(nsig_200,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.6403121948242188 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_3_mh200_s = []\n",
    "for element in abstract_sample:\n",
    "    shingles_3_mh = get_shingles(element, k=3)\n",
    "    signature = minhash_vectorized(shingles_3_mh, A_200, B_200, nextPrime, maxShingleID, nsig_200)\n",
    "    signatures_3_mh200_s.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_3_mh200_s)\n",
    "t = time.time()\n",
    "candidates_3_mh200_s = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_3_mh200_s[i] == signatures_3_mh200_s[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_3_mh200_s.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_3_mh200_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 5,  200 hashing functions, and similarity threshold s = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.676759719848633 seconds\n",
      "found 497771 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_5_mh200_s = []\n",
    "for element in abstract_sample:\n",
    "    shingles_5_mh = get_shingles(element, k=5)\n",
    "    signature = minhash_vectorized(shingles_5_mh, A_200, B_200, nextPrime, maxShingleID, nsig_200)\n",
    "    signatures_5_mh200_s.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_5_mh200_s)\n",
    "t = time.time()\n",
    "candidates_5_mh200_s = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_5_mh200_s[i] == signatures_5_mh200_s[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_5_mh200_s.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_5_mh200_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 10,  200 hashing functions, and similarity threshold s = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.534773349761963 seconds\n",
      "found 1388 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_10_mh200_s = []\n",
    "for element in abstract_sample:\n",
    "    shingles_10_mh = get_shingles(element, k=10)\n",
    "    signature = minhash_vectorized(shingles_10_mh, A_200, B_200, nextPrime, maxShingleID, nsig_200)\n",
    "    signatures_10_mh200_s.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_10_mh200_s)\n",
    "t = time.time()\n",
    "candidates_10_mh200_s = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_10_mh200_s[i] == signatures_10_mh200_s[j])  # average number of similar items in \n",
    "        if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_10_mh200_s.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_10_mh200_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 3,  200 hashing functions, and similarity threshold s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.6071908473968506 seconds\n",
      "found 499493 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_3_mh200_ss = []\n",
    "for element in abstract_sample:\n",
    "    shingles_3_mh = get_shingles(element, k=3)\n",
    "    signature = minhash_vectorized(shingles_3_mh, A_200, B_200, nextPrime, maxShingleID, nsig_200)\n",
    "    signatures_3_mh200_ss.append(signature)\n",
    "\n",
    "ss = 0.2  # similarity threshold\n",
    "Nfiles = len(signatures_3_mh200_ss)\n",
    "t = time.time()\n",
    "candidates_3_mh200_ss = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_3_mh200_ss[i] == signatures_3_mh200_ss[j])  # average number of similar items in \n",
    "        if Jsim >= ss:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_3_mh200_ss.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_3_mh200_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 5,  200 hashing functions, and similarity threshold s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.6953766345977783 seconds\n",
      "found 352210 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_5_mh200_ss = []\n",
    "for element in abstract_sample:\n",
    "    shingles_5_mh = get_shingles(element, k=5)\n",
    "    signature = minhash_vectorized(shingles_5_mh, A_200, B_200, nextPrime, maxShingleID, nsig_200)\n",
    "    signatures_5_mh200_ss.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_5_mh200_ss)\n",
    "t = time.time()\n",
    "candidates_5_mh200_ss = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_5_mh200_ss[i] == signatures_5_mh200_ss[j])  # average number of similar items in \n",
    "        if Jsim >= ss:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_5_mh200_ss.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_5_mh200_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing of k = 10,  200 hashing functions, and similarity threshold s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding candidates took 2.5934596061706543 seconds\n",
      "found 0 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_10_mh200_ss = []\n",
    "for element in abstract_sample:\n",
    "    shingles_10_mh = get_shingles(element, k=10)\n",
    "    signature = minhash_vectorized(shingles_10_mh, A_200, B_200, nextPrime, maxShingleID, nsig_200)\n",
    "    signatures_10_mh200_ss.append(signature)\n",
    "\n",
    "s = 0.1  # similarity threshold\n",
    "Nfiles = len(signatures_10_mh200_ss)\n",
    "t = time.time()\n",
    "candidates_10_mh200_ss = []\n",
    "for i in range(Nfiles):\n",
    "    for j in range(i+1, Nfiles):\n",
    "        Jsim = numpy.mean(signatures_10_mh200_ss[i] == signatures_10_mh200_ss[j])  # average number of similar items in \n",
    "        if Jsim >= ss:                                      # two vectors, equivalente to Jaccard \n",
    "            candidates_10_mh200_ss.append((i,j))\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_10_mh200_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local sensitive hashing with k=3 and 100 hashing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_100 = 10\n",
    "rows_100 = 10\n",
    "nsig_100 = bands_100*rows_100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1000)\n",
      "1000\n",
      "finding candidates took 14.21594786643982 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "# find candidates with LSH\n",
    "signatures_3_lsh100 = []\n",
    "for element in abstract_sample:\n",
    "    shingle_3 = get_shingles(element, k=3)\n",
    "    signature = minhash_vectorized(shingles_3, A_100, B_100, nextPrime, maxShingleID, nsig_100)\n",
    "    signatures_3_lsh100.append(signature)\n",
    "\n",
    "\n",
    "# prepare data for LSH\n",
    "A2_100 = numpy.random.randint(0, nextPrime/2, size=(bands_100, rows_100))  \n",
    "B2_100 = numpy.random.randint(0, nextPrime/2, size=(bands_100, ))\n",
    "signatures_3_lsh100 = numpy.array(signatures_3_lsh100).T  \n",
    "print(signatures_3_lsh100.shape)\n",
    "\n",
    "\n",
    "Nfiles = signatures_3_lsh100.shape[1]  # number of different files\n",
    "print(Nfiles)\n",
    "t = time.time()\n",
    "candidates_3_lsh100 = LSH(signatures_3_lsh100, bands_100, rows_100, A2_100, B2_100, nextPrime, maxShingleID)\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_3_lsh100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local sensitive hashing with k=5 and 100 hashing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1000)\n",
      "1000\n",
      "finding candidates took 15.617677688598633 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_5_lsh100 = []\n",
    "for element in abstract_sample:\n",
    "    shingle_5 = get_shingles(element, k=5)\n",
    "    signature = minhash_vectorized(shingles_5, A_100, B_100, nextPrime, maxShingleID, nsig_100)\n",
    "    signatures_5_lsh100.append(signature)\n",
    "\n",
    "\n",
    "# prepare data for LSH\n",
    "signatures_5_lsh100 = numpy.array(signatures_5_lsh100).T  \n",
    "print(signatures_5_lsh100.shape)\n",
    "\n",
    "\n",
    "Nfiles = signatures_5_lsh100.shape[1]  # number of different files\n",
    "print(Nfiles)\n",
    "t = time.time()\n",
    "candidates_5_lsh100 = LSH(signatures_5_lsh100, bands_100, rows_100, A2_100, B2_100, nextPrime, maxShingleID)\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_5_lsh100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local sensitive hashing with k=10 and 100 hashing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1000)\n",
      "1000\n",
      "finding candidates took 21.495899438858032 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_10_lsh100 = []\n",
    "for element in abstract_sample:\n",
    "    shingle_10 = get_shingles(element, k=10)\n",
    "    signature = minhash_vectorized(shingles_10, A_100, B_100, nextPrime, maxShingleID, nsig_100)\n",
    "    signatures_10_lsh100.append(signature)\n",
    "\n",
    "\n",
    "# prepare data for LSH\n",
    "signatures_10_lsh100 = numpy.array(signatures_10_lsh100).T  \n",
    "print(signatures_10_lsh100.shape)\n",
    "\n",
    "\n",
    "Nfiles = signatures_10_lsh100.shape[1]  # number of different files\n",
    "print(Nfiles)\n",
    "t = time.time()\n",
    "candidates_10_lsh100 = LSH(signatures_10_lsh100, bands_100, rows_100, A2_100, B2_100, nextPrime, maxShingleID)\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_10_lsh100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_200 = 10\n",
    "rows_200 =20\n",
    "nsig_200 = bands_200*rows_200 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local sensitive hashing with k=3 and 200 hashing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1000)\n",
      "1000\n",
      "finding candidates took 21.319637537002563 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_3_lsh200 = []\n",
    "for element in abstract_sample:\n",
    "    shingle_3 = get_shingles(element, k=3)\n",
    "    signature = minhash_vectorized(shingles_3, A_200, B_200, nextPrime, maxShingleID, nsig_200)\n",
    "    signatures_3_lsh200.append(signature)\n",
    "\n",
    "\n",
    "# prepare data for LSH\n",
    "A2_200 = numpy.random.randint(0, nextPrime/2, size=(bands_200, rows_200))  \n",
    "B2_200 = numpy.random.randint(0, nextPrime/2, size=(bands_200, ))\n",
    "signatures_3_lsh200 = numpy.array(signatures_3_lsh200).T  \n",
    "print(signatures_3_lsh200.shape)\n",
    "\n",
    "\n",
    "Nfiles = signatures_3_lsh200.shape[1]  # number of different files\n",
    "print(Nfiles)\n",
    "t = time.time()\n",
    "candidates_3_lsh200 = LSH(signatures_3_lsh200, bands_200, rows_200, A2_200, B2_200, nextPrime, maxShingleID)\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_3_lsh200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local sensitive hashing with k=5 and 200 hashing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1000)\n",
      "1000\n",
      "finding candidates took 14.3116774559021 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_5_lsh200 = []\n",
    "for element in abstract_sample:\n",
    "    shingle_5 = get_shingles(element, k=5)\n",
    "    signature = minhash_vectorized(shingles_5, A_200, B_200, nextPrime, maxShingleID, nsig_200)\n",
    "    signatures_5_lsh200.append(signature)\n",
    "\n",
    "\n",
    "# prepare data for LSH\n",
    "signatures_5_lsh200 = numpy.array(signatures_5_lsh200).T  \n",
    "print(signatures_5_lsh200.shape)\n",
    "\n",
    "\n",
    "Nfiles = signatures_5_lsh200.shape[1]  # number of different files\n",
    "print(Nfiles)\n",
    "t = time.time()\n",
    "candidates_5_lsh200 = LSH(signatures_5_lsh200, bands_200, rows_200, A2_200, B2_200, nextPrime, maxShingleID)\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_5_lsh200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local sensitive hashing with k=10 and 200 hashing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1000)\n",
      "1000\n",
      "finding candidates took 14.153261184692383 seconds\n",
      "found 499500 candidates\n"
     ]
    }
   ],
   "source": [
    "signatures_10_lsh200 = []\n",
    "for element in abstract_sample:\n",
    "    shingle_10 = get_shingles(element, k=10)\n",
    "    signature = minhash_vectorized(shingles_10, A_200, B_200, nextPrime, maxShingleID, nsig_200)\n",
    "    signatures_10_lsh200.append(signature)\n",
    "\n",
    "\n",
    "# prepare data for LSH\n",
    "signatures_10_lsh200 = numpy.array(signatures_10_lsh200).T  \n",
    "print(signatures_10_lsh200.shape)\n",
    "\n",
    "\n",
    "Nfiles = signatures_10_lsh200.shape[1]  # number of different files\n",
    "print(Nfiles)\n",
    "t = time.time()\n",
    "candidates_10_lsh200 = LSH(signatures_10_lsh200, bands_200, rows_200, A2_200, B2_200, nextPrime, maxShingleID)\n",
    "t2 = time.time() - t\n",
    "print(\"finding candidates took {} seconds\".format(t2))\n",
    "print(\"found {} candidates\".format(len(candidates_10_lsh200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard  distance among all pairs of minhashing of 100 hashing functions and k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 22 s\n",
      "Number of jaccard distance among all pairs: 499575\n",
      "Number of similar items: 499575\n"
     ]
    }
   ],
   "source": [
    "s = 0.1\n",
    "signatures_3_mh100_s.append(signature)\n",
    "jd_3_s = []\n",
    "candidate_3_s = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(signatures_3_mh100_s, 2):\n",
    "    js = jaccard_similarity_score(pair[0],pair[1])\n",
    "    \n",
    "    if js > s:\n",
    "        jaccard_distance = 1 - js\n",
    "        jd_3_s.append(jaccard_distance)\n",
    "        candidate_3_s.append(pair)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidate_3_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 22 s\n",
      "Number of similar items: 485064\n"
     ]
    }
   ],
   "source": [
    "ss = 0.2\n",
    "signatures_3_mh100_s.append(signature)\n",
    "jd_3_ss = []\n",
    "candidate_3_ss = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(signatures_3_mh100_s, 2):\n",
    "    js = jaccard_similarity_score(pair[0],pair[1])\n",
    "    \n",
    "    if js > ss:\n",
    "        jaccard_distance = 1 - js\n",
    "        jd_3_ss.append(jaccard_distance)\n",
    "        candidate_3_ss.append(pair)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidate_3_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard distance among all pairs of minhashing of 100 hashing functions and k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 20 s\n",
      "Number of similar items: 317052\n"
     ]
    }
   ],
   "source": [
    "s = 0.1\n",
    "signatures_5_mh100_s.append(signature)\n",
    "jd_5_s = []\n",
    "candidate_5_s = []\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(signatures_5_mh100_s, 2):\n",
    "    js = jaccard_similarity_score(pair[0],pair[1])\n",
    "    \n",
    "    if js > s:\n",
    "        jaccard_distance = 1 - js\n",
    "        jd_5_s.append(jaccard_distance)\n",
    "        candidate_5_s.append(pair)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidate_5_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 20 s\n",
      "Number of similar items: 1817\n"
     ]
    }
   ],
   "source": [
    "ss = 0.2\n",
    "signatures_5_mh100_s.append(signature)\n",
    "jd_5_ss = []\n",
    "candidate_5_ss = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(signatures_5_mh100_s, 2):\n",
    "    js = jaccard_similarity_score(pair[0],pair[1])\n",
    "    \n",
    "    if js > ss:\n",
    "        jaccard_distance = 1 - js\n",
    "        jd_5_ss.append(jaccard_distance)\n",
    "        candidate_5_ss.append(pair)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidate_5_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard distance among all pairs of minhashing of 100 hashing functions and k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 19 s\n",
      "Number of similar items: 12\n"
     ]
    }
   ],
   "source": [
    "s = 0.1\n",
    "signatures_10_mh100_s.append(signature)\n",
    "jd_10_s = []\n",
    "candidate_10_s = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(signatures_10_mh100_s, 2):\n",
    "    js = jaccard_similarity_score(pair[0],pair[1])\n",
    "    \n",
    "    if js > s:\n",
    "        jaccard_distance = 1 - js\n",
    "        jd_10_s.append(jaccard_distance)\n",
    "        candidate_10_s.append(pair)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidate_10_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed after the function: 20 s\n",
      "Number of similar items: 10\n"
     ]
    }
   ],
   "source": [
    "ss = 0.2\n",
    "signatures_10_mh100_s.append(signature)\n",
    "jd_10_ss = []\n",
    "candidate_10_ss = []\n",
    "\n",
    "start = time.time()\n",
    "for pair in itertools.combinations(signatures_10_mh100_s, 2):\n",
    "    js = jaccard_similarity_score(pair[0],pair[1])\n",
    "    \n",
    "    if js > ss:\n",
    "        jaccard_distance = 1 - js\n",
    "        jd_10_ss.append(jaccard_distance)\n",
    "        candidate_10_ss.append(pair)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed after the function: {} s'.format(round(end-start),2))\n",
    "print('Number of similar items: {}'.format(len(candidate_10_ss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8950276243093923, 0.8950276243093923, 0.8950276243093923, 0.8888888888888888, 0.8764044943820225, 0.8950276243093923]\n"
     ]
    }
   ],
   "source": [
    "print(jd_10_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62068966 0.5915493  0.73417722 ... 0.         0.         0.        ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1 features, but NearestNeighbors is expecting 499575 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4210/3627379290.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd_3_sa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mneigh_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd_3_sa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    715\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_precomputed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0mquery_is_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    401\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X has 1 features, but NearestNeighbors is expecting 499575 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "neigh_1 = NearestNeighbors(n_neighbors=1)\n",
    "jd_3_sa = np.array(jd_3_s)\n",
    "print(jd_3_sa)\n",
    "neigh_1.fit(jd_3_sa.reshape(1,-1))\n",
    "print(neigh_1.kneighbors([[1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Mining information from Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the whole anthologies abstract dataset. Extract a list of the authors and editors per publication and create baskets and perform a search of similar items, for example:\n",
    "\n",
    "- basket 1: Mostafazadeh Davani Aida,Kiela Douwe,Lambert Mathias,Vidgen, Bertie Prabhakaran Vinodkumar, Waseem, Zeerak\n",
    "- basket 2: Singh Sumer, Li Sheng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the frequent pair of items (2-tuples) using the nave, A-priori and PCY algorithms. For each of these compare the time of execution and results for supports s=10, 50, 100. Comment your results. \n",
    "\n",
    "2. For the PCY algorithm, create up to 5 compact hash tables. What is  the difference in results and time of execution for 1,2,3,4 and 5 tables? Comment your results.\n",
    "\n",
    "3. Find the final list of k-frequent items (k-tuples) for k=3 and 4. Experiment a bit and describe the best value for the support in each case. *Warning*: You can use any of the three algorithms, but be careful, because the algorithm can take too long if you don't chose it properly (well, basically don't use the nave approach ;)).\n",
    "\n",
    "4. Using one of the results of the previous items, for one k (k=2 or 3) find the possible clusters using the 1-NN criteria. Comment your results.\n",
    "\n",
    "> 1-NN means that if you have a tuple {A,B,C} and {C,E,F} then because they share one element {C}, then they belong to the same cluster  {A,B,C,E,F}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_editor = bib_df[['author', 'editor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>editor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mostafazadeh Davani, Aida  and\\nKiela, Douwe  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singh, Sumer  and\\nLi, Sheng</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hahn, Vanessa  and\\nRuiter, Dana  and\\nKleinba...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Caselli, Tommaso  and\\nBasile, Valerio  and\\nM...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kirk, Hannah  and\\nJun, Yennie  and\\nRauba, Pa...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kivlichan, Ian  and\\nLin, Zi  and\\nLiu, Jeremi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Caselli, Tommaso  and\\nSchelhaas, Arjan  and\\n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Niraula, Nobal B.  and\\nDulal, Saurab  and\\nKo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fortuna, Paula  and\\nCortez, Vanessa  and\\nSoz...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Manerba, Marta Marchiori  and\\nTonelli, Sara</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              author  \\\n",
       "0                                                NaN   \n",
       "1                       Singh, Sumer  and\\nLi, Sheng   \n",
       "2  Hahn, Vanessa  and\\nRuiter, Dana  and\\nKleinba...   \n",
       "3  Caselli, Tommaso  and\\nBasile, Valerio  and\\nM...   \n",
       "4  Kirk, Hannah  and\\nJun, Yennie  and\\nRauba, Pa...   \n",
       "5  Kivlichan, Ian  and\\nLin, Zi  and\\nLiu, Jeremi...   \n",
       "6  Caselli, Tommaso  and\\nSchelhaas, Arjan  and\\n...   \n",
       "7  Niraula, Nobal B.  and\\nDulal, Saurab  and\\nKo...   \n",
       "8  Fortuna, Paula  and\\nCortez, Vanessa  and\\nSoz...   \n",
       "9       Manerba, Marta Marchiori  and\\nTonelli, Sara   \n",
       "\n",
       "                                              editor  \n",
       "0  Mostafazadeh Davani, Aida  and\\nKiela, Douwe  ...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  \n",
       "5                                                NaN  \n",
       "6                                                NaN  \n",
       "7                                                NaN  \n",
       "8                                                NaN  \n",
       "9                                                NaN  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_editor.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mostafazadeh Davani, Aida  and\\nKiela, Douwe  and\\nLambert, Mathias  and\\nVidgen, Bertie  and\\nPrabhakaran, Vinodkumar  and\\nWaseem, Zeerak',\n",
       " 'Singh, Sumer  and\\nLi, Sheng',\n",
       " 'Hahn, Vanessa  and\\nRuiter, Dana  and\\nKleinbauer, Thomas  and\\nKlakow, Dietrich',\n",
       " \"Caselli, Tommaso  and\\nBasile, Valerio  and\\nMitrovi{\\\\'c}, Jelena  and\\nGranitzer, Michael\",\n",
       " 'Kirk, Hannah  and\\nJun, Yennie  and\\nRauba, Paulius  and\\nWachtel, Gal  and\\nLi, Ruining  and\\nBai, Xingjian  and\\nBroestl, Noah  and\\nDoff-Sotta, Martin  and\\nShtedritski, Aleksandar  and\\nAsano, Yuki M',\n",
       " 'Kivlichan, Ian  and\\nLin, Zi  and\\nLiu, Jeremiah  and\\nVasserman, Lucy',\n",
       " 'Caselli, Tommaso  and\\nSchelhaas, Arjan  and\\nWeultjes, Marieke  and\\nLeistra, Folkert  and\\nvan der Veen, Hylke  and\\nTimmerman, Gerben  and\\nNissim, Malvina',\n",
       " 'Niraula, Nobal B.  and\\nDulal, Saurab  and\\nKoirala, Diwa',\n",
       " \"Fortuna, Paula  and\\nCortez, Vanessa  and\\nSozinho Ramalho, Miguel  and\\nP{\\\\'e}rez-Mayos, Laura\",\n",
       " 'Manerba, Marta Marchiori  and\\nTonelli, Sara']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_editor_list = []\n",
    "\n",
    "for x, y in zip(author_editor['author'], author_editor['editor']):\n",
    "    author_editor_list.append(x)\n",
    "    author_editor_list.append(y)\n",
    "    \n",
    "raw_list = [i for i in author_editor_list if pd.isnull(i) == False and i != 'NaN']\n",
    "raw_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mostafazadeh Davani Aida, Kiela Douwe, Lambert Mathias, Vidgen Bertie, Prabhakaran Vinodkumar, Waseem Zeerak',\n",
       " 'Singh Sumer, Li Sheng',\n",
       " 'Hahn Vanessa, Ruiter Dana, Kleinbauer Thomas, Klakow Dietrich',\n",
       " \"Caselli Tommaso, Basile Valerio, Mitrovi{\\\\'c} Jelena, Granitzer Michael\",\n",
       " 'Kirk Hannah, Jun Yennie, Rauba Paulius, Wachtel Gal, Li Ruining, Bai Xingjian, Broestl Noah, Doff-Sotta Martin, Shtedritski Aleksandar, Asano Yuki M',\n",
       " 'Kivlichan Ian, Lin Zi, Liu Jeremiah, Vasserman Lucy',\n",
       " 'Caselli Tommaso, Schelhaas Arjan, Weultjes Marieke, Leistra Folkert, van der Veen Hylke, Timmerman Gerben, Nissim Malvina',\n",
       " 'Niraula Nobal B., Dulal Saurab, Koirala Diwa',\n",
       " \"Fortuna Paula, Cortez Vanessa, Sozinho Ramalho Miguel, P{\\\\'e}rez-Mayos Laura\",\n",
       " 'Manerba Marta Marchiori, Tonelli Sara']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = []\n",
    "for item in raw_list:\n",
    "    item = item.replace(',', '')\n",
    "    item = item.replace('  and\\n', ', ')\n",
    "    new_list.append(item)\n",
    "new_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(k, a_list, report=False):\n",
    "    C_k = []\n",
    "    b = 0\n",
    "    \n",
    "    for item in  a_list:\n",
    "        C_k.append(item)\n",
    "        for itemset in itertools.combinations(C_k, k):\n",
    "            yield frozenset(itemset)\n",
    "            C_k = []\n",
    "        if report:\n",
    "            print(\"\")\n",
    "\n",
    "                # report progress\n",
    "                # print every 1000th element to reduce clutter\n",
    "        if report:\n",
    "            if b % 1000 == 0:\n",
    "                print('processing bin ', b)\n",
    "                b += 1\n",
    "\n",
    "    # last basket\n",
    "    if len(C_k) > 0:\n",
    "        for itemset in itertools.combinations(C_k, k):\n",
    "            yield frozenset(itemset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_2 = readdata(2, new_list, report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'Singh Sumer, Li Sheng', 'Mostafazadeh Davani Aida, Kiela Douwe, Lambert Mathias, Vidgen Bertie, Prabhakaran Vinodkumar, Waseem Zeerak'})\n",
      "frozenset({'Hahn Vanessa, Ruiter Dana, Kleinbauer Thomas, Klakow Dietrich', \"Caselli Tommaso, Basile Valerio, Mitrovi{\\\\'c} Jelena, Granitzer Michael\"})\n",
      "frozenset({'Kirk Hannah, Jun Yennie, Rauba Paulius, Wachtel Gal, Li Ruining, Bai Xingjian, Broestl Noah, Doff-Sotta Martin, Shtedritski Aleksandar, Asano Yuki M', 'Kivlichan Ian, Lin Zi, Liu Jeremiah, Vasserman Lucy'})\n",
      "frozenset({'Niraula Nobal B., Dulal Saurab, Koirala Diwa', 'Caselli Tommaso, Schelhaas Arjan, Weultjes Marieke, Leistra Folkert, van der Veen Hylke, Timmerman Gerben, Nissim Malvina'})\n",
      "frozenset({'Manerba Marta Marchiori, Tonelli Sara', \"Fortuna Paula, Cortez Vanessa, Sozinho Ramalho Miguel, P{\\\\'e}rez-Mayos Laura\"})\n",
      "frozenset({'Mostafazadeh Davani Aida, Omrani Ali, Kennedy Brendan, Atari Mohammad, Ren Xiang, Dehghani Morteza', 'Zad Samira, Jimenez Joshuan, Finlayson Mark'})\n",
      "frozenset({'Chuang Yung-Sung, Gao Mingye, Luo Hongyin, Glass James, Lee Hung-yi, Chen Yun-Nung, Li Shang-Wen', 'Aksenov Dmitrii, Bourgonje Peter, Zaczynska Karolina, Ostendorff Malte, Moreno-Schneider Julian, Rehm Georg'})\n",
      "frozenset({'Xenos Alexandros, Pavlopoulos John, Androutsopoulos Ion', 'Sodhi Ravsimar, Pant Kartikey, Mamidi Radhika'})\n",
      "frozenset({'Salawu Semiu, Lumsden Jo, He Yulan', 'Risch Julian, Schmidt Philipp, Krestel Ralf'})\n",
      "frozenset({\"Trujillo Milo, Rosenblatt Sam, de Anda J{\\\\'a}uregui Guillermo, Moog Emily, Samson Briane Paul V., H{\\\\'e}bert-Dufresne Laurent, Roth Allison M.\", 'Shvets Alexander, Fortuna Paula, Soler Juan, Wanner Leo'})\n",
      "frozenset({'Mathias Lambert, Nie Shaoliang, Mostafazadeh Davani Aida, Kiela Douwe, Prabhakaran Vinodkumar, Vidgen Bertie, Waseem Zeerak', 'Bertaglia Thales, Grigoriu Andreea, Dumontier Michel, van Dijck Gijs'})\n",
      "frozenset({'Zia Haris Bin, Castro Ignacio, Tyson Gareth', 'Aggarwal Piush, Liman Michelle Espranita, Gold Darina, Zesch Torsten'})\n",
      "frozenset({'Kougia Vasiliki, Pavlopoulos John', 'Xu Wei, Ritter Alan, Baldwin Tim, Rahimi Afshin'})\n",
      "frozenset({'Dadu Tanvi, Pant Kartikey, Nagar Seema, Barbhuiya Ferdous, Dey Kuntal', 'Olsen Benjamin, Plank Barbara'})\n",
      "frozenset({'H{\\\\\"a}m{\\\\\"a}l{\\\\\"a}inen Mika, Patpong Pattama, Alnajjar Khalid, Partanen Niko, Rueter Jack', 'Lei Yanfei, Hu Chunming, Ma Guanghui, Zhang Richong'})\n",
      "frozenset({'Le Duong, Nguyen Thien Huu', 'Tran Phu Minh, Nguyen Minh Van, Nguyen Thien Huu'})\n",
      "frozenset({'Feucht Malte, Wu Zhiliang, Althammer Sophia, Tresp Volker', 'Cho Won Ik, Kim Soomin'})\n",
      "frozenset({'Higashiyama Shohei, Utiyama Masao, Watanabe Taro, Sumita Eiichiro', 'Cheong Sik Feng, Chieu Hai Leong, Lim Jing'})\n",
      "frozenset({'Chen Shuguang, Aguilar Gustavo, Neves Leonardo, Solorio Thamar', 'Plepi Joan, Flek Lucie'})\n",
      "frozenset({'Lent Heather, S{\\\\o}gaard Anders', 'Gao Mengyi, Xu Canran, Shi Peng'})\n",
      "frozenset({'Srivastava Vivek, Singh Mayank', 'Aghajani MohammadMahdi, Badri AliAkbar, Beigy Hamid'})\n"
     ]
    }
   ],
   "source": [
    "nitems = 21\n",
    "for C_k in readdata(2, new_list, report=False):\n",
    "    print(C_k)\n",
    "    nitems -= 1\n",
    "    \n",
    "    if nitems == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_C(k):\n",
    "\n",
    "    start = time.time()\n",
    "    C = {}\n",
    "    for key in readdata(k, new_list):  # False report\n",
    "        if key not in C:\n",
    "            C[key] = 1\n",
    "        else:\n",
    "            C[key] += 1\n",
    "    print(\"Took {}s for k={}\".format((time.time() - start), k))\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.36591625213623047s for k=2\n",
      "36493\n"
     ]
    }
   ],
   "source": [
    "C2 = get_C(2)\n",
    "print(len(C2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
